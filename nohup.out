--------------------加载数据中...--------------------
load data... : 1|100
load data... : 2|100
load data... : 3|100
load data... : 4|100
load data... : 5|100
load data... : 6|100
load data... : 7|100
load data... : 8|100
load data... : 9|100
load data... : 10|100
load data... : 11|100
load data... : 12|100
load data... : 13|100
load data... : 14|100
load data... : 15|100
load data... : 16|100
load data... : 17|100
load data... : 18|100
load data... : 19|100
load data... : 20|100
load data... : 21|100
load data... : 22|100
load data... : 23|100
load data... : 24|100
load data... : 25|100
load data... : 26|100
load data... : 27|100
load data... : 28|100
load data... : 29|100
load data... : 30|100
load data... : 31|100
load data... : 32|100
load data... : 33|100
load data... : 34|100
load data... : 35|100
load data... : 36|100
load data... : 37|100
load data... : 38|100
load data... : 39|100
load data... : 40|100
load data... : 41|100
load data... : 42|100
load data... : 43|100
load data... : 44|100
load data... : 45|100
load data... : 46|100
load data... : 47|100
load data... : 48|100
load data... : 49|100
load data... : 50|100
load data... : 51|100
load data... : 52|100
load data... : 53|100
load data... : 54|100
load data... : 55|100
load data... : 56|100
load data... : 57|100
load data... : 58|100
load data... : 59|100
load data... : 60|100
load data... : 61|100
load data... : 62|100
load data... : 63|100
load data... : 64|100
load data... : 65|100
load data... : 66|100
load data... : 67|100
load data... : 68|100
load data... : 69|100
load data... : 70|100
load data... : 71|100
load data... : 72|100
load data... : 73|100
load data... : 74|100
load data... : 75|100
load data... : 76|100
load data... : 77|100
load data... : 78|100
load data... : 79|100
load data... : 80|100
load data... : 81|100
load data... : 82|100
load data... : 83|100
load data... : 84|100
load data... : 85|100
load data... : 86|100
load data... : 87|100
load data... : 88|100
load data... : 89|100
load data... : 90|100
load data... : 91|100
load data... : 92|100
load data... : 93|100
load data... : 94|100
load data... : 95|100
load data... : 96|100
load data... : 97|100
load data... : 98|100
load data... : 99|100
load data... : 100|100
load data... : 1|1
--------------------数据加载完毕--------------------

Model file 'model.pkl' does not exist. You might need to train a new model.
--------------------开始训练...--------------------
step: 1, train_loss: 1.7918612957000732, acc: 0.17339999973773956, val_loss: 1.7923308610916138, val_acc: 0.1777999997138977,  lr: 1e-05
step: 2, train_loss: 1.7922295331954956, acc: 0.17299999296665192, val_loss: 1.7923003435134888, val_acc: 0.17960000038146973,  lr: 2e-05
step: 3, train_loss: 1.7921839952468872, acc: 0.1770000010728836, val_loss: 1.7922359704971313, val_acc: 0.18240000307559967,  lr: 3e-05
step: 4, train_loss: 1.7925857305526733, acc: 0.16840000450611115, val_loss: 1.7921377420425415, val_acc: 0.18559999763965607,  lr: 4e-05
step: 5, train_loss: 1.7921234369277954, acc: 0.18320000171661377, val_loss: 1.7920113801956177, val_acc: 0.19220000505447388,  lr: 5e-05
step: 6, train_loss: 1.7921310663223267, acc: 0.18700000643730164, val_loss: 1.7918609380722046, val_acc: 0.2021999955177307,  lr: 6e-05
step: 7, train_loss: 1.792115569114685, acc: 0.18960000574588776, val_loss: 1.791678547859192, val_acc: 0.2125999927520752,  lr: 7.000000000000001e-05
step: 8, train_loss: 1.791608452796936, acc: 0.20419999957084656, val_loss: 1.791478157043457, val_acc: 0.22100000083446503,  lr: 8e-05
step: 9, train_loss: 1.7912888526916504, acc: 0.20739999413490295, val_loss: 1.7912607192993164, val_acc: 0.2240000069141388,  lr: 8.999999999999999e-05
step: 10, train_loss: 1.7913075685501099, acc: 0.20499999821186066, val_loss: 1.7910195589065552, val_acc: 0.21539999544620514,  lr: 0.0001
step: 11, train_loss: 1.7907785177230835, acc: 0.21359999477863312, val_loss: 1.7907525300979614, val_acc: 0.20720000565052032,  lr: 0.00010999999999999999
step: 12, train_loss: 1.7906519174575806, acc: 0.20880000293254852, val_loss: 1.790452003479004, val_acc: 0.2046000063419342,  lr: 0.00012
step: 13, train_loss: 1.7907626628875732, acc: 0.2062000036239624, val_loss: 1.7901185750961304, val_acc: 0.2070000022649765,  lr: 0.00013
step: 14, train_loss: 1.790049433708191, acc: 0.21699999272823334, val_loss: 1.7897638082504272, val_acc: 0.20880000293254852,  lr: 0.00014000000000000001
step: 15, train_loss: 1.7896857261657715, acc: 0.21379999816417694, val_loss: 1.7893949747085571, val_acc: 0.20880000293254852,  lr: 0.00015
step: 16, train_loss: 1.7891638278961182, acc: 0.2287999987602234, val_loss: 1.7890058755874634, val_acc: 0.2078000009059906,  lr: 0.00016
step: 17, train_loss: 1.7888214588165283, acc: 0.22300000488758087, val_loss: 1.7885950803756714, val_acc: 0.20800000429153442,  lr: 0.00017
step: 18, train_loss: 1.7888175249099731, acc: 0.20819999277591705, val_loss: 1.788148283958435, val_acc: 0.2134000062942505,  lr: 0.00017999999999999998
step: 19, train_loss: 1.7882359027862549, acc: 0.22059999406337738, val_loss: 1.787705898284912, val_acc: 0.21719999611377716,  lr: 0.00019
step: 20, train_loss: 1.787935733795166, acc: 0.21899999678134918, val_loss: 1.7872562408447266, val_acc: 0.22040000557899475,  lr: 0.0002
step: 21, train_loss: 1.787609338760376, acc: 0.21899999678134918, val_loss: 1.7867658138275146, val_acc: 0.22579999268054962,  lr: 0.00021
step: 22, train_loss: 1.7865959405899048, acc: 0.23579999804496765, val_loss: 1.7862123250961304, val_acc: 0.23399999737739563,  lr: 0.00021999999999999998
step: 23, train_loss: 1.786222219467163, acc: 0.23739999532699585, val_loss: 1.785618782043457, val_acc: 0.23600000143051147,  lr: 0.00023
step: 24, train_loss: 1.785292148590088, acc: 0.2492000013589859, val_loss: 1.7849864959716797, val_acc: 0.23100000619888306,  lr: 0.00024
step: 25, train_loss: 1.7852742671966553, acc: 0.22939999401569366, val_loss: 1.7842681407928467, val_acc: 0.2386000007390976,  lr: 0.00025
step: 26, train_loss: 1.7842261791229248, acc: 0.2433999925851822, val_loss: 1.783459186553955, val_acc: 0.23839999735355377,  lr: 0.00026
step: 27, train_loss: 1.783522605895996, acc: 0.23080000281333923, val_loss: 1.7825499773025513, val_acc: 0.241799995303154,  lr: 0.00027
step: 28, train_loss: 1.7825298309326172, acc: 0.2393999993801117, val_loss: 1.7815238237380981, val_acc: 0.25519999861717224,  lr: 0.00028000000000000003
step: 29, train_loss: 1.7815874814987183, acc: 0.25540000200271606, val_loss: 1.7804101705551147, val_acc: 0.26840001344680786,  lr: 0.00029
step: 30, train_loss: 1.7805787324905396, acc: 0.2671999931335449, val_loss: 1.7792085409164429, val_acc: 0.26499998569488525,  lr: 0.0003
step: 31, train_loss: 1.779416561126709, acc: 0.26100000739097595, val_loss: 1.7778096199035645, val_acc: 0.2718000113964081,  lr: 0.00031
step: 32, train_loss: 1.7777949571609497, acc: 0.27480000257492065, val_loss: 1.7762765884399414, val_acc: 0.272599995136261,  lr: 0.00032
step: 33, train_loss: 1.7759593725204468, acc: 0.2808000147342682, val_loss: 1.7744911909103394, val_acc: 0.2766000032424927,  lr: 0.00033
step: 34, train_loss: 1.7753499746322632, acc: 0.2662000060081482, val_loss: 1.772375226020813, val_acc: 0.2824000120162964,  lr: 0.00034
step: 35, train_loss: 1.7723191976547241, acc: 0.2840000092983246, val_loss: 1.7700577974319458, val_acc: 0.28700000047683716,  lr: 0.00035000000000000005
step: 36, train_loss: 1.7702761888504028, acc: 0.2825999855995178, val_loss: 1.7674676179885864, val_acc: 0.3012000024318695,  lr: 0.00035999999999999997
step: 37, train_loss: 1.7682833671569824, acc: 0.29319998621940613, val_loss: 1.7645328044891357, val_acc: 0.3142000138759613,  lr: 0.00037
step: 38, train_loss: 1.7656275033950806, acc: 0.2888000011444092, val_loss: 1.7614482641220093, val_acc: 0.3109999895095825,  lr: 0.00038
step: 39, train_loss: 1.7620689868927002, acc: 0.2935999929904938, val_loss: 1.7579820156097412, val_acc: 0.30720001459121704,  lr: 0.00039
step: 40, train_loss: 1.7570470571517944, acc: 0.28540000319480896, val_loss: 1.7537040710449219, val_acc: 0.3068000078201294,  lr: 0.0004
step: 41, train_loss: 1.753625750541687, acc: 0.2825999855995178, val_loss: 1.7485287189483643, val_acc: 0.30979999899864197,  lr: 0.00041000000000000005
step: 42, train_loss: 1.7489341497421265, acc: 0.2809999883174896, val_loss: 1.743013620376587, val_acc: 0.2815999984741211,  lr: 0.00042
step: 43, train_loss: 1.7444416284561157, acc: 0.2720000147819519, val_loss: 1.7367644309997559, val_acc: 0.2773999869823456,  lr: 0.00043
step: 44, train_loss: 1.7403937578201294, acc: 0.2669999897480011, val_loss: 1.7298657894134521, val_acc: 0.30320000648498535,  lr: 0.00043999999999999996
step: 45, train_loss: 1.7315607070922852, acc: 0.28060001134872437, val_loss: 1.723106861114502, val_acc: 0.2978000044822693,  lr: 0.00045
step: 46, train_loss: 1.7230664491653442, acc: 0.2913999855518341, val_loss: 1.7158470153808594, val_acc: 0.30480000376701355,  lr: 0.00046
step: 47, train_loss: 1.7183446884155273, acc: 0.2879999876022339, val_loss: 1.7076427936553955, val_acc: 0.3019999861717224,  lr: 0.00047
step: 48, train_loss: 1.7096529006958008, acc: 0.287200003862381, val_loss: 1.7016255855560303, val_acc: 0.2736000120639801,  lr: 0.00048
step: 49, train_loss: 1.7053782939910889, acc: 0.27219998836517334, val_loss: 1.7033761739730835, val_acc: 0.25519999861717224,  lr: 0.00049
step: 50, train_loss: 1.7069164514541626, acc: 0.25780001282691956, val_loss: 1.6923402547836304, val_acc: 0.29919999837875366,  lr: 0.0005
step: 51, train_loss: 1.6931978464126587, acc: 0.28780001401901245, val_loss: 1.691119909286499, val_acc: 0.3009999990463257,  lr: 0.0005099999999999999
step: 52, train_loss: 1.694096326828003, acc: 0.2903999984264374, val_loss: 1.690677285194397, val_acc: 0.2964000105857849,  lr: 0.00052
step: 53, train_loss: 1.6896394491195679, acc: 0.29179999232292175, val_loss: 1.6886757612228394, val_acc: 0.3001999855041504,  lr: 0.00053
step: 54, train_loss: 1.689192533493042, acc: 0.29679998755455017, val_loss: 1.680175542831421, val_acc: 0.31540000438690186,  lr: 0.00054
step: 55, train_loss: 1.6841425895690918, acc: 0.3057999908924103, val_loss: 1.6802504062652588, val_acc: 0.3124000132083893,  lr: 0.00055
step: 56, train_loss: 1.6838207244873047, acc: 0.30300000309944153, val_loss: 1.6744776964187622, val_acc: 0.3093999922275543,  lr: 0.0005600000000000001
step: 57, train_loss: 1.6769903898239136, acc: 0.298799991607666, val_loss: 1.6717841625213623, val_acc: 0.3068000078201294,  lr: 0.0005700000000000001
step: 58, train_loss: 1.6698719263076782, acc: 0.2980000078678131, val_loss: 1.6691046953201294, val_acc: 0.3061999976634979,  lr: 0.00058
step: 59, train_loss: 1.667712688446045, acc: 0.30079999566078186, val_loss: 1.66251802444458, val_acc: 0.3107999861240387,  lr: 0.00059
step: 60, train_loss: 1.6634005308151245, acc: 0.3077999949455261, val_loss: 1.6559103727340698, val_acc: 0.31380000710487366,  lr: 0.0006
step: 61, train_loss: 1.6609978675842285, acc: 0.29660001397132874, val_loss: 1.650862455368042, val_acc: 0.3084000051021576,  lr: 0.00061
step: 62, train_loss: 1.6516039371490479, acc: 0.301800012588501, val_loss: 1.6480892896652222, val_acc: 0.29899999499320984,  lr: 0.00062
step: 63, train_loss: 1.644010305404663, acc: 0.296999990940094, val_loss: 1.6420931816101074, val_acc: 0.3025999963283539,  lr: 0.00063
step: 64, train_loss: 1.6433240175247192, acc: 0.30379998683929443, val_loss: 1.6361273527145386, val_acc: 0.3125999867916107,  lr: 0.00064
step: 65, train_loss: 1.638303279876709, acc: 0.30880001187324524, val_loss: 1.6299362182617188, val_acc: 0.3176000118255615,  lr: 0.0006500000000000001
step: 66, train_loss: 1.631569743156433, acc: 0.31279999017715454, val_loss: 1.6292468309402466, val_acc: 0.31220000982284546,  lr: 0.00066
step: 67, train_loss: 1.6357697248458862, acc: 0.3041999936103821, val_loss: 1.626776099205017, val_acc: 0.30480000376701355,  lr: 0.00067
step: 68, train_loss: 1.6269304752349854, acc: 0.3086000084877014, val_loss: 1.6246646642684937, val_acc: 0.3190000057220459,  lr: 0.00068
step: 69, train_loss: 1.6229888200759888, acc: 0.31220000982284546, val_loss: 1.6265699863433838, val_acc: 0.3181999921798706,  lr: 0.0006900000000000001
step: 70, train_loss: 1.6247447729110718, acc: 0.31520000100135803, val_loss: 1.6263418197631836, val_acc: 0.3176000118255615,  lr: 0.0007000000000000001
step: 71, train_loss: 1.626914381980896, acc: 0.3046000003814697, val_loss: 1.6232922077178955, val_acc: 0.31139999628067017,  lr: 0.0007099999999999999
step: 72, train_loss: 1.6254613399505615, acc: 0.31520000100135803, val_loss: 1.6221051216125488, val_acc: 0.31380000710487366,  lr: 0.0007199999999999999
step: 73, train_loss: 1.6220039129257202, acc: 0.3102000057697296, val_loss: 1.6192595958709717, val_acc: 0.31520000100135803,  lr: 0.00073
step: 74, train_loss: 1.6173999309539795, acc: 0.3156000077724457, val_loss: 1.6191351413726807, val_acc: 0.31279999017715454,  lr: 0.00074
step: 75, train_loss: 1.6177127361297607, acc: 0.31700000166893005, val_loss: 1.616606593132019, val_acc: 0.3197999894618988,  lr: 0.00075
step: 76, train_loss: 1.6185684204101562, acc: 0.31940001249313354, val_loss: 1.616940975189209, val_acc: 0.31779998540878296,  lr: 0.00076
step: 77, train_loss: 1.6173757314682007, acc: 0.3165999948978424, val_loss: 1.6129909753799438, val_acc: 0.32339999079704285,  lr: 0.00077
step: 78, train_loss: 1.6143163442611694, acc: 0.32499998807907104, val_loss: 1.6115206480026245, val_acc: 0.32199999690055847,  lr: 0.00078
step: 79, train_loss: 1.6102858781814575, acc: 0.3179999887943268, val_loss: 1.6104388236999512, val_acc: 0.3158000111579895,  lr: 0.00079
step: 80, train_loss: 1.6101782321929932, acc: 0.321399986743927, val_loss: 1.607737421989441, val_acc: 0.3165999948978424,  lr: 0.0008
step: 81, train_loss: 1.6063264608383179, acc: 0.321399986743927, val_loss: 1.609734058380127, val_acc: 0.319599986076355,  lr: 0.0008100000000000001
step: 82, train_loss: 1.6102675199508667, acc: 0.31119999289512634, val_loss: 1.6015501022338867, val_acc: 0.3237999975681305,  lr: 0.0008200000000000001
step: 83, train_loss: 1.6020870208740234, acc: 0.32499998807907104, val_loss: 1.600340485572815, val_acc: 0.32519999146461487,  lr: 0.00083
step: 84, train_loss: 1.595563292503357, acc: 0.3230000138282776, val_loss: 1.5966904163360596, val_acc: 0.3271999955177307,  lr: 0.00084
step: 85, train_loss: 1.5964845418930054, acc: 0.329800009727478, val_loss: 1.5944340229034424, val_acc: 0.32899999618530273,  lr: 0.0008500000000000001
step: 86, train_loss: 1.5950331687927246, acc: 0.3278000056743622, val_loss: 1.5916156768798828, val_acc: 0.33719998598098755,  lr: 0.00086
step: 87, train_loss: 1.5969173908233643, acc: 0.3285999894142151, val_loss: 1.5895190238952637, val_acc: 0.33559998869895935,  lr: 0.00087
step: 88, train_loss: 1.587167739868164, acc: 0.3328000009059906, val_loss: 1.5879297256469727, val_acc: 0.3330000042915344,  lr: 0.0008799999999999999
step: 89, train_loss: 1.5893652439117432, acc: 0.32919999957084656, val_loss: 1.5853300094604492, val_acc: 0.33899998664855957,  lr: 0.00089
step: 90, train_loss: 1.5804603099822998, acc: 0.3386000096797943, val_loss: 1.584905743598938, val_acc: 0.3424000144004822,  lr: 0.0009
step: 91, train_loss: 1.5792497396469116, acc: 0.33480000495910645, val_loss: 1.5787447690963745, val_acc: 0.3434000015258789,  lr: 0.00091
step: 92, train_loss: 1.5857142210006714, acc: 0.34139999747276306, val_loss: 1.581760287284851, val_acc: 0.34060001373291016,  lr: 0.00092
step: 93, train_loss: 1.5810863971710205, acc: 0.32899999618530273, val_loss: 1.5733832120895386, val_acc: 0.34360000491142273,  lr: 0.00093
step: 94, train_loss: 1.5706239938735962, acc: 0.3416000008583069, val_loss: 1.5766985416412354, val_acc: 0.3472000062465668,  lr: 0.00094
step: 95, train_loss: 1.5796278715133667, acc: 0.3330000042915344, val_loss: 1.5734646320343018, val_acc: 0.3481999933719635,  lr: 0.00095
step: 96, train_loss: 1.5714308023452759, acc: 0.3440000116825104, val_loss: 1.572343111038208, val_acc: 0.3476000130176544,  lr: 0.00096
step: 97, train_loss: 1.57296884059906, acc: 0.3472000062465668, val_loss: 1.563644289970398, val_acc: 0.35600000619888306,  lr: 0.00097
step: 98, train_loss: 1.565509557723999, acc: 0.34360000491142273, val_loss: 1.5631128549575806, val_acc: 0.3499999940395355,  lr: 0.00098
step: 99, train_loss: 1.5631154775619507, acc: 0.3418000042438507, val_loss: 1.5570108890533447, val_acc: 0.35580000281333923,  lr: 0.00099
step: 100, train_loss: 1.5635801553726196, acc: 0.3474000096321106, val_loss: 1.5604701042175293, val_acc: 0.3513999879360199,  lr: 0.001
step: 101, train_loss: 1.5633143186569214, acc: 0.34860000014305115, val_loss: 1.5506551265716553, val_acc: 0.35760000348091125,  lr: 0.00101
step: 102, train_loss: 1.5515943765640259, acc: 0.3529999852180481, val_loss: 1.5496898889541626, val_acc: 0.36000001430511475,  lr: 0.0010199999999999999
step: 103, train_loss: 1.5467445850372314, acc: 0.3555999994277954, val_loss: 1.5450308322906494, val_acc: 0.3614000082015991,  lr: 0.0010299999999999999
step: 104, train_loss: 1.5428494215011597, acc: 0.35339999198913574, val_loss: 1.5445729494094849, val_acc: 0.36000001430511475,  lr: 0.00104
step: 105, train_loss: 1.548431396484375, acc: 0.3479999899864197, val_loss: 1.5444726943969727, val_acc: 0.350600004196167,  lr: 0.00105
step: 106, train_loss: 1.554931640625, acc: 0.34940001368522644, val_loss: 1.5385607481002808, val_acc: 0.3564000129699707,  lr: 0.00106
step: 107, train_loss: 1.5504921674728394, acc: 0.35199999809265137, val_loss: 1.5373159646987915, val_acc: 0.36059999465942383,  lr: 0.00107
step: 108, train_loss: 1.546566128730774, acc: 0.350600004196167, val_loss: 1.537474274635315, val_acc: 0.3546000123023987,  lr: 0.00108
step: 109, train_loss: 1.5337843894958496, acc: 0.3547999858856201, val_loss: 1.536301612854004, val_acc: 0.3562000095844269,  lr: 0.00109
step: 110, train_loss: 1.5356673002243042, acc: 0.358599990606308, val_loss: 1.5352811813354492, val_acc: 0.3587999939918518,  lr: 0.0011
step: 111, train_loss: 1.5402014255523682, acc: 0.35420000553131104, val_loss: 1.5341713428497314, val_acc: 0.35580000281333923,  lr: 0.00111
step: 112, train_loss: 1.5400463342666626, acc: 0.3562000095844269, val_loss: 1.5341200828552246, val_acc: 0.3625999987125397,  lr: 0.0011200000000000001
step: 113, train_loss: 1.5372555255889893, acc: 0.35679998993873596, val_loss: 1.5308505296707153, val_acc: 0.3587999939918518,  lr: 0.0011300000000000001
step: 114, train_loss: 1.5234434604644775, acc: 0.3594000041484833, val_loss: 1.530595064163208, val_acc: 0.3619999885559082,  lr: 0.0011400000000000002
step: 115, train_loss: 1.5341235399246216, acc: 0.3635999858379364, val_loss: 1.530732274055481, val_acc: 0.36480000615119934,  lr: 0.00115
step: 116, train_loss: 1.5326017141342163, acc: 0.3598000109195709, val_loss: 1.529533863067627, val_acc: 0.36059999465942383,  lr: 0.00116
step: 117, train_loss: 1.5295213460922241, acc: 0.35499998927116394, val_loss: 1.5268361568450928, val_acc: 0.36320000886917114,  lr: 0.00117
step: 118, train_loss: 1.5326268672943115, acc: 0.3598000109195709, val_loss: 1.5261934995651245, val_acc: 0.3619999885559082,  lr: 0.00118
step: 119, train_loss: 1.5312646627426147, acc: 0.35679998993873596, val_loss: 1.5228012800216675, val_acc: 0.362199991941452,  lr: 0.0011899999999999999
step: 120, train_loss: 1.519818902015686, acc: 0.36160001158714294, val_loss: 1.5216128826141357, val_acc: 0.36419999599456787,  lr: 0.0012
step: 121, train_loss: 1.5223278999328613, acc: 0.3601999878883362, val_loss: 1.5193759202957153, val_acc: 0.36660000681877136,  lr: 0.00121
step: 122, train_loss: 1.5256589651107788, acc: 0.35679998993873596, val_loss: 1.5213871002197266, val_acc: 0.3625999987125397,  lr: 0.00122
step: 123, train_loss: 1.515811562538147, acc: 0.3601999878883362, val_loss: 1.5199215412139893, val_acc: 0.3628000020980835,  lr: 0.00123
step: 124, train_loss: 1.5236278772354126, acc: 0.36079999804496765, val_loss: 1.5178232192993164, val_acc: 0.36239999532699585,  lr: 0.00124
step: 125, train_loss: 1.5131022930145264, acc: 0.3614000082015991, val_loss: 1.5153803825378418, val_acc: 0.3668000102043152,  lr: 0.00125
step: 126, train_loss: 1.5138741731643677, acc: 0.3596000075340271, val_loss: 1.5136075019836426, val_acc: 0.3675999939441681,  lr: 0.00126
step: 127, train_loss: 1.5144634246826172, acc: 0.36399999260902405, val_loss: 1.512144923210144, val_acc: 0.3643999993801117,  lr: 0.00127
step: 128, train_loss: 1.5151188373565674, acc: 0.3677999973297119, val_loss: 1.5119647979736328, val_acc: 0.36320000886917114,  lr: 0.00128
step: 129, train_loss: 1.5162655115127563, acc: 0.3580000102519989, val_loss: 1.5144661664962769, val_acc: 0.36239999532699585,  lr: 0.0012900000000000001
step: 130, train_loss: 1.5147844552993774, acc: 0.3628000020980835, val_loss: 1.5115938186645508, val_acc: 0.365200012922287,  lr: 0.0013000000000000002
step: 131, train_loss: 1.5113404989242554, acc: 0.36320000886917114, val_loss: 1.5167858600616455, val_acc: 0.3625999987125397,  lr: 0.0013100000000000002
step: 132, train_loss: 1.5233925580978394, acc: 0.35899999737739563, val_loss: 1.5078684091567993, val_acc: 0.3691999912261963,  lr: 0.00132
step: 133, train_loss: 1.5142179727554321, acc: 0.3723999857902527, val_loss: 1.5117779970169067, val_acc: 0.3668000102043152,  lr: 0.00133
step: 134, train_loss: 1.508426308631897, acc: 0.365200012922287, val_loss: 1.5083154439926147, val_acc: 0.36340001225471497,  lr: 0.00134
step: 135, train_loss: 1.5002384185791016, acc: 0.3709999918937683, val_loss: 1.507969856262207, val_acc: 0.36559998989105225,  lr: 0.00135
step: 136, train_loss: 1.5096105337142944, acc: 0.3675999939441681, val_loss: 1.509108066558838, val_acc: 0.36660000681877136,  lr: 0.00136
step: 137, train_loss: 1.509883165359497, acc: 0.365200012922287, val_loss: 1.5024685859680176, val_acc: 0.37059998512268066,  lr: 0.0013700000000000001
step: 138, train_loss: 1.502835988998413, acc: 0.3637999892234802, val_loss: 1.5075281858444214, val_acc: 0.36719998717308044,  lr: 0.0013800000000000002
step: 139, train_loss: 1.4997907876968384, acc: 0.36559998989105225, val_loss: 1.503891944885254, val_acc: 0.37380000948905945,  lr: 0.0013900000000000002
step: 140, train_loss: 1.505038857460022, acc: 0.36079999804496765, val_loss: 1.5070101022720337, val_acc: 0.3693999946117401,  lr: 0.0014000000000000002
step: 141, train_loss: 1.4984320402145386, acc: 0.36880001425743103, val_loss: 1.5029489994049072, val_acc: 0.36980000138282776,  lr: 0.0014099999999999998
step: 142, train_loss: 1.5031318664550781, acc: 0.3720000088214874, val_loss: 1.5110440254211426, val_acc: 0.3675999939441681,  lr: 0.0014199999999999998
step: 143, train_loss: 1.5100464820861816, acc: 0.3675999939441681, val_loss: 1.5027214288711548, val_acc: 0.3725999891757965,  lr: 0.0014299999999999998
step: 144, train_loss: 1.5012104511260986, acc: 0.3709999918937683, val_loss: 1.503373384475708, val_acc: 0.367000013589859,  lr: 0.0014399999999999999
step: 145, train_loss: 1.5076818466186523, acc: 0.3569999933242798, val_loss: 1.511855125427246, val_acc: 0.3625999987125397,  lr: 0.00145
step: 146, train_loss: 1.513706922531128, acc: 0.35659998655319214, val_loss: 1.5045405626296997, val_acc: 0.36980000138282776,  lr: 0.00146
step: 147, train_loss: 1.5047872066497803, acc: 0.36079999804496765, val_loss: 1.5008671283721924, val_acc: 0.3723999857902527,  lr: 0.00147
step: 148, train_loss: 1.5022743940353394, acc: 0.3691999912261963, val_loss: 1.4998332262039185, val_acc: 0.3684000074863434,  lr: 0.00148
step: 149, train_loss: 1.4961150884628296, acc: 0.3677999973297119, val_loss: 1.4982260465621948, val_acc: 0.373199999332428,  lr: 0.00149
step: 150, train_loss: 1.5028197765350342, acc: 0.36579999327659607, val_loss: 1.497730016708374, val_acc: 0.37139999866485596,  lr: 0.0015
step: 151, train_loss: 1.4981087446212769, acc: 0.37279999256134033, val_loss: 1.4998226165771484, val_acc: 0.3716000020503998,  lr: 0.00151
step: 152, train_loss: 1.49735426902771, acc: 0.3659999966621399, val_loss: 1.497589111328125, val_acc: 0.37119999527931213,  lr: 0.00152
step: 153, train_loss: 1.4861531257629395, acc: 0.3720000088214874, val_loss: 1.4938386678695679, val_acc: 0.37220001220703125,  lr: 0.0015300000000000001
step: 154, train_loss: 1.4865468740463257, acc: 0.36719998717308044, val_loss: 1.4983632564544678, val_acc: 0.37119999527931213,  lr: 0.00154
step: 155, train_loss: 1.4942512512207031, acc: 0.37279999256134033, val_loss: 1.493256688117981, val_acc: 0.3723999857902527,  lr: 0.00155
step: 156, train_loss: 1.4925286769866943, acc: 0.3691999912261963, val_loss: 1.4930522441864014, val_acc: 0.37119999527931213,  lr: 0.00156
step: 157, train_loss: 1.4912692308425903, acc: 0.37619999051094055, val_loss: 1.4946562051773071, val_acc: 0.36980000138282776,  lr: 0.00157
step: 158, train_loss: 1.4966648817062378, acc: 0.36899998784065247, val_loss: 1.4866797924041748, val_acc: 0.37700000405311584,  lr: 0.00158
step: 159, train_loss: 1.487286925315857, acc: 0.3686000108718872, val_loss: 1.4897606372833252, val_acc: 0.3725999891757965,  lr: 0.00159
step: 160, train_loss: 1.4890971183776855, acc: 0.3736000061035156, val_loss: 1.4883960485458374, val_acc: 0.3684000074863434,  lr: 0.0016
step: 161, train_loss: 1.4793460369110107, acc: 0.3675999939441681, val_loss: 1.4883700609207153, val_acc: 0.37220001220703125,  lr: 0.00161
step: 162, train_loss: 1.4904212951660156, acc: 0.37279999256134033, val_loss: 1.4912668466567993, val_acc: 0.37540000677108765,  lr: 0.0016200000000000001
step: 163, train_loss: 1.4896528720855713, acc: 0.36980000138282776, val_loss: 1.4911723136901855, val_acc: 0.373199999332428,  lr: 0.0016300000000000002
step: 164, train_loss: 1.4934513568878174, acc: 0.36660000681877136, val_loss: 1.4837700128555298, val_acc: 0.3736000061035156,  lr: 0.0016400000000000002
step: 165, train_loss: 1.4853734970092773, acc: 0.3718000054359436, val_loss: 1.4878661632537842, val_acc: 0.3709999918937683,  lr: 0.0016500000000000002
step: 166, train_loss: 1.4959830045700073, acc: 0.3628000020980835, val_loss: 1.4813504219055176, val_acc: 0.37860000133514404,  lr: 0.00166
step: 167, train_loss: 1.478158712387085, acc: 0.373199999332428, val_loss: 1.4813932180404663, val_acc: 0.37779998779296875,  lr: 0.00167
step: 168, train_loss: 1.4838051795959473, acc: 0.3808000087738037, val_loss: 1.4812848567962646, val_acc: 0.37860000133514404,  lr: 0.00168
step: 169, train_loss: 1.48075532913208, acc: 0.3765999972820282, val_loss: 1.480500340461731, val_acc: 0.37540000677108765,  lr: 0.00169
step: 170, train_loss: 1.4837032556533813, acc: 0.37720000743865967, val_loss: 1.4773544073104858, val_acc: 0.3779999911785126,  lr: 0.0017000000000000001
step: 171, train_loss: 1.4806369543075562, acc: 0.3752000033855438, val_loss: 1.4771637916564941, val_acc: 0.3815999925136566,  lr: 0.0017100000000000001
step: 172, train_loss: 1.4760794639587402, acc: 0.37959998846054077, val_loss: 1.4758834838867188, val_acc: 0.3792000114917755,  lr: 0.00172
step: 173, train_loss: 1.4716606140136719, acc: 0.37779998779296875, val_loss: 1.476426124572754, val_acc: 0.3783999979496002,  lr: 0.00173
step: 174, train_loss: 1.4787291288375854, acc: 0.3693999946117401, val_loss: 1.4740402698516846, val_acc: 0.38179999589920044,  lr: 0.00174
step: 175, train_loss: 1.4840360879898071, acc: 0.365200012922287, val_loss: 1.4723440408706665, val_acc: 0.3808000087738037,  lr: 0.0017499999999999998
step: 176, train_loss: 1.4692866802215576, acc: 0.37279999256134033, val_loss: 1.47355318069458, val_acc: 0.3792000114917755,  lr: 0.0017599999999999998
step: 177, train_loss: 1.4779093265533447, acc: 0.3720000088214874, val_loss: 1.470447063446045, val_acc: 0.3808000087738037,  lr: 0.0017699999999999999
step: 178, train_loss: 1.473845362663269, acc: 0.3734000027179718, val_loss: 1.47040593624115, val_acc: 0.37860000133514404,  lr: 0.00178
step: 179, train_loss: 1.4603914022445679, acc: 0.3837999999523163, val_loss: 1.4698065519332886, val_acc: 0.3808000087738037,  lr: 0.00179
step: 180, train_loss: 1.4716346263885498, acc: 0.37959998846054077, val_loss: 1.4692596197128296, val_acc: 0.3815999925136566,  lr: 0.0018
step: 181, train_loss: 1.4702192544937134, acc: 0.37439998984336853, val_loss: 1.4691874980926514, val_acc: 0.3824000060558319,  lr: 0.00181
step: 182, train_loss: 1.4725064039230347, acc: 0.3734000027179718, val_loss: 1.465700626373291, val_acc: 0.38179999589920044,  lr: 0.00182
step: 183, train_loss: 1.4675211906433105, acc: 0.37439998984336853, val_loss: 1.4674150943756104, val_acc: 0.38179999589920044,  lr: 0.00183
step: 184, train_loss: 1.4727559089660645, acc: 0.37220001220703125, val_loss: 1.4702492952346802, val_acc: 0.3758000135421753,  lr: 0.00184
step: 185, train_loss: 1.4696779251098633, acc: 0.37959998846054077, val_loss: 1.4635698795318604, val_acc: 0.38260000944137573,  lr: 0.00185
step: 186, train_loss: 1.4643547534942627, acc: 0.37860000133514404, val_loss: 1.4656072854995728, val_acc: 0.38679999113082886,  lr: 0.00186
step: 187, train_loss: 1.4627301692962646, acc: 0.37860000133514404, val_loss: 1.4623112678527832, val_acc: 0.3813999891281128,  lr: 0.0018700000000000001
step: 188, train_loss: 1.4550466537475586, acc: 0.3856000006198883, val_loss: 1.4692991971969604, val_acc: 0.37720000743865967,  lr: 0.00188
step: 189, train_loss: 1.4644227027893066, acc: 0.37400001287460327, val_loss: 1.4669309854507446, val_acc: 0.3806000053882599,  lr: 0.00189
step: 190, train_loss: 1.4622271060943604, acc: 0.38679999113082886, val_loss: 1.4629632234573364, val_acc: 0.38260000944137573,  lr: 0.0019
step: 191, train_loss: 1.4621055126190186, acc: 0.37619999051094055, val_loss: 1.461180329322815, val_acc: 0.38100001215934753,  lr: 0.00191
step: 192, train_loss: 1.4506566524505615, acc: 0.3822000026702881, val_loss: 1.463248372077942, val_acc: 0.37880000472068787,  lr: 0.00192
step: 193, train_loss: 1.4558831453323364, acc: 0.3797999918460846, val_loss: 1.4645359516143799, val_acc: 0.3758000135421753,  lr: 0.00193
step: 194, train_loss: 1.4690091609954834, acc: 0.3716000020503998, val_loss: 1.4630967378616333, val_acc: 0.3822000026702881,  lr: 0.00194
step: 195, train_loss: 1.4455175399780273, acc: 0.3887999951839447, val_loss: 1.4608650207519531, val_acc: 0.38179999589920044,  lr: 0.0019500000000000001
step: 196, train_loss: 1.4604158401489258, acc: 0.387800008058548, val_loss: 1.4605590105056763, val_acc: 0.3808000087738037,  lr: 0.00196
step: 197, train_loss: 1.4576635360717773, acc: 0.37959998846054077, val_loss: 1.4593939781188965, val_acc: 0.3790000081062317,  lr: 0.00197
step: 198, train_loss: 1.4585767984390259, acc: 0.38339999318122864, val_loss: 1.462506890296936, val_acc: 0.37619999051094055,  lr: 0.00198
step: 199, train_loss: 1.461307168006897, acc: 0.37619999051094055, val_loss: 1.4604424238204956, val_acc: 0.38040000200271606,  lr: 0.00199
step: 200, train_loss: 1.458868145942688, acc: 0.38420000672340393, val_loss: 1.4552091360092163, val_acc: 0.38519999384880066,  lr: 0.002
step: 201, train_loss: 1.4532560110092163, acc: 0.38359999656677246, val_loss: 1.4604889154434204, val_acc: 0.37560001015663147,  lr: 0.00201
step: 202, train_loss: 1.4562695026397705, acc: 0.37720000743865967, val_loss: 1.4606214761734009, val_acc: 0.3837999999523163,  lr: 0.00202
step: 203, train_loss: 1.4589557647705078, acc: 0.3790000081062317, val_loss: 1.4553775787353516, val_acc: 0.38499999046325684,  lr: 0.00203
step: 204, train_loss: 1.4530909061431885, acc: 0.3837999999523163, val_loss: 1.4591772556304932, val_acc: 0.3822000026702881,  lr: 0.0020399999999999997
step: 205, train_loss: 1.4454628229141235, acc: 0.38600000739097595, val_loss: 1.4549165964126587, val_acc: 0.37959998846054077,  lr: 0.0020499999999999997
step: 206, train_loss: 1.4467679262161255, acc: 0.3901999890804291, val_loss: 1.458274245262146, val_acc: 0.37940001487731934,  lr: 0.0020599999999999998
step: 207, train_loss: 1.460252285003662, acc: 0.3776000142097473, val_loss: 1.4572045803070068, val_acc: 0.38420000672340393,  lr: 0.00207
step: 208, train_loss: 1.465357780456543, acc: 0.3792000114917755, val_loss: 1.4509575366973877, val_acc: 0.3882000148296356,  lr: 0.00208
step: 209, train_loss: 1.4449843168258667, acc: 0.3822000026702881, val_loss: 1.4597212076187134, val_acc: 0.37860000133514404,  lr: 0.00209
step: 210, train_loss: 1.4432770013809204, acc: 0.38839998841285706, val_loss: 1.45354163646698, val_acc: 0.38179999589920044,  lr: 0.0021
step: 211, train_loss: 1.4525090456008911, acc: 0.3808000087738037, val_loss: 1.4520015716552734, val_acc: 0.37959998846054077,  lr: 0.00211
step: 212, train_loss: 1.448285698890686, acc: 0.38420000672340393, val_loss: 1.4483261108398438, val_acc: 0.38679999113082886,  lr: 0.00212
step: 213, train_loss: 1.4435161352157593, acc: 0.3871999979019165, val_loss: 1.4499980211257935, val_acc: 0.38339999318122864,  lr: 0.00213
step: 214, train_loss: 1.4363950490951538, acc: 0.38999998569488525, val_loss: 1.452501654624939, val_acc: 0.38499999046325684,  lr: 0.00214
step: 215, train_loss: 1.443760633468628, acc: 0.3864000141620636, val_loss: 1.4448893070220947, val_acc: 0.3880000114440918,  lr: 0.00215
step: 216, train_loss: 1.4424142837524414, acc: 0.3874000012874603, val_loss: 1.4547306299209595, val_acc: 0.3840000033378601,  lr: 0.00216
step: 217, train_loss: 1.4546833038330078, acc: 0.37779998779296875, val_loss: 1.4451206922531128, val_acc: 0.38499999046325684,  lr: 0.00217
step: 218, train_loss: 1.449057936668396, acc: 0.38119998574256897, val_loss: 1.4475566148757935, val_acc: 0.38420000672340393,  lr: 0.00218
step: 219, train_loss: 1.4453386068344116, acc: 0.3815999925136566, val_loss: 1.4515588283538818, val_acc: 0.3781999945640564,  lr: 0.00219
step: 220, train_loss: 1.4345283508300781, acc: 0.39500001072883606, val_loss: 1.444014549255371, val_acc: 0.38440001010894775,  lr: 0.0022
step: 221, train_loss: 1.4312351942062378, acc: 0.3846000134944916, val_loss: 1.4479135274887085, val_acc: 0.3837999999523163,  lr: 0.00221
step: 222, train_loss: 1.4359091520309448, acc: 0.3862000107765198, val_loss: 1.4470018148422241, val_acc: 0.38519999384880066,  lr: 0.00222
step: 223, train_loss: 1.435887098312378, acc: 0.38339999318122864, val_loss: 1.4432215690612793, val_acc: 0.3853999972343445,  lr: 0.00223
step: 224, train_loss: 1.4427686929702759, acc: 0.3846000134944916, val_loss: 1.4462368488311768, val_acc: 0.3846000134944916,  lr: 0.0022400000000000002
step: 225, train_loss: 1.4395490884780884, acc: 0.3880000114440918, val_loss: 1.4433099031448364, val_acc: 0.38499999046325684,  lr: 0.0022500000000000003
step: 226, train_loss: 1.4359291791915894, acc: 0.38659998774528503, val_loss: 1.442070484161377, val_acc: 0.3880000114440918,  lr: 0.0022600000000000003
step: 227, train_loss: 1.4323269128799438, acc: 0.3822000026702881, val_loss: 1.4392610788345337, val_acc: 0.3921999931335449,  lr: 0.0022700000000000003
step: 228, train_loss: 1.4234020709991455, acc: 0.39239999651908875, val_loss: 1.4418219327926636, val_acc: 0.3880000114440918,  lr: 0.0022800000000000003
step: 229, train_loss: 1.4292545318603516, acc: 0.3864000141620636, val_loss: 1.4419413805007935, val_acc: 0.38940000534057617,  lr: 0.00229
step: 230, train_loss: 1.4394360780715942, acc: 0.3885999917984009, val_loss: 1.4400253295898438, val_acc: 0.3898000121116638,  lr: 0.0023
step: 231, train_loss: 1.4346266984939575, acc: 0.3806000053882599, val_loss: 1.4387954473495483, val_acc: 0.39079999923706055,  lr: 0.00231
step: 232, train_loss: 1.44590425491333, acc: 0.37959998846054077, val_loss: 1.440285563468933, val_acc: 0.387800008058548,  lr: 0.00232
step: 233, train_loss: 1.4333535432815552, acc: 0.38760000467300415, val_loss: 1.4383885860443115, val_acc: 0.39320001006126404,  lr: 0.00233
step: 234, train_loss: 1.4331772327423096, acc: 0.3864000141620636, val_loss: 1.437066674232483, val_acc: 0.3901999890804291,  lr: 0.00234
step: 235, train_loss: 1.4167792797088623, acc: 0.3953999876976013, val_loss: 1.437885046005249, val_acc: 0.38679999113082886,  lr: 0.00235
step: 236, train_loss: 1.4373664855957031, acc: 0.3822000026702881, val_loss: 1.4351917505264282, val_acc: 0.38839998841285706,  lr: 0.00236
step: 237, train_loss: 1.4228097200393677, acc: 0.3953999876976013, val_loss: 1.4354817867279053, val_acc: 0.39340001344680786,  lr: 0.00237
step: 238, train_loss: 1.429444670677185, acc: 0.3889999985694885, val_loss: 1.4385474920272827, val_acc: 0.3853999972343445,  lr: 0.0023799999999999997
step: 239, train_loss: 1.4197742938995361, acc: 0.387800008058548, val_loss: 1.4372923374176025, val_acc: 0.39640000462532043,  lr: 0.0023899999999999998
step: 240, train_loss: 1.4366376399993896, acc: 0.38420000672340393, val_loss: 1.4428889751434326, val_acc: 0.384799987077713,  lr: 0.0024
step: 241, train_loss: 1.4514693021774292, acc: 0.37959998846054077, val_loss: 1.4361768960952759, val_acc: 0.391400009393692,  lr: 0.00241
step: 242, train_loss: 1.4330617189407349, acc: 0.3871999979019165, val_loss: 1.4338406324386597, val_acc: 0.3882000148296356,  lr: 0.00242
step: 243, train_loss: 1.4283961057662964, acc: 0.39100000262260437, val_loss: 1.4326223134994507, val_acc: 0.39079999923706055,  lr: 0.00243
step: 244, train_loss: 1.4132585525512695, acc: 0.39399999380111694, val_loss: 1.4304510354995728, val_acc: 0.39160001277923584,  lr: 0.00244
step: 245, train_loss: 1.428003191947937, acc: 0.3905999958515167, val_loss: 1.4327597618103027, val_acc: 0.3919999897480011,  lr: 0.00245
step: 246, train_loss: 1.428368091583252, acc: 0.3869999945163727, val_loss: 1.430291771888733, val_acc: 0.3944000005722046,  lr: 0.00246
step: 247, train_loss: 1.4143791198730469, acc: 0.3944000005722046, val_loss: 1.4292874336242676, val_acc: 0.3889999985694885,  lr: 0.00247
step: 248, train_loss: 1.413595199584961, acc: 0.39160001277923584, val_loss: 1.4317593574523926, val_acc: 0.3930000066757202,  lr: 0.00248
step: 249, train_loss: 1.4347100257873535, acc: 0.384799987077713, val_loss: 1.4308559894561768, val_acc: 0.38940000534057617,  lr: 0.00249
step: 250, train_loss: 1.4330013990402222, acc: 0.38679999113082886, val_loss: 1.4333962202072144, val_acc: 0.3889999985694885,  lr: 0.0025
step: 251, train_loss: 1.4357506036758423, acc: 0.3882000148296356, val_loss: 1.4522897005081177, val_acc: 0.37700000405311584,  lr: 0.00251
step: 252, train_loss: 1.4629894495010376, acc: 0.37299999594688416, val_loss: 1.447490930557251, val_acc: 0.384799987077713,  lr: 0.00252
step: 253, train_loss: 1.4493889808654785, acc: 0.38119998574256897, val_loss: 1.4369761943817139, val_acc: 0.3831999897956848,  lr: 0.00253
step: 254, train_loss: 1.4329787492752075, acc: 0.3862000107765198, val_loss: 1.4333170652389526, val_acc: 0.3864000141620636,  lr: 0.00254
step: 255, train_loss: 1.4310976266860962, acc: 0.38040000200271606, val_loss: 1.4350987672805786, val_acc: 0.3880000114440918,  lr: 0.00255
step: 256, train_loss: 1.4315035343170166, acc: 0.38760000467300415, val_loss: 1.4353725910186768, val_acc: 0.3846000134944916,  lr: 0.00256
step: 257, train_loss: 1.4378658533096313, acc: 0.37959998846054077, val_loss: 1.4338127374649048, val_acc: 0.38940000534057617,  lr: 0.0025700000000000002
step: 258, train_loss: 1.4383926391601562, acc: 0.3831999897956848, val_loss: 1.4312149286270142, val_acc: 0.3898000121116638,  lr: 0.0025800000000000003
step: 259, train_loss: 1.4354865550994873, acc: 0.3822000026702881, val_loss: 1.4307788610458374, val_acc: 0.38760000467300415,  lr: 0.0025900000000000003
step: 260, train_loss: 1.4301948547363281, acc: 0.38960000872612, val_loss: 1.4304195642471313, val_acc: 0.39100000262260437,  lr: 0.0026000000000000003
step: 261, train_loss: 1.4276740550994873, acc: 0.38760000467300415, val_loss: 1.432440996170044, val_acc: 0.384799987077713,  lr: 0.0026100000000000003
step: 262, train_loss: 1.426485538482666, acc: 0.3871999979019165, val_loss: 1.4272593259811401, val_acc: 0.3887999951839447,  lr: 0.0026200000000000004
step: 263, train_loss: 1.4320528507232666, acc: 0.38960000872612, val_loss: 1.4296926259994507, val_acc: 0.38839998841285706,  lr: 0.00263
step: 264, train_loss: 1.4148308038711548, acc: 0.39239999651908875, val_loss: 1.4270402193069458, val_acc: 0.3903999924659729,  lr: 0.00264
step: 265, train_loss: 1.4283653497695923, acc: 0.3871999979019165, val_loss: 1.4270068407058716, val_acc: 0.39259999990463257,  lr: 0.00265
step: 266, train_loss: 1.4236524105072021, acc: 0.391400009393692, val_loss: 1.426283359527588, val_acc: 0.3917999863624573,  lr: 0.00266
step: 267, train_loss: 1.42036771774292, acc: 0.3912000060081482, val_loss: 1.4242786169052124, val_acc: 0.387800008058548,  lr: 0.00267
step: 268, train_loss: 1.4190503358840942, acc: 0.3880000114440918, val_loss: 1.425722360610962, val_acc: 0.38960000872612,  lr: 0.00268
step: 269, train_loss: 1.4236012697219849, acc: 0.3885999917984009, val_loss: 1.4262081384658813, val_acc: 0.3885999917984009,  lr: 0.00269
step: 270, train_loss: 1.4196134805679321, acc: 0.38839998841285706, val_loss: 1.425440788269043, val_acc: 0.3930000066757202,  lr: 0.0027
step: 271, train_loss: 1.4195834398269653, acc: 0.3840000033378601, val_loss: 1.4280709028244019, val_acc: 0.3901999890804291,  lr: 0.00271
step: 272, train_loss: 1.4256690740585327, acc: 0.3885999917984009, val_loss: 1.4285625219345093, val_acc: 0.3869999945163727,  lr: 0.00272
step: 273, train_loss: 1.4246488809585571, acc: 0.3885999917984009, val_loss: 1.424043893814087, val_acc: 0.3898000121116638,  lr: 0.0027300000000000002
step: 274, train_loss: 1.4157445430755615, acc: 0.3991999924182892, val_loss: 1.4486321210861206, val_acc: 0.3781999945640564,  lr: 0.0027400000000000002
step: 275, train_loss: 1.4456071853637695, acc: 0.3779999911785126, val_loss: 1.458897590637207, val_acc: 0.3741999864578247,  lr: 0.0027500000000000003
step: 276, train_loss: 1.468247413635254, acc: 0.36800000071525574, val_loss: 1.4245667457580566, val_acc: 0.3882000148296356,  lr: 0.0027600000000000003
step: 277, train_loss: 1.4348645210266113, acc: 0.37779998779296875, val_loss: 1.4412513971328735, val_acc: 0.38260000944137573,  lr: 0.0027700000000000003
step: 278, train_loss: 1.4220821857452393, acc: 0.3903999924659729, val_loss: 1.4326773881912231, val_acc: 0.3874000012874603,  lr: 0.0027800000000000004
step: 279, train_loss: 1.425058126449585, acc: 0.38659998774528503, val_loss: 1.4361239671707153, val_acc: 0.3885999917984009,  lr: 0.0027900000000000004
step: 280, train_loss: 1.4237922430038452, acc: 0.387800008058548, val_loss: 1.4332141876220703, val_acc: 0.38760000467300415,  lr: 0.0028000000000000004
step: 281, train_loss: 1.4301371574401855, acc: 0.3822000026702881, val_loss: 1.434543251991272, val_acc: 0.3853999972343445,  lr: 0.0028100000000000004
step: 282, train_loss: 1.4321218729019165, acc: 0.38339999318122864, val_loss: 1.427882432937622, val_acc: 0.3903999924659729,  lr: 0.0028199999999999996
step: 283, train_loss: 1.4226633310317993, acc: 0.3889999985694885, val_loss: 1.4309107065200806, val_acc: 0.3871999979019165,  lr: 0.0028299999999999996
step: 284, train_loss: 1.4264179468154907, acc: 0.38440001010894775, val_loss: 1.4506076574325562, val_acc: 0.3776000142097473,  lr: 0.0028399999999999996
step: 285, train_loss: 1.455933928489685, acc: 0.3702000081539154, val_loss: 1.501745581626892, val_acc: 0.36880001425743103,  lr: 0.0028499999999999997
step: 286, train_loss: 1.5017505884170532, acc: 0.3700000047683716, val_loss: 1.450136661529541, val_acc: 0.38040000200271606,  lr: 0.0028599999999999997
step: 287, train_loss: 1.4397341012954712, acc: 0.3874000012874603, val_loss: 1.4359867572784424, val_acc: 0.3808000087738037,  lr: 0.0028699999999999997
step: 288, train_loss: 1.432053565979004, acc: 0.38839998841285706, val_loss: 1.4432586431503296, val_acc: 0.384799987077713,  lr: 0.0028799999999999997
step: 289, train_loss: 1.439384937286377, acc: 0.38280001282691956, val_loss: 1.4867039918899536, val_acc: 0.37860000133514404,  lr: 0.0028899999999999998
step: 290, train_loss: 1.4853390455245972, acc: 0.3797999918460846, val_loss: 1.4809088706970215, val_acc: 0.37400001287460327,  lr: 0.0029
step: 291, train_loss: 1.4655872583389282, acc: 0.37619999051094055, val_loss: 1.4523465633392334, val_acc: 0.38260000944137573,  lr: 0.00291
step: 292, train_loss: 1.4495199918746948, acc: 0.38100001215934753, val_loss: 1.440965175628662, val_acc: 0.38339999318122864,  lr: 0.00292
step: 293, train_loss: 1.4329588413238525, acc: 0.38420000672340393, val_loss: 1.4766783714294434, val_acc: 0.37220001220703125,  lr: 0.00293
step: 294, train_loss: 1.490106463432312, acc: 0.3774000108242035, val_loss: 1.4861339330673218, val_acc: 0.3718000054359436,  lr: 0.00294
step: 295, train_loss: 1.4802660942077637, acc: 0.375, val_loss: 1.465934157371521, val_acc: 0.376800000667572,  lr: 0.00295
step: 296, train_loss: 1.4655845165252686, acc: 0.3734000027179718, val_loss: 1.4648654460906982, val_acc: 0.3806000053882599,  lr: 0.00296
step: 297, train_loss: 1.4686750173568726, acc: 0.37779998779296875, val_loss: 1.4558504819869995, val_acc: 0.376800000667572,  lr: 0.00297
step: 298, train_loss: 1.4496461153030396, acc: 0.3824000060558319, val_loss: 1.4570423364639282, val_acc: 0.37779998779296875,  lr: 0.00298
step: 299, train_loss: 1.4587961435317993, acc: 0.37860000133514404, val_loss: 1.4508109092712402, val_acc: 0.38119998574256897,  lr: 0.00299
step: 300, train_loss: 1.4449743032455444, acc: 0.3837999999523163, val_loss: 1.4585577249526978, val_acc: 0.3709999918937683,  lr: 0.003
step: 301, train_loss: 1.4583584070205688, acc: 0.36959999799728394, val_loss: 1.4576308727264404, val_acc: 0.3831999897956848,  lr: 0.00301
step: 302, train_loss: 1.457899570465088, acc: 0.3765999972820282, val_loss: 1.442409873008728, val_acc: 0.38679999113082886,  lr: 0.00302
step: 303, train_loss: 1.4443235397338867, acc: 0.37940001487731934, val_loss: 1.4435765743255615, val_acc: 0.3862000107765198,  lr: 0.00303
step: 304, train_loss: 1.4459288120269775, acc: 0.38280001282691956, val_loss: 1.4457980394363403, val_acc: 0.38580000400543213,  lr: 0.00304
step: 305, train_loss: 1.438686490058899, acc: 0.3837999999523163, val_loss: 1.4387588500976562, val_acc: 0.38499999046325684,  lr: 0.00305
step: 306, train_loss: 1.4401109218597412, acc: 0.3781999945640564, val_loss: 1.4334489107131958, val_acc: 0.3856000006198883,  lr: 0.0030600000000000002
step: 307, train_loss: 1.426186442375183, acc: 0.39399999380111694, val_loss: 1.4394142627716064, val_acc: 0.38600000739097595,  lr: 0.00307
step: 308, train_loss: 1.4403754472732544, acc: 0.38440001010894775, val_loss: 1.4340500831604004, val_acc: 0.3822000026702881,  lr: 0.00308
step: 309, train_loss: 1.431210994720459, acc: 0.38659998774528503, val_loss: 1.428660273551941, val_acc: 0.38659998774528503,  lr: 0.00309
step: 310, train_loss: 1.4219979047775269, acc: 0.3953999876976013, val_loss: 1.4314861297607422, val_acc: 0.38920000195503235,  lr: 0.0031
step: 311, train_loss: 1.4202371835708618, acc: 0.3885999917984009, val_loss: 1.4470570087432861, val_acc: 0.3862000107765198,  lr: 0.00311
step: 312, train_loss: 1.4458931684494019, acc: 0.3885999917984009, val_loss: 1.5277130603790283, val_acc: 0.3736000061035156,  lr: 0.00312
step: 313, train_loss: 1.543764591217041, acc: 0.3643999993801117, val_loss: 1.4675689935684204, val_acc: 0.37560001015663147,  lr: 0.00313
step: 314, train_loss: 1.4666334390640259, acc: 0.3723999857902527, val_loss: 1.4606385231018066, val_acc: 0.376800000667572,  lr: 0.00314
step: 315, train_loss: 1.4530705213546753, acc: 0.37220001220703125, val_loss: 1.4646838903427124, val_acc: 0.37599998712539673,  lr: 0.00315
step: 316, train_loss: 1.471897006034851, acc: 0.37139999866485596, val_loss: 1.6243430376052856, val_acc: 0.3246000111103058,  lr: 0.00316
step: 317, train_loss: 1.6489789485931396, acc: 0.32739999890327454, val_loss: 1.5041453838348389, val_acc: 0.37059998512268066,  lr: 0.00317
step: 318, train_loss: 1.4968130588531494, acc: 0.3734000027179718, val_loss: 1.5552257299423218, val_acc: 0.34380000829696655,  lr: 0.00318
step: 319, train_loss: 1.5949937105178833, acc: 0.32839998602867126, val_loss: 1.6555237770080566, val_acc: 0.3208000063896179,  lr: 0.00319
step: 320, train_loss: 1.6542469263076782, acc: 0.3237999975681305, val_loss: 1.521666169166565, val_acc: 0.3668000102043152,  lr: 0.0032
step: 321, train_loss: 1.5212557315826416, acc: 0.36419999599456787, val_loss: 1.5493216514587402, val_acc: 0.3384000062942505,  lr: 0.00321
step: 322, train_loss: 1.5495648384094238, acc: 0.3368000090122223, val_loss: 1.5254663228988647, val_acc: 0.3668000102043152,  lr: 0.00322
step: 323, train_loss: 1.5159469842910767, acc: 0.3630000054836273, val_loss: 1.5272237062454224, val_acc: 0.3684000074863434,  lr: 0.0032300000000000002
step: 324, train_loss: 1.5184975862503052, acc: 0.3617999851703644, val_loss: 1.5165040493011475, val_acc: 0.36820000410079956,  lr: 0.0032400000000000003
step: 325, train_loss: 1.5223203897476196, acc: 0.35839998722076416, val_loss: 1.5070487260818481, val_acc: 0.36800000071525574,  lr: 0.0032500000000000003
step: 326, train_loss: 1.5112159252166748, acc: 0.3601999878883362, val_loss: 1.5006370544433594, val_acc: 0.37220001220703125,  lr: 0.0032600000000000003
step: 327, train_loss: 1.5044997930526733, acc: 0.3709999918937683, val_loss: 1.4973196983337402, val_acc: 0.37700000405311584,  lr: 0.0032700000000000003
step: 328, train_loss: 1.5067379474639893, acc: 0.367000013589859, val_loss: 1.4925330877304077, val_acc: 0.3718000054359436,  lr: 0.0032800000000000004
step: 329, train_loss: 1.4981367588043213, acc: 0.367000013589859, val_loss: 1.499855399131775, val_acc: 0.36899998784065247,  lr: 0.0032900000000000004
step: 330, train_loss: 1.4974195957183838, acc: 0.3684000074863434, val_loss: 1.4924625158309937, val_acc: 0.3702000081539154,  lr: 0.0033000000000000004
step: 331, train_loss: 1.4802799224853516, acc: 0.37560001015663147, val_loss: 1.4925239086151123, val_acc: 0.3716000020503998,  lr: 0.0033100000000000004
step: 332, train_loss: 1.4905833005905151, acc: 0.3752000033855438, val_loss: 1.4887475967407227, val_acc: 0.37400001287460327,  lr: 0.00332
step: 333, train_loss: 1.490440845489502, acc: 0.3734000027179718, val_loss: 1.4840863943099976, val_acc: 0.3765999972820282,  lr: 0.00333
step: 334, train_loss: 1.486641764640808, acc: 0.37139999866485596, val_loss: 1.480394959449768, val_acc: 0.38040000200271606,  lr: 0.00334
step: 335, train_loss: 1.4775691032409668, acc: 0.37959998846054077, val_loss: 1.4737827777862549, val_acc: 0.3840000033378601,  lr: 0.00335
step: 336, train_loss: 1.471937894821167, acc: 0.37299999594688416, val_loss: 1.4715490341186523, val_acc: 0.3792000114917755,  lr: 0.00336
step: 337, train_loss: 1.4683616161346436, acc: 0.3734000027179718, val_loss: 1.4699677228927612, val_acc: 0.37959998846054077,  lr: 0.00337
step: 338, train_loss: 1.4618743658065796, acc: 0.3776000142097473, val_loss: 1.4629319906234741, val_acc: 0.3822000026702881,  lr: 0.00338
step: 339, train_loss: 1.4579886198043823, acc: 0.387800008058548, val_loss: 1.4608168601989746, val_acc: 0.38440001010894775,  lr: 0.0033900000000000002
step: 340, train_loss: 1.446270227432251, acc: 0.38999998569488525, val_loss: 1.463732361793518, val_acc: 0.38359999656677246,  lr: 0.0034000000000000002
step: 341, train_loss: 1.4658217430114746, acc: 0.37540000677108765, val_loss: 1.460155725479126, val_acc: 0.37940001487731934,  lr: 0.0034100000000000003
step: 342, train_loss: 1.457757592201233, acc: 0.38179999589920044, val_loss: 1.461306095123291, val_acc: 0.3824000060558319,  lr: 0.0034200000000000003
step: 343, train_loss: 1.4639911651611328, acc: 0.37599998712539673, val_loss: 1.4568419456481934, val_acc: 0.382999986410141,  lr: 0.0034300000000000003
step: 344, train_loss: 1.4392489194869995, acc: 0.3862000107765198, val_loss: 1.451332926750183, val_acc: 0.38499999046325684,  lr: 0.00344
step: 345, train_loss: 1.44984769821167, acc: 0.3846000134944916, val_loss: 1.449265956878662, val_acc: 0.3885999917984009,  lr: 0.00345
step: 346, train_loss: 1.4290133714675903, acc: 0.39079999923706055, val_loss: 1.4441457986831665, val_acc: 0.38440001010894775,  lr: 0.00346
step: 347, train_loss: 1.453057050704956, acc: 0.38260000944137573, val_loss: 1.442460536956787, val_acc: 0.38760000467300415,  lr: 0.00347
step: 348, train_loss: 1.4260079860687256, acc: 0.3885999917984009, val_loss: 1.4403373003005981, val_acc: 0.387800008058548,  lr: 0.00348
step: 349, train_loss: 1.4283345937728882, acc: 0.38280001282691956, val_loss: 1.4386281967163086, val_acc: 0.3864000141620636,  lr: 0.00349
step: 350, train_loss: 1.4183127880096436, acc: 0.39480000734329224, val_loss: 1.4386507272720337, val_acc: 0.3901999890804291,  lr: 0.0034999999999999996
step: 351, train_loss: 1.4400765895843506, acc: 0.3864000141620636, val_loss: 1.4425370693206787, val_acc: 0.3887999951839447,  lr: 0.0035099999999999997
step: 352, train_loss: 1.4441217184066772, acc: 0.38260000944137573, val_loss: 1.4407002925872803, val_acc: 0.387800008058548,  lr: 0.0035199999999999997
step: 353, train_loss: 1.4164576530456543, acc: 0.3962000012397766, val_loss: 1.4394575357437134, val_acc: 0.3874000012874603,  lr: 0.0035299999999999997
step: 354, train_loss: 1.4394514560699463, acc: 0.38100001215934753, val_loss: 1.435664415359497, val_acc: 0.38999998569488525,  lr: 0.0035399999999999997
step: 355, train_loss: 1.4331486225128174, acc: 0.3903999924659729, val_loss: 1.4363378286361694, val_acc: 0.3905999958515167,  lr: 0.0035499999999999998
step: 356, train_loss: 1.4405590295791626, acc: 0.3856000006198883, val_loss: 1.4346110820770264, val_acc: 0.3880000114440918,  lr: 0.00356
step: 357, train_loss: 1.4373561143875122, acc: 0.3871999979019165, val_loss: 1.4351441860198975, val_acc: 0.3869999945163727,  lr: 0.00357
step: 358, train_loss: 1.4315502643585205, acc: 0.38679999113082886, val_loss: 1.4351431131362915, val_acc: 0.38999998569488525,  lr: 0.00358
step: 359, train_loss: 1.4296860694885254, acc: 0.3921999931335449, val_loss: 1.4332211017608643, val_acc: 0.39079999923706055,  lr: 0.00359
step: 360, train_loss: 1.4317468404769897, acc: 0.3874000012874603, val_loss: 1.4322822093963623, val_acc: 0.3880000114440918,  lr: 0.0036
step: 361, train_loss: 1.4307307004928589, acc: 0.38179999589920044, val_loss: 1.4286682605743408, val_acc: 0.3935999870300293,  lr: 0.00361
step: 362, train_loss: 1.4189660549163818, acc: 0.38999998569488525, val_loss: 1.4270156621932983, val_acc: 0.38940000534057617,  lr: 0.00362
step: 363, train_loss: 1.4296363592147827, acc: 0.3779999911785126, val_loss: 1.4433673620224, val_acc: 0.38260000944137573,  lr: 0.00363
step: 364, train_loss: 1.4413453340530396, acc: 0.3792000114917755, val_loss: 1.425935983657837, val_acc: 0.39100000262260437,  lr: 0.00364
step: 365, train_loss: 1.405324101448059, acc: 0.39719998836517334, val_loss: 1.4277809858322144, val_acc: 0.3928000032901764,  lr: 0.00365
step: 366, train_loss: 1.4295892715454102, acc: 0.3880000114440918, val_loss: 1.4328166246414185, val_acc: 0.3880000114440918,  lr: 0.00366
step: 367, train_loss: 1.4332902431488037, acc: 0.3889999985694885, val_loss: 1.4248769283294678, val_acc: 0.3901999890804291,  lr: 0.00367
step: 368, train_loss: 1.4162120819091797, acc: 0.3959999978542328, val_loss: 1.4293347597122192, val_acc: 0.391400009393692,  lr: 0.00368
step: 369, train_loss: 1.4320095777511597, acc: 0.3937999904155731, val_loss: 1.4209272861480713, val_acc: 0.39399999380111694,  lr: 0.00369
step: 370, train_loss: 1.4136873483657837, acc: 0.3959999978542328, val_loss: 1.4258188009262085, val_acc: 0.3901999890804291,  lr: 0.0037
step: 371, train_loss: 1.4208544492721558, acc: 0.3887999951839447, val_loss: 1.420982837677002, val_acc: 0.391400009393692,  lr: 0.00371
step: 372, train_loss: 1.4218546152114868, acc: 0.38679999113082886, val_loss: 1.4192287921905518, val_acc: 0.3935999870300293,  lr: 0.00372
step: 373, train_loss: 1.4299968481063843, acc: 0.3853999972343445, val_loss: 1.4191702604293823, val_acc: 0.3917999863624573,  lr: 0.0037300000000000002
step: 374, train_loss: 1.4243040084838867, acc: 0.38339999318122864, val_loss: 1.419423222541809, val_acc: 0.3928000032901764,  lr: 0.0037400000000000003
step: 375, train_loss: 1.4163891077041626, acc: 0.3882000148296356, val_loss: 1.4242671728134155, val_acc: 0.39079999923706055,  lr: 0.00375
step: 376, train_loss: 1.4281599521636963, acc: 0.387800008058548, val_loss: 1.4149901866912842, val_acc: 0.38960000872612,  lr: 0.00376
step: 377, train_loss: 1.41560697555542, acc: 0.38960000872612, val_loss: 1.4182779788970947, val_acc: 0.39559999108314514,  lr: 0.00377
step: 378, train_loss: 1.4163669347763062, acc: 0.38960000872612, val_loss: 1.4157838821411133, val_acc: 0.3970000147819519,  lr: 0.00378
step: 379, train_loss: 1.4179071187973022, acc: 0.39259999990463257, val_loss: 1.4144911766052246, val_acc: 0.3946000039577484,  lr: 0.00379
step: 380, train_loss: 1.4108585119247437, acc: 0.39320001006126404, val_loss: 1.4175872802734375, val_acc: 0.39079999923706055,  lr: 0.0038
step: 381, train_loss: 1.4082088470458984, acc: 0.3935999870300293, val_loss: 1.414253830909729, val_acc: 0.3905999958515167,  lr: 0.00381
step: 382, train_loss: 1.4088337421417236, acc: 0.3946000039577484, val_loss: 1.4159126281738281, val_acc: 0.39239999651908875,  lr: 0.00382
step: 383, train_loss: 1.4158517122268677, acc: 0.3937999904155731, val_loss: 1.4129843711853027, val_acc: 0.3930000066757202,  lr: 0.00383
step: 384, train_loss: 1.412840723991394, acc: 0.39419999718666077, val_loss: 1.4152966737747192, val_acc: 0.3921999931335449,  lr: 0.00384
step: 385, train_loss: 1.4028047323226929, acc: 0.3928000032901764, val_loss: 1.4139426946640015, val_acc: 0.3921999931335449,  lr: 0.00385
step: 386, train_loss: 1.423495888710022, acc: 0.3869999945163727, val_loss: 1.413448691368103, val_acc: 0.39559999108314514,  lr: 0.00386
step: 387, train_loss: 1.4130938053131104, acc: 0.3912000060081482, val_loss: 1.4127334356307983, val_acc: 0.39160001277923584,  lr: 0.00387
step: 388, train_loss: 1.4097137451171875, acc: 0.3917999863624573, val_loss: 1.4136096239089966, val_acc: 0.39340001344680786,  lr: 0.00388
step: 389, train_loss: 1.4185150861740112, acc: 0.3882000148296356, val_loss: 1.411012053489685, val_acc: 0.39419999718666077,  lr: 0.0038900000000000002
step: 390, train_loss: 1.405981183052063, acc: 0.3930000066757202, val_loss: 1.4164272546768188, val_acc: 0.39419999718666077,  lr: 0.0039000000000000003
step: 391, train_loss: 1.4062557220458984, acc: 0.39079999923706055, val_loss: 1.4097263813018799, val_acc: 0.3946000039577484,  lr: 0.00391
step: 392, train_loss: 1.404023289680481, acc: 0.39160001277923584, val_loss: 1.4143885374069214, val_acc: 0.39419999718666077,  lr: 0.00392
step: 393, train_loss: 1.412611722946167, acc: 0.38760000467300415, val_loss: 1.4116737842559814, val_acc: 0.39340001344680786,  lr: 0.00393
step: 394, train_loss: 1.406489610671997, acc: 0.3921999931335449, val_loss: 1.4116395711898804, val_acc: 0.39640000462532043,  lr: 0.00394
step: 395, train_loss: 1.4102083444595337, acc: 0.3917999863624573, val_loss: 1.4104670286178589, val_acc: 0.3959999978542328,  lr: 0.00395
step: 396, train_loss: 1.413305640220642, acc: 0.3874000012874603, val_loss: 1.4084022045135498, val_acc: 0.39340001344680786,  lr: 0.00396
step: 397, train_loss: 1.40214204788208, acc: 0.3928000032901764, val_loss: 1.407894492149353, val_acc: 0.39719998836517334,  lr: 0.0039700000000000004
step: 398, train_loss: 1.4067819118499756, acc: 0.3953999876976013, val_loss: 1.4068384170532227, val_acc: 0.3970000147819519,  lr: 0.00398
step: 399, train_loss: 1.3960925340652466, acc: 0.39480000734329224, val_loss: 1.4087493419647217, val_acc: 0.3944000005722046,  lr: 0.0039900000000000005
step: 400, train_loss: 1.4105559587478638, acc: 0.3935999870300293, val_loss: 1.407268762588501, val_acc: 0.3952000141143799,  lr: 0.004
step: 401, train_loss: 1.4084434509277344, acc: 0.3930000066757202, val_loss: 1.4064128398895264, val_acc: 0.39719998836517334,  lr: 0.0040100000000000005
step: 402, train_loss: 1.4034727811813354, acc: 0.3977999985218048, val_loss: 1.4071131944656372, val_acc: 0.39640000462532043,  lr: 0.00402
step: 403, train_loss: 1.399936556816101, acc: 0.4002000093460083, val_loss: 1.407711386680603, val_acc: 0.39820000529289246,  lr: 0.004030000000000001
step: 404, train_loss: 1.4014110565185547, acc: 0.3930000066757202, val_loss: 1.4070199728012085, val_acc: 0.39899998903274536,  lr: 0.00404
step: 405, train_loss: 1.3983289003372192, acc: 0.39559999108314514, val_loss: 1.4058423042297363, val_acc: 0.3968000113964081,  lr: 0.004050000000000001
step: 406, train_loss: 1.394046664237976, acc: 0.3968000113964081, val_loss: 1.4054428339004517, val_acc: 0.39660000801086426,  lr: 0.00406
step: 407, train_loss: 1.3959484100341797, acc: 0.4018000066280365, val_loss: 1.4048351049423218, val_acc: 0.3977999985218048,  lr: 0.00407
step: 408, train_loss: 1.4031997919082642, acc: 0.3917999863624573, val_loss: 1.4099286794662476, val_acc: 0.39320001006126404,  lr: 0.004079999999999999
step: 409, train_loss: 1.419660210609436, acc: 0.3912000060081482, val_loss: 1.4111977815628052, val_acc: 0.3928000032901764,  lr: 0.00409
step: 410, train_loss: 1.4124139547348022, acc: 0.3831999897956848, val_loss: 1.4169546365737915, val_acc: 0.391400009393692,  lr: 0.0040999999999999995
step: 411, train_loss: 1.4076149463653564, acc: 0.39559999108314514, val_loss: 1.4056804180145264, val_acc: 0.39480000734329224,  lr: 0.00411
step: 412, train_loss: 1.3958238363265991, acc: 0.39579999446868896, val_loss: 1.4074959754943848, val_acc: 0.3930000066757202,  lr: 0.0041199999999999995
step: 413, train_loss: 1.3999300003051758, acc: 0.4016000032424927, val_loss: 1.4083503484725952, val_acc: 0.39719998836517334,  lr: 0.00413
step: 414, train_loss: 1.3938190937042236, acc: 0.39500001072883606, val_loss: 1.4085129499435425, val_acc: 0.3977999985218048,  lr: 0.00414
step: 415, train_loss: 1.4035087823867798, acc: 0.3882000148296356, val_loss: 1.4062424898147583, val_acc: 0.39399999380111694,  lr: 0.00415
step: 416, train_loss: 1.412170171737671, acc: 0.39320001006126404, val_loss: 1.4037176370620728, val_acc: 0.39500001072883606,  lr: 0.00416
step: 417, train_loss: 1.4062187671661377, acc: 0.391400009393692, val_loss: 1.402891755104065, val_acc: 0.3935999870300293,  lr: 0.00417
step: 418, train_loss: 1.3957507610321045, acc: 0.39500001072883606, val_loss: 1.4032272100448608, val_acc: 0.3928000032901764,  lr: 0.00418
step: 419, train_loss: 1.3916563987731934, acc: 0.39980000257492065, val_loss: 1.4019659757614136, val_acc: 0.3962000012397766,  lr: 0.00419
step: 420, train_loss: 1.3986141681671143, acc: 0.39660000801086426, val_loss: 1.3990370035171509, val_acc: 0.39500001072883606,  lr: 0.0042
step: 421, train_loss: 1.3943499326705933, acc: 0.3984000086784363, val_loss: 1.4027788639068604, val_acc: 0.3991999924182892,  lr: 0.00421
step: 422, train_loss: 1.3920334577560425, acc: 0.40139999985694885, val_loss: 1.40015709400177, val_acc: 0.39879998564720154,  lr: 0.00422
step: 423, train_loss: 1.389528512954712, acc: 0.3984000086784363, val_loss: 1.401017427444458, val_acc: 0.39480000734329224,  lr: 0.00423
step: 424, train_loss: 1.3873506784439087, acc: 0.4004000127315521, val_loss: 1.403161644935608, val_acc: 0.3959999978542328,  lr: 0.00424
step: 425, train_loss: 1.4031559228897095, acc: 0.39239999651908875, val_loss: 1.4015880823135376, val_acc: 0.3937999904155731,  lr: 0.00425
step: 426, train_loss: 1.3953146934509277, acc: 0.399399995803833, val_loss: 1.4017566442489624, val_acc: 0.39399999380111694,  lr: 0.00426
step: 427, train_loss: 1.4050960540771484, acc: 0.39660000801086426, val_loss: 1.4038070440292358, val_acc: 0.39480000734329224,  lr: 0.00427
step: 428, train_loss: 1.3991323709487915, acc: 0.38960000872612, val_loss: 1.4000977277755737, val_acc: 0.39879998564720154,  lr: 0.00428
step: 429, train_loss: 1.4007141590118408, acc: 0.3919999897480011, val_loss: 1.3996717929840088, val_acc: 0.4009999930858612,  lr: 0.00429
step: 430, train_loss: 1.3878493309020996, acc: 0.3882000148296356, val_loss: 1.3998671770095825, val_acc: 0.39800000190734863,  lr: 0.0043
step: 431, train_loss: 1.401067852973938, acc: 0.391400009393692, val_loss: 1.4065951108932495, val_acc: 0.3970000147819519,  lr: 0.0043100000000000005
step: 432, train_loss: 1.3952568769454956, acc: 0.39800000190734863, val_loss: 1.4007295370101929, val_acc: 0.3962000012397766,  lr: 0.00432
step: 433, train_loss: 1.3930199146270752, acc: 0.39640000462532043, val_loss: 1.4028258323669434, val_acc: 0.3977999985218048,  lr: 0.00433
step: 434, train_loss: 1.3987642526626587, acc: 0.40220001339912415, val_loss: 1.4058849811553955, val_acc: 0.3937999904155731,  lr: 0.00434
step: 435, train_loss: 1.3968636989593506, acc: 0.3952000141143799, val_loss: 1.4041327238082886, val_acc: 0.39719998836517334,  lr: 0.00435
step: 436, train_loss: 1.3973984718322754, acc: 0.40119999647140503, val_loss: 1.403415322303772, val_acc: 0.3986000120639801,  lr: 0.00436
step: 437, train_loss: 1.3881231546401978, acc: 0.397599995136261, val_loss: 1.4118460416793823, val_acc: 0.39100000262260437,  lr: 0.00437
step: 438, train_loss: 1.4021717309951782, acc: 0.3898000121116638, val_loss: 1.4083744287490845, val_acc: 0.391400009393692,  lr: 0.00438
step: 439, train_loss: 1.4003818035125732, acc: 0.4047999978065491, val_loss: 1.4005184173583984, val_acc: 0.39419999718666077,  lr: 0.00439
step: 440, train_loss: 1.3952898979187012, acc: 0.39899998903274536, val_loss: 1.405040979385376, val_acc: 0.39079999923706055,  lr: 0.0044
step: 441, train_loss: 1.4171403646469116, acc: 0.3952000141143799, val_loss: 1.403112530708313, val_acc: 0.3986000120639801,  lr: 0.00441
step: 442, train_loss: 1.3952187299728394, acc: 0.39559999108314514, val_loss: 1.4037498235702515, val_acc: 0.3986000120639801,  lr: 0.00442
step: 443, train_loss: 1.3953419923782349, acc: 0.39500001072883606, val_loss: 1.396833062171936, val_acc: 0.39559999108314514,  lr: 0.00443
step: 444, train_loss: 1.3880467414855957, acc: 0.4007999897003174, val_loss: 1.403470516204834, val_acc: 0.39419999718666077,  lr: 0.00444
step: 445, train_loss: 1.4016444683074951, acc: 0.3937999904155731, val_loss: 1.40053129196167, val_acc: 0.3970000147819519,  lr: 0.00445
step: 446, train_loss: 1.3811734914779663, acc: 0.4018000066280365, val_loss: 1.4027507305145264, val_acc: 0.3984000086784363,  lr: 0.00446
step: 447, train_loss: 1.3962311744689941, acc: 0.4002000093460083, val_loss: 1.400650143623352, val_acc: 0.39660000801086426,  lr: 0.00447
step: 448, train_loss: 1.3979722261428833, acc: 0.39579999446868896, val_loss: 1.4020640850067139, val_acc: 0.3905999958515167,  lr: 0.0044800000000000005
step: 449, train_loss: 1.4069730043411255, acc: 0.3887999951839447, val_loss: 1.3989044427871704, val_acc: 0.38940000534057617,  lr: 0.00449
step: 450, train_loss: 1.396512746810913, acc: 0.39419999718666077, val_loss: 1.400640606880188, val_acc: 0.3986000120639801,  lr: 0.0045000000000000005
step: 451, train_loss: 1.3940900564193726, acc: 0.3991999924182892, val_loss: 1.3993867635726929, val_acc: 0.397599995136261,  lr: 0.00451
step: 452, train_loss: 1.396051287651062, acc: 0.39739999175071716, val_loss: 1.3980716466903687, val_acc: 0.3959999978542328,  lr: 0.004520000000000001
step: 453, train_loss: 1.4123364686965942, acc: 0.38839998841285706, val_loss: 1.398701786994934, val_acc: 0.3917999863624573,  lr: 0.00453
step: 454, train_loss: 1.37714684009552, acc: 0.40540000796318054, val_loss: 1.401640772819519, val_acc: 0.39660000801086426,  lr: 0.004540000000000001
step: 455, train_loss: 1.3954403400421143, acc: 0.397599995136261, val_loss: 1.3988860845565796, val_acc: 0.40220001339912415,  lr: 0.00455
step: 456, train_loss: 1.4048271179199219, acc: 0.391400009393692, val_loss: 1.3984525203704834, val_acc: 0.4018000066280365,  lr: 0.004560000000000001
step: 457, train_loss: 1.3905853033065796, acc: 0.39419999718666077, val_loss: 1.3978350162506104, val_acc: 0.39899998903274536,  lr: 0.00457
step: 458, train_loss: 1.3833403587341309, acc: 0.4007999897003174, val_loss: 1.397513747215271, val_acc: 0.3946000039577484,  lr: 0.00458
step: 459, train_loss: 1.3956855535507202, acc: 0.39640000462532043, val_loss: 1.3979967832565308, val_acc: 0.3984000086784363,  lr: 0.00459
step: 460, train_loss: 1.395383358001709, acc: 0.3930000066757202, val_loss: 1.3979614973068237, val_acc: 0.39820000529289246,  lr: 0.0046
step: 461, train_loss: 1.3873172998428345, acc: 0.3977999985218048, val_loss: 1.3932217359542847, val_acc: 0.39800000190734863,  lr: 0.00461
step: 462, train_loss: 1.3920007944107056, acc: 0.397599995136261, val_loss: 1.3941324949264526, val_acc: 0.399399995803833,  lr: 0.00462
step: 463, train_loss: 1.390073537826538, acc: 0.39239999651908875, val_loss: 1.4023104906082153, val_acc: 0.39399999380111694,  lr: 0.0046300000000000004
step: 464, train_loss: 1.4073903560638428, acc: 0.39160001277923584, val_loss: 1.4057321548461914, val_acc: 0.3962000012397766,  lr: 0.00464
step: 465, train_loss: 1.3949546813964844, acc: 0.39719998836517334, val_loss: 1.3937731981277466, val_acc: 0.3986000120639801,  lr: 0.0046500000000000005
step: 466, train_loss: 1.3895851373672485, acc: 0.3986000120639801, val_loss: 1.3989070653915405, val_acc: 0.4020000100135803,  lr: 0.00466
step: 467, train_loss: 1.39453125, acc: 0.39500001072883606, val_loss: 1.3920722007751465, val_acc: 0.40299999713897705,  lr: 0.0046700000000000005
step: 468, train_loss: 1.3935242891311646, acc: 0.39739999175071716, val_loss: 1.3926963806152344, val_acc: 0.3986000120639801,  lr: 0.00468
step: 469, train_loss: 1.388575553894043, acc: 0.399399995803833, val_loss: 1.3918095827102661, val_acc: 0.399399995803833,  lr: 0.00469
step: 470, train_loss: 1.3930668830871582, acc: 0.40139999985694885, val_loss: 1.3907727003097534, val_acc: 0.39959999918937683,  lr: 0.0047
step: 471, train_loss: 1.3935177326202393, acc: 0.3944000005722046, val_loss: 1.3902045488357544, val_acc: 0.399399995803833,  lr: 0.00471
step: 472, train_loss: 1.3862794637680054, acc: 0.40220001339912415, val_loss: 1.3891031742095947, val_acc: 0.40459999442100525,  lr: 0.00472
step: 473, train_loss: 1.3840168714523315, acc: 0.4018000066280365, val_loss: 1.38715660572052, val_acc: 0.4009999930858612,  lr: 0.00473
step: 474, train_loss: 1.3873531818389893, acc: 0.39820000529289246, val_loss: 1.3918429613113403, val_acc: 0.3984000086784363,  lr: 0.00474
step: 475, train_loss: 1.3854848146438599, acc: 0.40059998631477356, val_loss: 1.3873454332351685, val_acc: 0.4025999903678894,  lr: 0.00475
step: 476, train_loss: 1.379760503768921, acc: 0.4000000059604645, val_loss: 1.3929309844970703, val_acc: 0.4018000066280365,  lr: 0.0047599999999999995
step: 477, train_loss: 1.3869366645812988, acc: 0.3986000120639801, val_loss: 1.3867930173873901, val_acc: 0.40299999713897705,  lr: 0.00477
step: 478, train_loss: 1.3804941177368164, acc: 0.39959999918937683, val_loss: 1.3903024196624756, val_acc: 0.39800000190734863,  lr: 0.0047799999999999995
step: 479, train_loss: 1.3817954063415527, acc: 0.3977999985218048, val_loss: 1.3868540525436401, val_acc: 0.3991999924182892,  lr: 0.00479
step: 480, train_loss: 1.370882511138916, acc: 0.4081999957561493, val_loss: 1.3919703960418701, val_acc: 0.3977999985218048,  lr: 0.0048
step: 481, train_loss: 1.3893235921859741, acc: 0.39259999990463257, val_loss: 1.3940637111663818, val_acc: 0.3970000147819519,  lr: 0.00481
step: 482, train_loss: 1.387351155281067, acc: 0.3970000147819519, val_loss: 1.3871890306472778, val_acc: 0.4036000072956085,  lr: 0.00482
step: 483, train_loss: 1.3852295875549316, acc: 0.39739999175071716, val_loss: 1.395402193069458, val_acc: 0.3962000012397766,  lr: 0.00483
step: 484, train_loss: 1.4049328565597534, acc: 0.3917999863624573, val_loss: 1.3942511081695557, val_acc: 0.3991999924182892,  lr: 0.00484
step: 485, train_loss: 1.398793339729309, acc: 0.391400009393692, val_loss: 1.4049718379974365, val_acc: 0.3962000012397766,  lr: 0.00485
step: 486, train_loss: 1.4093822240829468, acc: 0.39419999718666077, val_loss: 1.3976848125457764, val_acc: 0.4002000093460083,  lr: 0.00486
step: 487, train_loss: 1.4018865823745728, acc: 0.39239999651908875, val_loss: 1.3937981128692627, val_acc: 0.4004000127315521,  lr: 0.00487
step: 488, train_loss: 1.3935922384262085, acc: 0.39559999108314514, val_loss: 1.4012967348098755, val_acc: 0.3928000032901764,  lr: 0.00488
step: 489, train_loss: 1.392177939414978, acc: 0.39739999175071716, val_loss: 1.3914482593536377, val_acc: 0.39640000462532043,  lr: 0.00489
step: 490, train_loss: 1.3917105197906494, acc: 0.3962000012397766, val_loss: 1.3931429386138916, val_acc: 0.40119999647140503,  lr: 0.0049
step: 491, train_loss: 1.3803222179412842, acc: 0.4081999957561493, val_loss: 1.394225001335144, val_acc: 0.399399995803833,  lr: 0.00491
step: 492, train_loss: 1.3885691165924072, acc: 0.4004000127315521, val_loss: 1.3930563926696777, val_acc: 0.39660000801086426,  lr: 0.00492
step: 493, train_loss: 1.3859329223632812, acc: 0.39500001072883606, val_loss: 1.392745852470398, val_acc: 0.3968000113964081,  lr: 0.00493
step: 494, train_loss: 1.3928226232528687, acc: 0.4016000032424927, val_loss: 1.3887475728988647, val_acc: 0.39640000462532043,  lr: 0.00494
step: 495, train_loss: 1.3838192224502563, acc: 0.4002000093460083, val_loss: 1.3941746950149536, val_acc: 0.39719998836517334,  lr: 0.00495
step: 496, train_loss: 1.3889665603637695, acc: 0.40380001068115234, val_loss: 1.3919697999954224, val_acc: 0.4002000093460083,  lr: 0.00496
step: 497, train_loss: 1.3771930932998657, acc: 0.4036000072956085, val_loss: 1.3951767683029175, val_acc: 0.39500001072883606,  lr: 0.0049700000000000005
step: 498, train_loss: 1.4042372703552246, acc: 0.3959999978542328, val_loss: 1.3891370296478271, val_acc: 0.39559999108314514,  lr: 0.00498
step: 499, train_loss: 1.382557988166809, acc: 0.3986000120639801, val_loss: 1.3943244218826294, val_acc: 0.3946000039577484,  lr: 0.0049900000000000005
step: 500, train_loss: 1.3881688117980957, acc: 0.3977999985218048, val_loss: 1.3943994045257568, val_acc: 0.3984000086784363,  lr: 0.005
step: 501, train_loss: 1.3880912065505981, acc: 0.39800000190734863, val_loss: 1.3891103267669678, val_acc: 0.4007999897003174,  lr: 0.00501
step: 502, train_loss: 1.3723005056381226, acc: 0.4106000065803528, val_loss: 1.3934141397476196, val_acc: 0.3935999870300293,  lr: 0.00502
step: 503, train_loss: 1.387309193611145, acc: 0.40119999647140503, val_loss: 1.3938921689987183, val_acc: 0.39820000529289246,  lr: 0.00503
step: 504, train_loss: 1.4024397134780884, acc: 0.3903999924659729, val_loss: 1.392639398574829, val_acc: 0.39899998903274536,  lr: 0.00504
step: 505, train_loss: 1.3843672275543213, acc: 0.39980000257492065, val_loss: 1.3966795206069946, val_acc: 0.39399999380111694,  lr: 0.00505
step: 506, train_loss: 1.3892760276794434, acc: 0.39640000462532043, val_loss: 1.3894233703613281, val_acc: 0.4016000032424927,  lr: 0.00506
step: 507, train_loss: 1.372301459312439, acc: 0.3962000012397766, val_loss: 1.39664888381958, val_acc: 0.3977999985218048,  lr: 0.00507
step: 508, train_loss: 1.3919531106948853, acc: 0.4007999897003174, val_loss: 1.3909229040145874, val_acc: 0.40119999647140503,  lr: 0.00508
step: 509, train_loss: 1.3694934844970703, acc: 0.4041999876499176, val_loss: 1.3917737007141113, val_acc: 0.39320001006126404,  lr: 0.00509
step: 510, train_loss: 1.3781120777130127, acc: 0.399399995803833, val_loss: 1.3880386352539062, val_acc: 0.40059998631477356,  lr: 0.0051
step: 511, train_loss: 1.37410306930542, acc: 0.4047999978065491, val_loss: 1.394848108291626, val_acc: 0.4000000059604645,  lr: 0.00511
step: 512, train_loss: 1.4002801179885864, acc: 0.3901999890804291, val_loss: 1.386495590209961, val_acc: 0.399399995803833,  lr: 0.00512
step: 513, train_loss: 1.3860278129577637, acc: 0.40059998631477356, val_loss: 1.390323281288147, val_acc: 0.39879998564720154,  lr: 0.00513
step: 514, train_loss: 1.3815137147903442, acc: 0.4020000100135803, val_loss: 1.3889448642730713, val_acc: 0.40059998631477356,  lr: 0.0051400000000000005
step: 515, train_loss: 1.3829565048217773, acc: 0.4007999897003174, val_loss: 1.3906116485595703, val_acc: 0.4009999930858612,  lr: 0.00515
step: 516, train_loss: 1.3850326538085938, acc: 0.40220001339912415, val_loss: 1.3899810314178467, val_acc: 0.40380001068115234,  lr: 0.0051600000000000005
step: 517, train_loss: 1.3829052448272705, acc: 0.40400001406669617, val_loss: 1.3863171339035034, val_acc: 0.40400001406669617,  lr: 0.00517
step: 518, train_loss: 1.3649770021438599, acc: 0.40639999508857727, val_loss: 1.3896788358688354, val_acc: 0.4050000011920929,  lr: 0.005180000000000001
step: 519, train_loss: 1.3877887725830078, acc: 0.39419999718666077, val_loss: 1.4009764194488525, val_acc: 0.397599995136261,  lr: 0.00519
step: 520, train_loss: 1.3909348249435425, acc: 0.40560001134872437, val_loss: 1.3886919021606445, val_acc: 0.39820000529289246,  lr: 0.005200000000000001
step: 521, train_loss: 1.3839730024337769, acc: 0.4004000127315521, val_loss: 1.3872730731964111, val_acc: 0.4009999930858612,  lr: 0.00521
step: 522, train_loss: 1.3751130104064941, acc: 0.4050000011920929, val_loss: 1.3904896974563599, val_acc: 0.39899998903274536,  lr: 0.005220000000000001
step: 523, train_loss: 1.3705908060073853, acc: 0.4058000147342682, val_loss: 1.3861724138259888, val_acc: 0.40059998631477356,  lr: 0.00523
step: 524, train_loss: 1.401275396347046, acc: 0.39259999990463257, val_loss: 1.385860562324524, val_acc: 0.4020000100135803,  lr: 0.005240000000000001
step: 525, train_loss: 1.372705340385437, acc: 0.39899998903274536, val_loss: 1.392828106880188, val_acc: 0.3946000039577484,  lr: 0.00525
step: 526, train_loss: 1.3801146745681763, acc: 0.3952000141143799, val_loss: 1.385448932647705, val_acc: 0.39640000462532043,  lr: 0.00526
step: 527, train_loss: 1.3780710697174072, acc: 0.4074000120162964, val_loss: 1.3879700899124146, val_acc: 0.40220001339912415,  lr: 0.00527
step: 528, train_loss: 1.3791509866714478, acc: 0.4050000011920929, val_loss: 1.3846540451049805, val_acc: 0.40119999647140503,  lr: 0.00528
step: 529, train_loss: 1.3917416334152222, acc: 0.39879998564720154, val_loss: 1.3861939907073975, val_acc: 0.3984000086784363,  lr: 0.00529
step: 530, train_loss: 1.3817387819290161, acc: 0.3930000066757202, val_loss: 1.3844265937805176, val_acc: 0.39660000801086426,  lr: 0.0053
step: 531, train_loss: 1.388691782951355, acc: 0.399399995803833, val_loss: 1.3790712356567383, val_acc: 0.4027999937534332,  lr: 0.0053100000000000005
step: 532, train_loss: 1.3783040046691895, acc: 0.40380001068115234, val_loss: 1.3773318529129028, val_acc: 0.4007999897003174,  lr: 0.00532
step: 533, train_loss: 1.371476411819458, acc: 0.4025999903678894, val_loss: 1.3791090250015259, val_acc: 0.4041999876499176,  lr: 0.0053300000000000005
step: 534, train_loss: 1.376957654953003, acc: 0.397599995136261, val_loss: 1.37766695022583, val_acc: 0.40400001406669617,  lr: 0.00534
step: 535, train_loss: 1.3806740045547485, acc: 0.4025999903678894, val_loss: 1.3780237436294556, val_acc: 0.40720000863075256,  lr: 0.005350000000000001
step: 536, train_loss: 1.3717222213745117, acc: 0.40400001406669617, val_loss: 1.3750320672988892, val_acc: 0.40540000796318054,  lr: 0.00536
step: 537, train_loss: 1.375623106956482, acc: 0.40540000796318054, val_loss: 1.3790583610534668, val_acc: 0.4036000072956085,  lr: 0.005370000000000001
step: 538, train_loss: 1.3787024021148682, acc: 0.4002000093460083, val_loss: 1.3718398809432983, val_acc: 0.4027999937534332,  lr: 0.00538
step: 539, train_loss: 1.3909142017364502, acc: 0.39579999446868896, val_loss: 1.3727513551712036, val_acc: 0.40619999170303345,  lr: 0.005390000000000001
step: 540, train_loss: 1.3689522743225098, acc: 0.4004000127315521, val_loss: 1.375382423400879, val_acc: 0.4059999883174896,  lr: 0.0054
step: 541, train_loss: 1.3807733058929443, acc: 0.39579999446868896, val_loss: 1.3761874437332153, val_acc: 0.40139999985694885,  lr: 0.005410000000000001
step: 542, train_loss: 1.3760573863983154, acc: 0.40400001406669617, val_loss: 1.372267484664917, val_acc: 0.40540000796318054,  lr: 0.00542
step: 543, train_loss: 1.3798447847366333, acc: 0.4016000032424927, val_loss: 1.371445655822754, val_acc: 0.4058000147342682,  lr: 0.005430000000000001
step: 544, train_loss: 1.373515248298645, acc: 0.4059999883174896, val_loss: 1.373625636100769, val_acc: 0.4075999855995178,  lr: 0.00544
step: 545, train_loss: 1.3788573741912842, acc: 0.40220001339912415, val_loss: 1.3714311122894287, val_acc: 0.4058000147342682,  lr: 0.005450000000000001
step: 546, train_loss: 1.3921717405319214, acc: 0.39820000529289246, val_loss: 1.3757908344268799, val_acc: 0.40299999713897705,  lr: 0.0054600000000000004
step: 547, train_loss: 1.3771889209747314, acc: 0.40220001339912415, val_loss: 1.3764442205429077, val_acc: 0.4007999897003174,  lr: 0.005470000000000001
step: 548, train_loss: 1.37567138671875, acc: 0.4047999978065491, val_loss: 1.3763290643692017, val_acc: 0.4065999984741211,  lr: 0.0054800000000000005
step: 549, train_loss: 1.3813774585723877, acc: 0.39419999718666077, val_loss: 1.3764140605926514, val_acc: 0.4065999984741211,  lr: 0.005490000000000001
step: 550, train_loss: 1.3652595281600952, acc: 0.40299999713897705, val_loss: 1.3749215602874756, val_acc: 0.40799999237060547,  lr: 0.0055000000000000005
step: 551, train_loss: 1.370341420173645, acc: 0.40380001068115234, val_loss: 1.3767966032028198, val_acc: 0.4023999869823456,  lr: 0.00551
step: 552, train_loss: 1.3950562477111816, acc: 0.39579999446868896, val_loss: 1.3774853944778442, val_acc: 0.4020000100135803,  lr: 0.005520000000000001
step: 553, train_loss: 1.3857014179229736, acc: 0.39980000257492065, val_loss: 1.3759697675704956, val_acc: 0.40139999985694885,  lr: 0.00553
step: 554, train_loss: 1.367085576057434, acc: 0.40720000863075256, val_loss: 1.3771151304244995, val_acc: 0.4108000099658966,  lr: 0.005540000000000001
step: 555, train_loss: 1.3737050294876099, acc: 0.4041999876499176, val_loss: 1.3772594928741455, val_acc: 0.4036000072956085,  lr: 0.00555
step: 556, train_loss: 1.3659753799438477, acc: 0.40939998626708984, val_loss: 1.3764666318893433, val_acc: 0.40639999508857727,  lr: 0.005560000000000001
step: 557, train_loss: 1.3694108724594116, acc: 0.4043999910354614, val_loss: 1.3764398097991943, val_acc: 0.4058000147342682,  lr: 0.00557
step: 558, train_loss: 1.3664137125015259, acc: 0.40540000796318054, val_loss: 1.380003571510315, val_acc: 0.4016000032424927,  lr: 0.005580000000000001
step: 559, train_loss: 1.3701329231262207, acc: 0.40459999442100525, val_loss: 1.3847218751907349, val_acc: 0.4034000039100647,  lr: 0.00559
step: 560, train_loss: 1.3871403932571411, acc: 0.3991999924182892, val_loss: 1.3803409337997437, val_acc: 0.4020000100135803,  lr: 0.005600000000000001
step: 561, train_loss: 1.3672699928283691, acc: 0.40540000796318054, val_loss: 1.3832459449768066, val_acc: 0.4000000059604645,  lr: 0.00561
step: 562, train_loss: 1.3836209774017334, acc: 0.3962000012397766, val_loss: 1.3845148086547852, val_acc: 0.4041999876499176,  lr: 0.005620000000000001
step: 563, train_loss: 1.374731421470642, acc: 0.4115999937057495, val_loss: 1.3769687414169312, val_acc: 0.4068000018596649,  lr: 0.00563
step: 564, train_loss: 1.374267816543579, acc: 0.397599995136261, val_loss: 1.386794924736023, val_acc: 0.4025999903678894,  lr: 0.005639999999999999
step: 565, train_loss: 1.3743938207626343, acc: 0.4043999910354614, val_loss: 1.3826723098754883, val_acc: 0.40139999985694885,  lr: 0.00565
step: 566, train_loss: 1.3719035387039185, acc: 0.4018000066280365, val_loss: 1.3797756433486938, val_acc: 0.40400001406669617,  lr: 0.005659999999999999
step: 567, train_loss: 1.364434003829956, acc: 0.4065999984741211, val_loss: 1.383533000946045, val_acc: 0.4036000072956085,  lr: 0.00567
step: 568, train_loss: 1.3868486881256104, acc: 0.3970000147819519, val_loss: 1.3774569034576416, val_acc: 0.40059998631477356,  lr: 0.005679999999999999
step: 569, train_loss: 1.3643378019332886, acc: 0.4000000059604645, val_loss: 1.379673719406128, val_acc: 0.40119999647140503,  lr: 0.00569
step: 570, train_loss: 1.3686723709106445, acc: 0.4131999909877777, val_loss: 1.3769149780273438, val_acc: 0.4047999978065491,  lr: 0.005699999999999999
step: 571, train_loss: 1.3592169284820557, acc: 0.4065999984741211, val_loss: 1.383885383605957, val_acc: 0.4027999937534332,  lr: 0.00571
step: 572, train_loss: 1.3797211647033691, acc: 0.4004000127315521, val_loss: 1.3763352632522583, val_acc: 0.4074000120162964,  lr: 0.005719999999999999
step: 573, train_loss: 1.3604167699813843, acc: 0.4115999937057495, val_loss: 1.3773834705352783, val_acc: 0.3991999924182892,  lr: 0.00573
step: 574, train_loss: 1.3770676851272583, acc: 0.40139999985694885, val_loss: 1.3830052614212036, val_acc: 0.39879998564720154,  lr: 0.0057399999999999994
step: 575, train_loss: 1.387789011001587, acc: 0.39660000801086426, val_loss: 1.384741187095642, val_acc: 0.4036000072956085,  lr: 0.00575
step: 576, train_loss: 1.3707084655761719, acc: 0.3991999924182892, val_loss: 1.374466061592102, val_acc: 0.4009999930858612,  lr: 0.0057599999999999995
step: 577, train_loss: 1.3699772357940674, acc: 0.4059999883174896, val_loss: 1.3771473169326782, val_acc: 0.4058000147342682,  lr: 0.00577
step: 578, train_loss: 1.3771982192993164, acc: 0.40619999170303345, val_loss: 1.3797496557235718, val_acc: 0.40119999647140503,  lr: 0.0057799999999999995
step: 579, train_loss: 1.3680449724197388, acc: 0.40540000796318054, val_loss: 1.3780925273895264, val_acc: 0.40560001134872437,  lr: 0.00579
step: 580, train_loss: 1.370138168334961, acc: 0.4099999964237213, val_loss: 1.3807209730148315, val_acc: 0.39879998564720154,  lr: 0.0058
step: 581, train_loss: 1.3820569515228271, acc: 0.40139999985694885, val_loss: 1.3763805627822876, val_acc: 0.4025999903678894,  lr: 0.00581
step: 582, train_loss: 1.3629714250564575, acc: 0.4027999937534332, val_loss: 1.378659963607788, val_acc: 0.4068000018596649,  lr: 0.00582
step: 583, train_loss: 1.3582260608673096, acc: 0.4090000092983246, val_loss: 1.3801310062408447, val_acc: 0.4059999883174896,  lr: 0.00583
step: 584, train_loss: 1.364362120628357, acc: 0.40880000591278076, val_loss: 1.3764526844024658, val_acc: 0.4052000045776367,  lr: 0.00584
step: 585, train_loss: 1.3684263229370117, acc: 0.4023999869823456, val_loss: 1.3794126510620117, val_acc: 0.40400001406669617,  lr: 0.00585
step: 586, train_loss: 1.3677570819854736, acc: 0.4083999991416931, val_loss: 1.38507878780365, val_acc: 0.3962000012397766,  lr: 0.00586
step: 587, train_loss: 1.3808879852294922, acc: 0.397599995136261, val_loss: 1.374415636062622, val_acc: 0.40459999442100525,  lr: 0.00587
step: 588, train_loss: 1.3637828826904297, acc: 0.4074000120162964, val_loss: 1.385377287864685, val_acc: 0.4018000066280365,  lr: 0.00588
step: 589, train_loss: 1.3823952674865723, acc: 0.4043999910354614, val_loss: 1.3796794414520264, val_acc: 0.3968000113964081,  lr: 0.005889999999999999
step: 590, train_loss: 1.3742526769638062, acc: 0.40119999647140503, val_loss: 1.3827612400054932, val_acc: 0.40139999985694885,  lr: 0.0059
step: 591, train_loss: 1.3744213581085205, acc: 0.40860000252723694, val_loss: 1.3813389539718628, val_acc: 0.4004000127315521,  lr: 0.0059099999999999995
step: 592, train_loss: 1.3815339803695679, acc: 0.3962000012397766, val_loss: 1.379270076751709, val_acc: 0.399399995803833,  lr: 0.00592
step: 593, train_loss: 1.3729411363601685, acc: 0.41040000319480896, val_loss: 1.3762058019638062, val_acc: 0.40459999442100525,  lr: 0.0059299999999999995
step: 594, train_loss: 1.3571579456329346, acc: 0.4129999876022339, val_loss: 1.3765389919281006, val_acc: 0.4009999930858612,  lr: 0.00594
step: 595, train_loss: 1.3709330558776855, acc: 0.4020000100135803, val_loss: 1.3732227087020874, val_acc: 0.40459999442100525,  lr: 0.0059499999999999996
step: 596, train_loss: 1.3655260801315308, acc: 0.4083999991416931, val_loss: 1.3708240985870361, val_acc: 0.40459999442100525,  lr: 0.00596
step: 597, train_loss: 1.3673042058944702, acc: 0.40700000524520874, val_loss: 1.3733396530151367, val_acc: 0.40639999508857727,  lr: 0.00597
step: 598, train_loss: 1.369337558746338, acc: 0.4083999991416931, val_loss: 1.3763872385025024, val_acc: 0.3986000120639801,  lr: 0.00598
step: 599, train_loss: 1.3625773191452026, acc: 0.41519999504089355, val_loss: 1.3752065896987915, val_acc: 0.4124000072479248,  lr: 0.00599
step: 600, train_loss: 1.3605962991714478, acc: 0.4146000146865845, val_loss: 1.378112554550171, val_acc: 0.4074000120162964,  lr: 0.006
step: 601, train_loss: 1.374847650527954, acc: 0.399399995803833, val_loss: 1.379520058631897, val_acc: 0.40459999442100525,  lr: 0.00601
step: 602, train_loss: 1.3736753463745117, acc: 0.4041999876499176, val_loss: 1.3806438446044922, val_acc: 0.40799999237060547,  lr: 0.00602
step: 603, train_loss: 1.3580358028411865, acc: 0.4138000011444092, val_loss: 1.3740870952606201, val_acc: 0.40779998898506165,  lr: 0.00603
step: 604, train_loss: 1.365597128868103, acc: 0.4043999910354614, val_loss: 1.3758642673492432, val_acc: 0.40380001068115234,  lr: 0.00604
step: 605, train_loss: 1.3849279880523682, acc: 0.4025999903678894, val_loss: 1.3724958896636963, val_acc: 0.40220001339912415,  lr: 0.00605
step: 606, train_loss: 1.3682258129119873, acc: 0.41100001335144043, val_loss: 1.374855875968933, val_acc: 0.40619999170303345,  lr: 0.00606
step: 607, train_loss: 1.3737925291061401, acc: 0.39980000257492065, val_loss: 1.3712046146392822, val_acc: 0.40400001406669617,  lr: 0.00607
step: 608, train_loss: 1.3824049234390259, acc: 0.40119999647140503, val_loss: 1.3700677156448364, val_acc: 0.4041999876499176,  lr: 0.00608
step: 609, train_loss: 1.3693519830703735, acc: 0.4009999930858612, val_loss: 1.371628999710083, val_acc: 0.40700000524520874,  lr: 0.00609
step: 610, train_loss: 1.370287299156189, acc: 0.4050000011920929, val_loss: 1.370369791984558, val_acc: 0.4047999978065491,  lr: 0.0061
step: 611, train_loss: 1.3696790933609009, acc: 0.40139999985694885, val_loss: 1.3679218292236328, val_acc: 0.4090000092983246,  lr: 0.00611
step: 612, train_loss: 1.3673797845840454, acc: 0.4052000045776367, val_loss: 1.3662745952606201, val_acc: 0.40560001134872437,  lr: 0.0061200000000000004
step: 613, train_loss: 1.3633582592010498, acc: 0.41119998693466187, val_loss: 1.3693193197250366, val_acc: 0.4052000045776367,  lr: 0.00613
step: 614, train_loss: 1.3670843839645386, acc: 0.40459999442100525, val_loss: 1.368478536605835, val_acc: 0.4092000126838684,  lr: 0.00614
step: 615, train_loss: 1.354922890663147, acc: 0.41200000047683716, val_loss: 1.3698697090148926, val_acc: 0.40880000591278076,  lr: 0.00615
step: 616, train_loss: 1.3691582679748535, acc: 0.4083999991416931, val_loss: 1.373185396194458, val_acc: 0.40560001134872437,  lr: 0.00616
step: 617, train_loss: 1.3584831953048706, acc: 0.41280001401901245, val_loss: 1.365875005722046, val_acc: 0.4065999984741211,  lr: 0.00617
step: 618, train_loss: 1.3608728647232056, acc: 0.4106000065803528, val_loss: 1.3687806129455566, val_acc: 0.40779998898506165,  lr: 0.00618
step: 619, train_loss: 1.3723315000534058, acc: 0.40560001134872437, val_loss: 1.3683102130889893, val_acc: 0.41040000319480896,  lr: 0.00619
step: 620, train_loss: 1.359315276145935, acc: 0.41600000858306885, val_loss: 1.3652217388153076, val_acc: 0.4081999957561493,  lr: 0.0062
step: 621, train_loss: 1.379361629486084, acc: 0.4009999930858612, val_loss: 1.3701311349868774, val_acc: 0.4059999883174896,  lr: 0.00621
step: 622, train_loss: 1.3565524816513062, acc: 0.41339999437332153, val_loss: 1.3813353776931763, val_acc: 0.40059998631477356,  lr: 0.00622
step: 623, train_loss: 1.3732848167419434, acc: 0.39419999718666077, val_loss: 1.3703395128250122, val_acc: 0.4075999855995178,  lr: 0.00623
step: 624, train_loss: 1.367211103439331, acc: 0.41280001401901245, val_loss: 1.3687125444412231, val_acc: 0.4083999991416931,  lr: 0.00624
step: 625, train_loss: 1.3546086549758911, acc: 0.4108000099658966, val_loss: 1.3704147338867188, val_acc: 0.4068000018596649,  lr: 0.00625
step: 626, train_loss: 1.3593628406524658, acc: 0.4163999855518341, val_loss: 1.3675096035003662, val_acc: 0.4074000120162964,  lr: 0.00626
step: 627, train_loss: 1.370511531829834, acc: 0.4047999978065491, val_loss: 1.3677926063537598, val_acc: 0.4036000072956085,  lr: 0.00627
step: 628, train_loss: 1.37669837474823, acc: 0.40400001406669617, val_loss: 1.3679536581039429, val_acc: 0.4036000072956085,  lr: 0.00628
step: 629, train_loss: 1.3599612712860107, acc: 0.4142000079154968, val_loss: 1.3705854415893555, val_acc: 0.4023999869823456,  lr: 0.0062900000000000005
step: 630, train_loss: 1.3725734949111938, acc: 0.39980000257492065, val_loss: 1.3667207956314087, val_acc: 0.40139999985694885,  lr: 0.0063
step: 631, train_loss: 1.349027395248413, acc: 0.4097999930381775, val_loss: 1.3661376237869263, val_acc: 0.4043999910354614,  lr: 0.0063100000000000005
step: 632, train_loss: 1.362119197845459, acc: 0.4058000147342682, val_loss: 1.36670982837677, val_acc: 0.4058000147342682,  lr: 0.00632
step: 633, train_loss: 1.3666220903396606, acc: 0.40220001339912415, val_loss: 1.360855221748352, val_acc: 0.40619999170303345,  lr: 0.0063300000000000006
step: 634, train_loss: 1.3517320156097412, acc: 0.412200003862381, val_loss: 1.3609776496887207, val_acc: 0.40959998965263367,  lr: 0.00634
step: 635, train_loss: 1.3717291355133057, acc: 0.4041999876499176, val_loss: 1.3610223531723022, val_acc: 0.40799999237060547,  lr: 0.006350000000000001
step: 636, train_loss: 1.3589460849761963, acc: 0.40939998626708984, val_loss: 1.3582085371017456, val_acc: 0.4124000072479248,  lr: 0.00636
step: 637, train_loss: 1.358340859413147, acc: 0.4131999909877777, val_loss: 1.358084797859192, val_acc: 0.40939998626708984,  lr: 0.006370000000000001
step: 638, train_loss: 1.3579256534576416, acc: 0.4059999883174896, val_loss: 1.3562828302383423, val_acc: 0.4113999903202057,  lr: 0.00638
step: 639, train_loss: 1.3578096628189087, acc: 0.4020000100135803, val_loss: 1.355757474899292, val_acc: 0.4169999957084656,  lr: 0.00639
step: 640, train_loss: 1.347975254058838, acc: 0.41359999775886536, val_loss: 1.3539764881134033, val_acc: 0.4142000079154968,  lr: 0.0064
step: 641, train_loss: 1.3629050254821777, acc: 0.40639999508857727, val_loss: 1.3528292179107666, val_acc: 0.4142000079154968,  lr: 0.00641
step: 642, train_loss: 1.352273941040039, acc: 0.4163999855518341, val_loss: 1.3558393716812134, val_acc: 0.4146000146865845,  lr: 0.00642
step: 643, train_loss: 1.3591516017913818, acc: 0.41920000314712524, val_loss: 1.3766120672225952, val_acc: 0.4092000126838684,  lr: 0.00643
step: 644, train_loss: 1.3943283557891846, acc: 0.3901999890804291, val_loss: 1.3596594333648682, val_acc: 0.4113999903202057,  lr: 0.00644
step: 645, train_loss: 1.3696643114089966, acc: 0.4050000011920929, val_loss: 1.369271159172058, val_acc: 0.4113999903202057,  lr: 0.00645
step: 646, train_loss: 1.3870257139205933, acc: 0.39899998903274536, val_loss: 1.3623783588409424, val_acc: 0.41260001063346863,  lr: 0.0064600000000000005
step: 647, train_loss: 1.3575242757797241, acc: 0.4129999876022339, val_loss: 1.3691359758377075, val_acc: 0.41200000047683716,  lr: 0.00647
step: 648, train_loss: 1.3791981935501099, acc: 0.4050000011920929, val_loss: 1.3561971187591553, val_acc: 0.41040000319480896,  lr: 0.0064800000000000005
step: 649, train_loss: 1.3562276363372803, acc: 0.412200003862381, val_loss: 1.3662145137786865, val_acc: 0.3984000086784363,  lr: 0.00649
step: 650, train_loss: 1.3668328523635864, acc: 0.4016000032424927, val_loss: 1.3580427169799805, val_acc: 0.4097999930381775,  lr: 0.006500000000000001
step: 651, train_loss: 1.359667420387268, acc: 0.4090000092983246, val_loss: 1.3632363080978394, val_acc: 0.4124000072479248,  lr: 0.00651
step: 652, train_loss: 1.3648630380630493, acc: 0.41179999709129333, val_loss: 1.3677921295166016, val_acc: 0.4113999903202057,  lr: 0.006520000000000001
step: 653, train_loss: 1.3629428148269653, acc: 0.4002000093460083, val_loss: 1.3588817119598389, val_acc: 0.4065999984741211,  lr: 0.00653
step: 654, train_loss: 1.3668246269226074, acc: 0.40119999647140503, val_loss: 1.3600454330444336, val_acc: 0.4097999930381775,  lr: 0.006540000000000001
step: 655, train_loss: 1.3567466735839844, acc: 0.4108000099658966, val_loss: 1.3649777173995972, val_acc: 0.40779998898506165,  lr: 0.00655
step: 656, train_loss: 1.3660956621170044, acc: 0.40220001339912415, val_loss: 1.3637505769729614, val_acc: 0.414000004529953,  lr: 0.006560000000000001
step: 657, train_loss: 1.366456389427185, acc: 0.40700000524520874, val_loss: 1.3643709421157837, val_acc: 0.40939998626708984,  lr: 0.00657
step: 658, train_loss: 1.3485878705978394, acc: 0.415800005197525, val_loss: 1.3600683212280273, val_acc: 0.4075999855995178,  lr: 0.006580000000000001
step: 659, train_loss: 1.354659080505371, acc: 0.41760000586509705, val_loss: 1.3646374940872192, val_acc: 0.40540000796318054,  lr: 0.00659
step: 660, train_loss: 1.35700261592865, acc: 0.4108000099658966, val_loss: 1.3629753589630127, val_acc: 0.40959998965263367,  lr: 0.006600000000000001
step: 661, train_loss: 1.3667595386505127, acc: 0.39820000529289246, val_loss: 1.3621686697006226, val_acc: 0.4115999937057495,  lr: 0.00661
step: 662, train_loss: 1.3609066009521484, acc: 0.4050000011920929, val_loss: 1.3680825233459473, val_acc: 0.40560001134872437,  lr: 0.006620000000000001
step: 663, train_loss: 1.3592196702957153, acc: 0.414000004529953, val_loss: 1.3591135740280151, val_acc: 0.40799999237060547,  lr: 0.0066300000000000005
step: 664, train_loss: 1.354673981666565, acc: 0.41620001196861267, val_loss: 1.3679252862930298, val_acc: 0.40700000524520874,  lr: 0.00664
step: 665, train_loss: 1.3590798377990723, acc: 0.4142000079154968, val_loss: 1.3644676208496094, val_acc: 0.40619999170303345,  lr: 0.0066500000000000005
step: 666, train_loss: 1.355983018875122, acc: 0.4083999991416931, val_loss: 1.3628675937652588, val_acc: 0.4043999910354614,  lr: 0.00666
step: 667, train_loss: 1.3536607027053833, acc: 0.412200003862381, val_loss: 1.3662984371185303, val_acc: 0.4043999910354614,  lr: 0.006670000000000001
step: 668, train_loss: 1.3509070873260498, acc: 0.40860000252723694, val_loss: 1.3633296489715576, val_acc: 0.4025999903678894,  lr: 0.00668
step: 669, train_loss: 1.3530341386795044, acc: 0.41600000858306885, val_loss: 1.3640336990356445, val_acc: 0.40959998965263367,  lr: 0.006690000000000001
step: 670, train_loss: 1.3491796255111694, acc: 0.4147999882698059, val_loss: 1.3670382499694824, val_acc: 0.40619999170303345,  lr: 0.0067
step: 671, train_loss: 1.3540396690368652, acc: 0.4081999957561493, val_loss: 1.3712371587753296, val_acc: 0.40220001339912415,  lr: 0.006710000000000001
step: 672, train_loss: 1.3513151407241821, acc: 0.41440001130104065, val_loss: 1.3654024600982666, val_acc: 0.40459999442100525,  lr: 0.00672
step: 673, train_loss: 1.3580023050308228, acc: 0.4129999876022339, val_loss: 1.376790165901184, val_acc: 0.4043999910354614,  lr: 0.006730000000000001
step: 674, train_loss: 1.351857304573059, acc: 0.40779998898506165, val_loss: 1.3689274787902832, val_acc: 0.40459999442100525,  lr: 0.00674
step: 675, train_loss: 1.3498804569244385, acc: 0.41819998621940613, val_loss: 1.368483066558838, val_acc: 0.40119999647140503,  lr: 0.006750000000000001
step: 676, train_loss: 1.3732880353927612, acc: 0.4059999883174896, val_loss: 1.3610939979553223, val_acc: 0.41019999980926514,  lr: 0.00676
step: 677, train_loss: 1.3438204526901245, acc: 0.415800005197525, val_loss: 1.3655011653900146, val_acc: 0.4074000120162964,  lr: 0.006770000000000001
step: 678, train_loss: 1.3885878324508667, acc: 0.3935999870300293, val_loss: 1.3612914085388184, val_acc: 0.4068000018596649,  lr: 0.0067800000000000004
step: 679, train_loss: 1.3428523540496826, acc: 0.4185999929904938, val_loss: 1.3633464574813843, val_acc: 0.40619999170303345,  lr: 0.006790000000000001
step: 680, train_loss: 1.3446968793869019, acc: 0.4142000079154968, val_loss: 1.3656116724014282, val_acc: 0.40700000524520874,  lr: 0.0068000000000000005
step: 681, train_loss: 1.3654327392578125, acc: 0.41179999709129333, val_loss: 1.3567970991134644, val_acc: 0.4065999984741211,  lr: 0.006810000000000001
step: 682, train_loss: 1.3561939001083374, acc: 0.41280001401901245, val_loss: 1.3565118312835693, val_acc: 0.40400001406669617,  lr: 0.0068200000000000005
step: 683, train_loss: 1.3385568857192993, acc: 0.41260001063346863, val_loss: 1.3617674112319946, val_acc: 0.40139999985694885,  lr: 0.006830000000000001
step: 684, train_loss: 1.3589731454849243, acc: 0.4009999930858612, val_loss: 1.3624510765075684, val_acc: 0.4036000072956085,  lr: 0.006840000000000001
step: 685, train_loss: 1.350525140762329, acc: 0.41179999709129333, val_loss: 1.3715970516204834, val_acc: 0.4034000039100647,  lr: 0.006850000000000001
step: 686, train_loss: 1.3910919427871704, acc: 0.39419999718666077, val_loss: 1.3640772104263306, val_acc: 0.4074000120162964,  lr: 0.006860000000000001
step: 687, train_loss: 1.3451465368270874, acc: 0.41819998621940613, val_loss: 1.3648464679718018, val_acc: 0.4065999984741211,  lr: 0.006870000000000001
step: 688, train_loss: 1.3364864587783813, acc: 0.4203999936580658, val_loss: 1.367171049118042, val_acc: 0.4074000120162964,  lr: 0.00688
step: 689, train_loss: 1.3651374578475952, acc: 0.4059999883174896, val_loss: 1.3679311275482178, val_acc: 0.4081999957561493,  lr: 0.006889999999999999
step: 690, train_loss: 1.3501687049865723, acc: 0.4153999984264374, val_loss: 1.3617444038391113, val_acc: 0.4065999984741211,  lr: 0.0069
step: 691, train_loss: 1.3301197290420532, acc: 0.4171999990940094, val_loss: 1.3627983331680298, val_acc: 0.4007999897003174,  lr: 0.0069099999999999995
step: 692, train_loss: 1.3473628759384155, acc: 0.4235999882221222, val_loss: 1.3669956922531128, val_acc: 0.40139999985694885,  lr: 0.00692
step: 693, train_loss: 1.342437982559204, acc: 0.4156000018119812, val_loss: 1.3654214143753052, val_acc: 0.40400001406669617,  lr: 0.0069299999999999995
step: 694, train_loss: 1.3636314868927002, acc: 0.4099999964237213, val_loss: 1.3647761344909668, val_acc: 0.4081999957561493,  lr: 0.00694
step: 695, train_loss: 1.358889102935791, acc: 0.4115999937057495, val_loss: 1.3960192203521729, val_acc: 0.39640000462532043,  lr: 0.00695
step: 696, train_loss: 1.3899903297424316, acc: 0.4099999964237213, val_loss: 1.3857941627502441, val_acc: 0.40560001134872437,  lr: 0.00696
step: 697, train_loss: 1.3590471744537354, acc: 0.40779998898506165, val_loss: 1.371855616569519, val_acc: 0.4075999855995178,  lr: 0.00697
step: 698, train_loss: 1.3661162853240967, acc: 0.40380001068115234, val_loss: 1.3805550336837769, val_acc: 0.40220001339912415,  lr: 0.00698
step: 699, train_loss: 1.3792524337768555, acc: 0.4041999876499176, val_loss: 1.3707606792449951, val_acc: 0.4090000092983246,  lr: 0.00699
step: 700, train_loss: 1.3577165603637695, acc: 0.41119998693466187, val_loss: 1.3650504350662231, val_acc: 0.4027999937534332,  lr: 0.006999999999999999
step: 701, train_loss: 1.3538000583648682, acc: 0.4146000146865845, val_loss: 1.3683820962905884, val_acc: 0.40299999713897705,  lr: 0.00701
step: 702, train_loss: 1.361167311668396, acc: 0.41280001401901245, val_loss: 1.3675107955932617, val_acc: 0.40860000252723694,  lr: 0.007019999999999999
step: 703, train_loss: 1.3476263284683228, acc: 0.41600000858306885, val_loss: 1.3714901208877563, val_acc: 0.39800000190734863,  lr: 0.00703
step: 704, train_loss: 1.3618359565734863, acc: 0.40220001339912415, val_loss: 1.3656946420669556, val_acc: 0.3968000113964081,  lr: 0.007039999999999999
step: 705, train_loss: 1.356985330581665, acc: 0.4068000018596649, val_loss: 1.363922119140625, val_acc: 0.4043999910354614,  lr: 0.00705
step: 706, train_loss: 1.3576723337173462, acc: 0.41600000858306885, val_loss: 1.3653360605239868, val_acc: 0.40560001134872437,  lr: 0.0070599999999999994
step: 707, train_loss: 1.3578743934631348, acc: 0.4124000072479248, val_loss: 1.3605824708938599, val_acc: 0.4115999937057495,  lr: 0.00707
step: 708, train_loss: 1.3339252471923828, acc: 0.42399999499320984, val_loss: 1.364410400390625, val_acc: 0.4106000065803528,  lr: 0.0070799999999999995
step: 709, train_loss: 1.3378978967666626, acc: 0.4253999888896942, val_loss: 1.362447738647461, val_acc: 0.40459999442100525,  lr: 0.00709
step: 710, train_loss: 1.3370378017425537, acc: 0.41179999709129333, val_loss: 1.3620191812515259, val_acc: 0.40380001068115234,  lr: 0.0070999999999999995
step: 711, train_loss: 1.3469698429107666, acc: 0.4115999937057495, val_loss: 1.3593064546585083, val_acc: 0.40939998626708984,  lr: 0.00711
step: 712, train_loss: 1.3579862117767334, acc: 0.40779998898506165, val_loss: 1.3613288402557373, val_acc: 0.4138000011444092,  lr: 0.00712
step: 713, train_loss: 1.3324369192123413, acc: 0.415800005197525, val_loss: 1.366564393043518, val_acc: 0.4058000147342682,  lr: 0.00713
step: 714, train_loss: 1.3403857946395874, acc: 0.4081999957561493, val_loss: 1.3606595993041992, val_acc: 0.40619999170303345,  lr: 0.00714
step: 715, train_loss: 1.3451741933822632, acc: 0.42419999837875366, val_loss: 1.3633136749267578, val_acc: 0.40400001406669617,  lr: 0.00715
step: 716, train_loss: 1.3242380619049072, acc: 0.42419999837875366, val_loss: 1.3730244636535645, val_acc: 0.4034000039100647,  lr: 0.00716
step: 717, train_loss: 1.3380297422409058, acc: 0.4185999929904938, val_loss: 1.3815226554870605, val_acc: 0.3991999924182892,  lr: 0.00717
step: 718, train_loss: 1.3569743633270264, acc: 0.41359999775886536, val_loss: 1.4131053686141968, val_acc: 0.3937999904155731,  lr: 0.00718
step: 719, train_loss: 1.421325922012329, acc: 0.3952000141143799, val_loss: 1.3796098232269287, val_acc: 0.4050000011920929,  lr: 0.00719
step: 720, train_loss: 1.3702014684677124, acc: 0.40639999508857727, val_loss: 1.3981060981750488, val_acc: 0.39559999108314514,  lr: 0.0072
step: 721, train_loss: 1.4037805795669556, acc: 0.39399999380111694, val_loss: 1.3837478160858154, val_acc: 0.39559999108314514,  lr: 0.00721
step: 722, train_loss: 1.3583978414535522, acc: 0.41519999504089355, val_loss: 1.3788979053497314, val_acc: 0.39719998836517334,  lr: 0.00722
step: 723, train_loss: 1.3649790287017822, acc: 0.40720000863075256, val_loss: 1.3853363990783691, val_acc: 0.4009999930858612,  lr: 0.00723
step: 724, train_loss: 1.3613052368164062, acc: 0.399399995803833, val_loss: 1.374711513519287, val_acc: 0.40560001134872437,  lr: 0.00724
step: 725, train_loss: 1.3440654277801514, acc: 0.42080000042915344, val_loss: 1.3707375526428223, val_acc: 0.39800000190734863,  lr: 0.0072499999999999995
step: 726, train_loss: 1.3650568723678589, acc: 0.4002000093460083, val_loss: 1.3785185813903809, val_acc: 0.39899998903274536,  lr: 0.00726
step: 727, train_loss: 1.3480726480484009, acc: 0.414000004529953, val_loss: 1.367497205734253, val_acc: 0.40540000796318054,  lr: 0.0072699999999999996
step: 728, train_loss: 1.3577052354812622, acc: 0.40779998898506165, val_loss: 1.371306300163269, val_acc: 0.4018000066280365,  lr: 0.00728
step: 729, train_loss: 1.3637030124664307, acc: 0.40299999713897705, val_loss: 1.3654085397720337, val_acc: 0.4043999910354614,  lr: 0.00729
step: 730, train_loss: 1.3349756002426147, acc: 0.420199990272522, val_loss: 1.3676152229309082, val_acc: 0.40380001068115234,  lr: 0.0073
step: 731, train_loss: 1.3467031717300415, acc: 0.41519999504089355, val_loss: 1.3636571168899536, val_acc: 0.4041999876499176,  lr: 0.00731
step: 732, train_loss: 1.3322659730911255, acc: 0.4187999963760376, val_loss: 1.3610607385635376, val_acc: 0.4052000045776367,  lr: 0.00732
step: 733, train_loss: 1.3248401880264282, acc: 0.421999990940094, val_loss: 1.3639416694641113, val_acc: 0.40720000863075256,  lr: 0.00733
step: 734, train_loss: 1.3781012296676636, acc: 0.40939998626708984, val_loss: 1.3642202615737915, val_acc: 0.4036000072956085,  lr: 0.00734
step: 735, train_loss: 1.3541241884231567, acc: 0.4016000032424927, val_loss: 1.360515832901001, val_acc: 0.40779998898506165,  lr: 0.00735
step: 736, train_loss: 1.3482385873794556, acc: 0.4203999936580658, val_loss: 1.3599215745925903, val_acc: 0.4113999903202057,  lr: 0.00736
step: 737, train_loss: 1.3217413425445557, acc: 0.42820000648498535, val_loss: 1.361025333404541, val_acc: 0.40720000863075256,  lr: 0.00737
step: 738, train_loss: 1.3764636516571045, acc: 0.4041999876499176, val_loss: 1.3585261106491089, val_acc: 0.4059999883174896,  lr: 0.00738
step: 739, train_loss: 1.3267332315444946, acc: 0.42080000042915344, val_loss: 1.3557780981063843, val_acc: 0.41040000319480896,  lr: 0.00739
step: 740, train_loss: 1.3575425148010254, acc: 0.40639999508857727, val_loss: 1.3582484722137451, val_acc: 0.4052000045776367,  lr: 0.0074
step: 741, train_loss: 1.3281344175338745, acc: 0.42239999771118164, val_loss: 1.358430027961731, val_acc: 0.4047999978065491,  lr: 0.00741
step: 742, train_loss: 1.3737834692001343, acc: 0.39800000190734863, val_loss: 1.3591256141662598, val_acc: 0.4047999978065491,  lr: 0.00742
step: 743, train_loss: 1.3392096757888794, acc: 0.421999990940094, val_loss: 1.3628736734390259, val_acc: 0.40220001339912415,  lr: 0.00743
step: 744, train_loss: 1.356447696685791, acc: 0.40380001068115234, val_loss: 1.3578590154647827, val_acc: 0.40459999442100525,  lr: 0.00744
step: 745, train_loss: 1.3559081554412842, acc: 0.4138000011444092, val_loss: 1.3547594547271729, val_acc: 0.40720000863075256,  lr: 0.00745
step: 746, train_loss: 1.3353794813156128, acc: 0.42399999499320984, val_loss: 1.3589396476745605, val_acc: 0.4065999984741211,  lr: 0.0074600000000000005
step: 747, train_loss: 1.371886134147644, acc: 0.39579999446868896, val_loss: 1.3619368076324463, val_acc: 0.40779998898506165,  lr: 0.00747
step: 748, train_loss: 1.3804993629455566, acc: 0.39419999718666077, val_loss: 1.3501882553100586, val_acc: 0.4090000092983246,  lr: 0.0074800000000000005
step: 749, train_loss: 1.3431099653244019, acc: 0.41040000319480896, val_loss: 1.3576017618179321, val_acc: 0.41119998693466187,  lr: 0.00749
step: 750, train_loss: 1.3475139141082764, acc: 0.41920000314712524, val_loss: 1.3507771492004395, val_acc: 0.41440001130104065,  lr: 0.0075
step: 751, train_loss: 1.3491467237472534, acc: 0.40860000252723694, val_loss: 1.3504083156585693, val_acc: 0.41040000319480896,  lr: 0.00751
step: 752, train_loss: 1.347835898399353, acc: 0.4099999964237213, val_loss: 1.34676992893219, val_acc: 0.41260001063346863,  lr: 0.00752
step: 753, train_loss: 1.3388020992279053, acc: 0.41179999709129333, val_loss: 1.3487440347671509, val_acc: 0.41260001063346863,  lr: 0.00753
step: 754, train_loss: 1.3179975748062134, acc: 0.4275999963283539, val_loss: 1.3505865335464478, val_acc: 0.41260001063346863,  lr: 0.00754
step: 755, train_loss: 1.355031967163086, acc: 0.41760000586509705, val_loss: 1.355682611465454, val_acc: 0.4115999937057495,  lr: 0.00755
step: 756, train_loss: 1.3504749536514282, acc: 0.42179998755455017, val_loss: 1.3729757070541382, val_acc: 0.4004000127315521,  lr: 0.00756
step: 757, train_loss: 1.3660777807235718, acc: 0.4083999991416931, val_loss: 1.376672625541687, val_acc: 0.40459999442100525,  lr: 0.00757
step: 758, train_loss: 1.3716723918914795, acc: 0.39899998903274536, val_loss: 1.3635790348052979, val_acc: 0.41260001063346863,  lr: 0.00758
step: 759, train_loss: 1.3544838428497314, acc: 0.41200000047683716, val_loss: 1.3645191192626953, val_acc: 0.4075999855995178,  lr: 0.00759
step: 760, train_loss: 1.3809462785720825, acc: 0.39980000257492065, val_loss: 1.367544412612915, val_acc: 0.4065999984741211,  lr: 0.0076
step: 761, train_loss: 1.371613621711731, acc: 0.40299999713897705, val_loss: 1.3688932657241821, val_acc: 0.4036000072956085,  lr: 0.0076100000000000004
step: 762, train_loss: 1.3691548109054565, acc: 0.4124000072479248, val_loss: 1.3543379306793213, val_acc: 0.4108000099658966,  lr: 0.00762
step: 763, train_loss: 1.3593999147415161, acc: 0.40639999508857727, val_loss: 1.360687494277954, val_acc: 0.4065999984741211,  lr: 0.0076300000000000005
step: 764, train_loss: 1.3728843927383423, acc: 0.4050000011920929, val_loss: 1.3521578311920166, val_acc: 0.4131999909877777,  lr: 0.00764
step: 765, train_loss: 1.3444238901138306, acc: 0.4228000044822693, val_loss: 1.3530972003936768, val_acc: 0.41679999232292175,  lr: 0.0076500000000000005
step: 766, train_loss: 1.3533875942230225, acc: 0.40779998898506165, val_loss: 1.352445363998413, val_acc: 0.41359999775886536,  lr: 0.00766
step: 767, train_loss: 1.3529207706451416, acc: 0.412200003862381, val_loss: 1.3495985269546509, val_acc: 0.41999998688697815,  lr: 0.007670000000000001
step: 768, train_loss: 1.3606921434402466, acc: 0.4124000072479248, val_loss: 1.3410104513168335, val_acc: 0.41760000586509705,  lr: 0.00768
step: 769, train_loss: 1.3403764963150024, acc: 0.41819998621940613, val_loss: 1.3451179265975952, val_acc: 0.4163999855518341,  lr: 0.007690000000000001
step: 770, train_loss: 1.3532472848892212, acc: 0.4131999909877777, val_loss: 1.3420259952545166, val_acc: 0.41679999232292175,  lr: 0.0077
step: 771, train_loss: 1.3402413129806519, acc: 0.4115999937057495, val_loss: 1.3380553722381592, val_acc: 0.4212000072002411,  lr: 0.007710000000000001
step: 772, train_loss: 1.3450477123260498, acc: 0.41920000314712524, val_loss: 1.3395782709121704, val_acc: 0.41999998688697815,  lr: 0.00772
step: 773, train_loss: 1.347370982170105, acc: 0.415800005197525, val_loss: 1.3353651762008667, val_acc: 0.4221999943256378,  lr: 0.007730000000000001
step: 774, train_loss: 1.3417669534683228, acc: 0.42100000381469727, val_loss: 1.3353570699691772, val_acc: 0.41600000858306885,  lr: 0.00774
step: 775, train_loss: 1.3363404273986816, acc: 0.4196000099182129, val_loss: 1.3361518383026123, val_acc: 0.4205999970436096,  lr: 0.007750000000000001
step: 776, train_loss: 1.3599960803985596, acc: 0.40700000524520874, val_loss: 1.3362241983413696, val_acc: 0.41819998621940613,  lr: 0.00776
step: 777, train_loss: 1.3419816493988037, acc: 0.42080000042915344, val_loss: 1.3387643098831177, val_acc: 0.4113999903202057,  lr: 0.00777
step: 778, train_loss: 1.3398768901824951, acc: 0.4171999990940094, val_loss: 1.3414324522018433, val_acc: 0.4131999909877777,  lr: 0.0077800000000000005
step: 779, train_loss: 1.3310120105743408, acc: 0.4275999963283539, val_loss: 1.3350249528884888, val_acc: 0.4205999970436096,  lr: 0.00779
step: 780, train_loss: 1.3288863897323608, acc: 0.42100000381469727, val_loss: 1.3342701196670532, val_acc: 0.4251999855041504,  lr: 0.0078000000000000005
step: 781, train_loss: 1.3498409986495972, acc: 0.4180000126361847, val_loss: 1.341671109199524, val_acc: 0.41780000925064087,  lr: 0.00781
step: 782, train_loss: 1.37577486038208, acc: 0.40059998631477356, val_loss: 1.3453173637390137, val_acc: 0.41999998688697815,  lr: 0.00782
step: 783, train_loss: 1.35379159450531, acc: 0.412200003862381, val_loss: 1.3436834812164307, val_acc: 0.4180000126361847,  lr: 0.00783
step: 784, train_loss: 1.3353750705718994, acc: 0.42100000381469727, val_loss: 1.351559042930603, val_acc: 0.4153999984264374,  lr: 0.00784
step: 785, train_loss: 1.3675365447998047, acc: 0.4047999978065491, val_loss: 1.3449455499649048, val_acc: 0.41440001130104065,  lr: 0.007850000000000001
step: 786, train_loss: 1.3393200635910034, acc: 0.41119998693466187, val_loss: 1.3508648872375488, val_acc: 0.41499999165534973,  lr: 0.00786
step: 787, train_loss: 1.3499692678451538, acc: 0.41839998960494995, val_loss: 1.353514552116394, val_acc: 0.4115999937057495,  lr: 0.00787
step: 788, train_loss: 1.3538657426834106, acc: 0.4189999997615814, val_loss: 1.349763035774231, val_acc: 0.41100001335144043,  lr: 0.00788
step: 789, train_loss: 1.3414157629013062, acc: 0.41920000314712524, val_loss: 1.344253659248352, val_acc: 0.4142000079154968,  lr: 0.007890000000000001
step: 790, train_loss: 1.328081488609314, acc: 0.42480000853538513, val_loss: 1.3427760601043701, val_acc: 0.41100001335144043,  lr: 0.0079
step: 791, train_loss: 1.3315926790237427, acc: 0.42419999837875366, val_loss: 1.3483459949493408, val_acc: 0.4124000072479248,  lr: 0.00791
step: 792, train_loss: 1.331597924232483, acc: 0.41999998688697815, val_loss: 1.3421459197998047, val_acc: 0.4108000099658966,  lr: 0.00792
step: 793, train_loss: 1.3175151348114014, acc: 0.4300000071525574, val_loss: 1.3417787551879883, val_acc: 0.4174000024795532,  lr: 0.007930000000000001
step: 794, train_loss: 1.324319839477539, acc: 0.42640000581741333, val_loss: 1.3441683053970337, val_acc: 0.4205999970436096,  lr: 0.007940000000000001
step: 795, train_loss: 1.3395408391952515, acc: 0.41920000314712524, val_loss: 1.3453588485717773, val_acc: 0.42179998755455017,  lr: 0.00795
step: 796, train_loss: 1.342329740524292, acc: 0.42160001397132874, val_loss: 1.3630313873291016, val_acc: 0.41359999775886536,  lr: 0.00796
step: 797, train_loss: 1.3848079442977905, acc: 0.39739999175071716, val_loss: 1.3508309125900269, val_acc: 0.414000004529953,  lr: 0.007970000000000001
step: 798, train_loss: 1.3236950635910034, acc: 0.4323999881744385, val_loss: 1.3562943935394287, val_acc: 0.41200000047683716,  lr: 0.007980000000000001
step: 799, train_loss: 1.3530821800231934, acc: 0.4189999997615814, val_loss: 1.3566837310791016, val_acc: 0.41780000925064087,  lr: 0.00799
step: 800, train_loss: 1.3642184734344482, acc: 0.40380001068115234, val_loss: 1.365949034690857, val_acc: 0.4113999903202057,  lr: 0.008
step: 801, train_loss: 1.3556666374206543, acc: 0.41260001063346863, val_loss: 1.3487130403518677, val_acc: 0.4108000099658966,  lr: 0.00801
step: 802, train_loss: 1.3253436088562012, acc: 0.42559999227523804, val_loss: 1.3628078699111938, val_acc: 0.40799999237060547,  lr: 0.008020000000000001
step: 803, train_loss: 1.3358458280563354, acc: 0.43380001187324524, val_loss: 1.3573545217514038, val_acc: 0.41200000047683716,  lr: 0.00803
step: 804, train_loss: 1.339158058166504, acc: 0.42399999499320984, val_loss: 1.3655956983566284, val_acc: 0.4174000024795532,  lr: 0.00804
step: 805, train_loss: 1.3805932998657227, acc: 0.39959999918937683, val_loss: 1.354265570640564, val_acc: 0.4131999909877777,  lr: 0.00805
step: 806, train_loss: 1.329721212387085, acc: 0.4374000132083893, val_loss: 1.3536266088485718, val_acc: 0.41359999775886536,  lr: 0.008060000000000001
step: 807, train_loss: 1.3617568016052246, acc: 0.40299999713897705, val_loss: 1.3541194200515747, val_acc: 0.4147999882698059,  lr: 0.00807
step: 808, train_loss: 1.3438243865966797, acc: 0.41780000925064087, val_loss: 1.3506474494934082, val_acc: 0.40939998626708984,  lr: 0.00808
step: 809, train_loss: 1.3564561605453491, acc: 0.40299999713897705, val_loss: 1.3479392528533936, val_acc: 0.4203999936580658,  lr: 0.00809
step: 810, train_loss: 1.3557401895523071, acc: 0.4018000066280365, val_loss: 1.3454405069351196, val_acc: 0.4198000133037567,  lr: 0.008100000000000001
step: 811, train_loss: 1.3270155191421509, acc: 0.4334000051021576, val_loss: 1.3421863317489624, val_acc: 0.4196000099182129,  lr: 0.008110000000000001
step: 812, train_loss: 1.3537477254867554, acc: 0.4090000092983246, val_loss: 1.344222903251648, val_acc: 0.41519999504089355,  lr: 0.00812
step: 813, train_loss: 1.33933687210083, acc: 0.41839998960494995, val_loss: 1.3457375764846802, val_acc: 0.42260000109672546,  lr: 0.00813
step: 814, train_loss: 1.3346425294876099, acc: 0.4246000051498413, val_loss: 1.3379395008087158, val_acc: 0.4214000105857849,  lr: 0.00814
step: 815, train_loss: 1.3245744705200195, acc: 0.430400013923645, val_loss: 1.3418434858322144, val_acc: 0.4169999957084656,  lr: 0.00815
step: 816, train_loss: 1.336440086364746, acc: 0.4230000078678131, val_loss: 1.347592830657959, val_acc: 0.4185999929904938,  lr: 0.008159999999999999
step: 817, train_loss: 1.3356523513793945, acc: 0.4284000098705292, val_loss: 1.3432538509368896, val_acc: 0.4138000011444092,  lr: 0.00817
step: 818, train_loss: 1.3531475067138672, acc: 0.41100001335144043, val_loss: 1.3436393737792969, val_acc: 0.41600000858306885,  lr: 0.00818
step: 819, train_loss: 1.3327096700668335, acc: 0.43560001254081726, val_loss: 1.3426361083984375, val_acc: 0.4124000072479248,  lr: 0.00819
step: 820, train_loss: 1.3224742412567139, acc: 0.4300000071525574, val_loss: 1.352621078491211, val_acc: 0.4171999990940094,  lr: 0.008199999999999999
step: 821, train_loss: 1.346208930015564, acc: 0.40939998626708984, val_loss: 1.3477504253387451, val_acc: 0.41499999165534973,  lr: 0.00821
step: 822, train_loss: 1.324540376663208, acc: 0.4196000099182129, val_loss: 1.3508681058883667, val_acc: 0.41339999437332153,  lr: 0.00822
step: 823, train_loss: 1.3474607467651367, acc: 0.4163999855518341, val_loss: 1.3634757995605469, val_acc: 0.4106000065803528,  lr: 0.00823
step: 824, train_loss: 1.3588389158248901, acc: 0.4131999909877777, val_loss: 1.3526716232299805, val_acc: 0.4138000011444092,  lr: 0.008239999999999999
step: 825, train_loss: 1.3264482021331787, acc: 0.42080000042915344, val_loss: 1.356549859046936, val_acc: 0.40939998626708984,  lr: 0.00825
step: 826, train_loss: 1.3218886852264404, acc: 0.4259999990463257, val_loss: 1.3503198623657227, val_acc: 0.41040000319480896,  lr: 0.00826
step: 827, train_loss: 1.3135337829589844, acc: 0.4296000003814697, val_loss: 1.3598936796188354, val_acc: 0.40880000591278076,  lr: 0.00827
step: 828, train_loss: 1.3707581758499146, acc: 0.4004000127315521, val_loss: 1.3533145189285278, val_acc: 0.41100001335144043,  lr: 0.00828
step: 829, train_loss: 1.3493691682815552, acc: 0.41600000858306885, val_loss: 1.3574851751327515, val_acc: 0.40720000863075256,  lr: 0.00829
step: 830, train_loss: 1.3521395921707153, acc: 0.41040000319480896, val_loss: 1.3494991064071655, val_acc: 0.4129999876022339,  lr: 0.0083
step: 831, train_loss: 1.3552420139312744, acc: 0.4041999876499176, val_loss: 1.3572033643722534, val_acc: 0.4129999876022339,  lr: 0.00831
step: 832, train_loss: 1.3265740871429443, acc: 0.42640000581741333, val_loss: 1.362410306930542, val_acc: 0.4083999991416931,  lr: 0.00832
step: 833, train_loss: 1.3350461721420288, acc: 0.426800012588501, val_loss: 1.3506475687026978, val_acc: 0.4097999930381775,  lr: 0.00833
step: 834, train_loss: 1.3209662437438965, acc: 0.4275999963283539, val_loss: 1.3631370067596436, val_acc: 0.4083999991416931,  lr: 0.00834
step: 835, train_loss: 1.329332709312439, acc: 0.42320001125335693, val_loss: 1.3557639122009277, val_acc: 0.4156000018119812,  lr: 0.00835
step: 836, train_loss: 1.328277587890625, acc: 0.4307999908924103, val_loss: 1.3545119762420654, val_acc: 0.41819998621940613,  lr: 0.00836
step: 837, train_loss: 1.3491716384887695, acc: 0.41179999709129333, val_loss: 1.3628029823303223, val_acc: 0.4113999903202057,  lr: 0.00837
step: 838, train_loss: 1.3174927234649658, acc: 0.43560001254081726, val_loss: 1.3660788536071777, val_acc: 0.4097999930381775,  lr: 0.00838
step: 839, train_loss: 1.321951150894165, acc: 0.43160000443458557, val_loss: 1.3582632541656494, val_acc: 0.41359999775886536,  lr: 0.00839
step: 840, train_loss: 1.3333479166030884, acc: 0.4269999861717224, val_loss: 1.3536481857299805, val_acc: 0.4115999937057495,  lr: 0.0084
step: 841, train_loss: 1.3136664628982544, acc: 0.4307999908924103, val_loss: 1.3583377599716187, val_acc: 0.41499999165534973,  lr: 0.008409999999999999
step: 842, train_loss: 1.3607335090637207, acc: 0.41659998893737793, val_loss: 1.3571743965148926, val_acc: 0.4097999930381775,  lr: 0.00842
step: 843, train_loss: 1.3624995946884155, acc: 0.41339999437332153, val_loss: 1.3476203680038452, val_acc: 0.41659998893737793,  lr: 0.00843
step: 844, train_loss: 1.3396269083023071, acc: 0.4185999929904938, val_loss: 1.3462928533554077, val_acc: 0.4180000126361847,  lr: 0.00844
step: 845, train_loss: 1.3576889038085938, acc: 0.41119998693466187, val_loss: 1.3409174680709839, val_acc: 0.4228000044822693,  lr: 0.00845
step: 846, train_loss: 1.3265799283981323, acc: 0.4259999990463257, val_loss: 1.3364555835723877, val_acc: 0.42080000042915344,  lr: 0.00846
step: 847, train_loss: 1.3739233016967773, acc: 0.3968000113964081, val_loss: 1.3377068042755127, val_acc: 0.4228000044822693,  lr: 0.00847
step: 848, train_loss: 1.3112527132034302, acc: 0.4413999915122986, val_loss: 1.336835503578186, val_acc: 0.41620001196861267,  lr: 0.00848
step: 849, train_loss: 1.3504945039749146, acc: 0.4047999978065491, val_loss: 1.33577561378479, val_acc: 0.41760000586509705,  lr: 0.00849
step: 850, train_loss: 1.324560523033142, acc: 0.4253999888896942, val_loss: 1.338366985321045, val_acc: 0.4212000072002411,  lr: 0.0085
step: 851, train_loss: 1.342559814453125, acc: 0.4129999876022339, val_loss: 1.3405267000198364, val_acc: 0.4169999957084656,  lr: 0.00851
step: 852, train_loss: 1.328243374824524, acc: 0.4189999997615814, val_loss: 1.3371978998184204, val_acc: 0.41659998893737793,  lr: 0.00852
step: 853, train_loss: 1.3176621198654175, acc: 0.42640000581741333, val_loss: 1.3422396183013916, val_acc: 0.41519999504089355,  lr: 0.00853
step: 854, train_loss: 1.3387410640716553, acc: 0.41999998688697815, val_loss: 1.3455150127410889, val_acc: 0.4221999943256378,  lr: 0.00854
step: 855, train_loss: 1.3217933177947998, acc: 0.426800012588501, val_loss: 1.3396861553192139, val_acc: 0.4174000024795532,  lr: 0.00855
step: 856, train_loss: 1.3532463312149048, acc: 0.42100000381469727, val_loss: 1.339080810546875, val_acc: 0.4235999882221222,  lr: 0.00856
step: 857, train_loss: 1.3207794427871704, acc: 0.4359999895095825, val_loss: 1.3516780138015747, val_acc: 0.414000004529953,  lr: 0.00857
step: 858, train_loss: 1.353359341621399, acc: 0.4090000092983246, val_loss: 1.339996099472046, val_acc: 0.41920000314712524,  lr: 0.00858
step: 859, train_loss: 1.3424893617630005, acc: 0.41659998893737793, val_loss: 1.342926025390625, val_acc: 0.4169999957084656,  lr: 0.00859
step: 860, train_loss: 1.3551827669143677, acc: 0.41519999504089355, val_loss: 1.3471741676330566, val_acc: 0.4138000011444092,  lr: 0.0086
step: 861, train_loss: 1.32205069065094, acc: 0.4318000078201294, val_loss: 1.3441280126571655, val_acc: 0.4142000079154968,  lr: 0.00861
step: 862, train_loss: 1.3167937994003296, acc: 0.4307999908924103, val_loss: 1.3399715423583984, val_acc: 0.41679999232292175,  lr: 0.008620000000000001
step: 863, train_loss: 1.3446036577224731, acc: 0.4230000078678131, val_loss: 1.3378690481185913, val_acc: 0.41659998893737793,  lr: 0.00863
step: 864, train_loss: 1.3430370092391968, acc: 0.41780000925064087, val_loss: 1.3355307579040527, val_acc: 0.4221999943256378,  lr: 0.00864
step: 865, train_loss: 1.3256709575653076, acc: 0.41600000858306885, val_loss: 1.3392401933670044, val_acc: 0.4131999909877777,  lr: 0.00865
step: 866, train_loss: 1.3261617422103882, acc: 0.426800012588501, val_loss: 1.340019941329956, val_acc: 0.41760000586509705,  lr: 0.00866
step: 867, train_loss: 1.315657615661621, acc: 0.43320000171661377, val_loss: 1.3453497886657715, val_acc: 0.414000004529953,  lr: 0.00867
step: 868, train_loss: 1.3234636783599854, acc: 0.43320000171661377, val_loss: 1.3486921787261963, val_acc: 0.40860000252723694,  lr: 0.00868
step: 869, train_loss: 1.304205060005188, acc: 0.4323999881744385, val_loss: 1.3446258306503296, val_acc: 0.4090000092983246,  lr: 0.00869
step: 870, train_loss: 1.3291351795196533, acc: 0.430400013923645, val_loss: 1.352822184562683, val_acc: 0.40880000591278076,  lr: 0.0087
step: 871, train_loss: 1.330061674118042, acc: 0.42660000920295715, val_loss: 1.350329041481018, val_acc: 0.4068000018596649,  lr: 0.00871
step: 872, train_loss: 1.3306232690811157, acc: 0.4284000098705292, val_loss: 1.3479470014572144, val_acc: 0.4146000146865845,  lr: 0.00872
step: 873, train_loss: 1.3163470029830933, acc: 0.43380001187324524, val_loss: 1.3525182008743286, val_acc: 0.40779998898506165,  lr: 0.00873
step: 874, train_loss: 1.317411184310913, acc: 0.4323999881744385, val_loss: 1.349961757659912, val_acc: 0.4023999869823456,  lr: 0.00874
step: 875, train_loss: 1.3225833177566528, acc: 0.4311999976634979, val_loss: 1.3507739305496216, val_acc: 0.40380001068115234,  lr: 0.00875
step: 876, train_loss: 1.3184362649917603, acc: 0.42739999294281006, val_loss: 1.3480916023254395, val_acc: 0.40720000863075256,  lr: 0.00876
step: 877, train_loss: 1.2933189868927002, acc: 0.4374000132083893, val_loss: 1.3450000286102295, val_acc: 0.4129999876022339,  lr: 0.00877
step: 878, train_loss: 1.3157930374145508, acc: 0.42399999499320984, val_loss: 1.3454769849777222, val_acc: 0.41359999775886536,  lr: 0.00878
step: 879, train_loss: 1.3662198781967163, acc: 0.4027999937534332, val_loss: 1.3463679552078247, val_acc: 0.4041999876499176,  lr: 0.008790000000000001
step: 880, train_loss: 1.3119513988494873, acc: 0.4392000138759613, val_loss: 1.3378063440322876, val_acc: 0.4153999984264374,  lr: 0.0088
step: 881, train_loss: 1.3238457441329956, acc: 0.43779999017715454, val_loss: 1.343347191810608, val_acc: 0.4156000018119812,  lr: 0.00881
step: 882, train_loss: 1.3576045036315918, acc: 0.40720000863075256, val_loss: 1.3391854763031006, val_acc: 0.42500001192092896,  lr: 0.00882
step: 883, train_loss: 1.3576651811599731, acc: 0.41339999437332153, val_loss: 1.3293242454528809, val_acc: 0.42320001125335693,  lr: 0.008830000000000001
step: 884, train_loss: 1.3054020404815674, acc: 0.43479999899864197, val_loss: 1.326642632484436, val_acc: 0.42239999771118164,  lr: 0.00884
step: 885, train_loss: 1.3127195835113525, acc: 0.43560001254081726, val_loss: 1.331285834312439, val_acc: 0.42399999499320984,  lr: 0.00885
step: 886, train_loss: 1.3486323356628418, acc: 0.40400001406669617, val_loss: 1.3275978565216064, val_acc: 0.4291999936103821,  lr: 0.00886
step: 887, train_loss: 1.3184396028518677, acc: 0.4368000030517578, val_loss: 1.3263152837753296, val_acc: 0.43059998750686646,  lr: 0.008870000000000001
step: 888, train_loss: 1.3370667695999146, acc: 0.4259999990463257, val_loss: 1.3235950469970703, val_acc: 0.42480000853538513,  lr: 0.00888
step: 889, train_loss: 1.3319354057312012, acc: 0.4180000126361847, val_loss: 1.3237980604171753, val_acc: 0.42419999837875366,  lr: 0.00889
step: 890, train_loss: 1.3236092329025269, acc: 0.4302000105381012, val_loss: 1.3264062404632568, val_acc: 0.4302000105381012,  lr: 0.0089
step: 891, train_loss: 1.3180111646652222, acc: 0.43299999833106995, val_loss: 1.3213796615600586, val_acc: 0.43059998750686646,  lr: 0.00891
step: 892, train_loss: 1.329703688621521, acc: 0.4244000017642975, val_loss: 1.3235220909118652, val_acc: 0.42260000109672546,  lr: 0.00892
step: 893, train_loss: 1.302746057510376, acc: 0.43779999017715454, val_loss: 1.3216652870178223, val_acc: 0.430400013923645,  lr: 0.00893
step: 894, train_loss: 1.3290956020355225, acc: 0.426800012588501, val_loss: 1.324829339981079, val_acc: 0.4284000098705292,  lr: 0.00894
step: 895, train_loss: 1.3145045042037964, acc: 0.4325999915599823, val_loss: 1.3301869630813599, val_acc: 0.42660000920295715,  lr: 0.00895
step: 896, train_loss: 1.3028606176376343, acc: 0.44179999828338623, val_loss: 1.3314697742462158, val_acc: 0.4275999963283539,  lr: 0.008960000000000001
step: 897, train_loss: 1.3246256113052368, acc: 0.4307999908924103, val_loss: 1.3344703912734985, val_acc: 0.41760000586509705,  lr: 0.00897
step: 898, train_loss: 1.3311538696289062, acc: 0.4291999936103821, val_loss: 1.3304187059402466, val_acc: 0.4198000133037567,  lr: 0.00898
step: 899, train_loss: 1.3820738792419434, acc: 0.40139999985694885, val_loss: 1.338762640953064, val_acc: 0.4124000072479248,  lr: 0.00899
step: 900, train_loss: 1.3428024053573608, acc: 0.4174000024795532, val_loss: 1.334220290184021, val_acc: 0.42640000581741333,  lr: 0.009000000000000001
step: 901, train_loss: 1.3219090700149536, acc: 0.43160000443458557, val_loss: 1.3373922109603882, val_acc: 0.4251999855041504,  lr: 0.00901
step: 902, train_loss: 1.33040189743042, acc: 0.42899999022483826, val_loss: 1.3323472738265991, val_acc: 0.42500001192092896,  lr: 0.00902
step: 903, train_loss: 1.367655873298645, acc: 0.40639999508857727, val_loss: 1.3341397047042847, val_acc: 0.4251999855041504,  lr: 0.00903
step: 904, train_loss: 1.3229029178619385, acc: 0.43140000104904175, val_loss: 1.3240668773651123, val_acc: 0.4196000099182129,  lr: 0.009040000000000001
step: 905, train_loss: 1.3179775476455688, acc: 0.4275999963283539, val_loss: 1.3501472473144531, val_acc: 0.4185999929904938,  lr: 0.00905
step: 906, train_loss: 1.351438045501709, acc: 0.4108000099658966, val_loss: 1.3602306842803955, val_acc: 0.4169999957084656,  lr: 0.00906
step: 907, train_loss: 1.3501596450805664, acc: 0.4251999855041504, val_loss: 1.3792988061904907, val_acc: 0.4036000072956085,  lr: 0.00907
step: 908, train_loss: 1.3734850883483887, acc: 0.4074000120162964, val_loss: 1.364323616027832, val_acc: 0.40639999508857727,  lr: 0.009080000000000001
step: 909, train_loss: 1.3787847757339478, acc: 0.4059999883174896, val_loss: 1.382789969444275, val_acc: 0.39899998903274536,  lr: 0.00909
step: 910, train_loss: 1.361041784286499, acc: 0.41359999775886536, val_loss: 1.3561363220214844, val_acc: 0.41040000319480896,  lr: 0.0091
step: 911, train_loss: 1.3516730070114136, acc: 0.4259999990463257, val_loss: 1.3553972244262695, val_acc: 0.41760000586509705,  lr: 0.00911
step: 912, train_loss: 1.3602697849273682, acc: 0.40799999237060547, val_loss: 1.366385579109192, val_acc: 0.4099999964237213,  lr: 0.009120000000000001
step: 913, train_loss: 1.3857598304748535, acc: 0.4041999876499176, val_loss: 1.346034288406372, val_acc: 0.4180000126361847,  lr: 0.009130000000000001
step: 914, train_loss: 1.3561179637908936, acc: 0.41760000586509705, val_loss: 1.348603367805481, val_acc: 0.41440001130104065,  lr: 0.00914
step: 915, train_loss: 1.3353641033172607, acc: 0.43140000104904175, val_loss: 1.339159369468689, val_acc: 0.41359999775886536,  lr: 0.00915
step: 916, train_loss: 1.3391331434249878, acc: 0.428600013256073, val_loss: 1.3413712978363037, val_acc: 0.41679999232292175,  lr: 0.00916
step: 917, train_loss: 1.3266228437423706, acc: 0.4235999882221222, val_loss: 1.33609139919281, val_acc: 0.42160001397132874,  lr: 0.009170000000000001
step: 918, train_loss: 1.3568730354309082, acc: 0.4131999909877777, val_loss: 1.3373494148254395, val_acc: 0.42179998755455017,  lr: 0.00918
step: 919, train_loss: 1.3436006307601929, acc: 0.41260001063346863, val_loss: 1.3291953802108765, val_acc: 0.4320000112056732,  lr: 0.00919
step: 920, train_loss: 1.3277153968811035, acc: 0.43140000104904175, val_loss: 1.3298534154891968, val_acc: 0.4251999855041504,  lr: 0.0092
step: 921, train_loss: 1.3316141366958618, acc: 0.4302000105381012, val_loss: 1.3326162099838257, val_acc: 0.4212000072002411,  lr: 0.009210000000000001
step: 922, train_loss: 1.349669098854065, acc: 0.42100000381469727, val_loss: 1.322624683380127, val_acc: 0.4284000098705292,  lr: 0.00922
step: 923, train_loss: 1.340989112854004, acc: 0.4050000011920929, val_loss: 1.3281053304672241, val_acc: 0.4269999861717224,  lr: 0.00923
step: 924, train_loss: 1.3375102281570435, acc: 0.42660000920295715, val_loss: 1.326559066772461, val_acc: 0.42419999837875366,  lr: 0.00924
step: 925, train_loss: 1.340716004371643, acc: 0.4174000024795532, val_loss: 1.3360116481781006, val_acc: 0.4203999936580658,  lr: 0.009250000000000001
step: 926, train_loss: 1.3459161520004272, acc: 0.41600000858306885, val_loss: 1.3274056911468506, val_acc: 0.4259999990463257,  lr: 0.009260000000000001
step: 927, train_loss: 1.330862045288086, acc: 0.43299999833106995, val_loss: 1.337972640991211, val_acc: 0.42579999566078186,  lr: 0.00927
step: 928, train_loss: 1.3618757724761963, acc: 0.4097999930381775, val_loss: 1.3287092447280884, val_acc: 0.4334000051021576,  lr: 0.00928
step: 929, train_loss: 1.3546017408370972, acc: 0.41819998621940613, val_loss: 1.3360836505889893, val_acc: 0.4228000044822693,  lr: 0.009290000000000001
step: 930, train_loss: 1.381057858467102, acc: 0.397599995136261, val_loss: 1.3300926685333252, val_acc: 0.42340001463890076,  lr: 0.009300000000000001
step: 931, train_loss: 1.339881420135498, acc: 0.4153999984264374, val_loss: 1.33265221118927, val_acc: 0.42719998955726624,  lr: 0.00931
step: 932, train_loss: 1.3231357336044312, acc: 0.42879998683929443, val_loss: 1.3298234939575195, val_acc: 0.423799991607666,  lr: 0.00932
step: 933, train_loss: 1.3108466863632202, acc: 0.44040000438690186, val_loss: 1.335557460784912, val_acc: 0.423799991607666,  lr: 0.009330000000000001
step: 934, train_loss: 1.3283748626708984, acc: 0.426800012588501, val_loss: 1.3282376527786255, val_acc: 0.42719998955726624,  lr: 0.009340000000000001
step: 935, train_loss: 1.3403700590133667, acc: 0.42160001397132874, val_loss: 1.335523009300232, val_acc: 0.42660000920295715,  lr: 0.00935
step: 936, train_loss: 1.3202646970748901, acc: 0.42719998955726624, val_loss: 1.331325888633728, val_acc: 0.42419999837875366,  lr: 0.00936
step: 937, train_loss: 1.326046109199524, acc: 0.43140000104904175, val_loss: 1.325366735458374, val_acc: 0.41920000314712524,  lr: 0.009370000000000002
step: 938, train_loss: 1.3215172290802002, acc: 0.4336000084877014, val_loss: 1.3238563537597656, val_acc: 0.4327999949455261,  lr: 0.00938
step: 939, train_loss: 1.3231273889541626, acc: 0.43220001459121704, val_loss: 1.3222696781158447, val_acc: 0.42980000376701355,  lr: 0.009389999999999999
step: 940, train_loss: 1.3113784790039062, acc: 0.4323999881744385, val_loss: 1.3206123113632202, val_acc: 0.4293999969959259,  lr: 0.0094
step: 941, train_loss: 1.3214871883392334, acc: 0.42559999227523804, val_loss: 1.3213751316070557, val_acc: 0.4352000057697296,  lr: 0.00941
step: 942, train_loss: 1.324518084526062, acc: 0.4300000071525574, val_loss: 1.319923758506775, val_acc: 0.43459999561309814,  lr: 0.00942
step: 943, train_loss: 1.3104774951934814, acc: 0.4357999861240387, val_loss: 1.3286627531051636, val_acc: 0.4300000071525574,  lr: 0.00943
step: 944, train_loss: 1.3242796659469604, acc: 0.42739999294281006, val_loss: 1.317345142364502, val_acc: 0.4327999949455261,  lr: 0.00944
step: 945, train_loss: 1.2988380193710327, acc: 0.4375999867916107, val_loss: 1.3239201307296753, val_acc: 0.4341999888420105,  lr: 0.00945
step: 946, train_loss: 1.3094066381454468, acc: 0.43639999628067017, val_loss: 1.3149665594100952, val_acc: 0.4339999854564667,  lr: 0.00946
step: 947, train_loss: 1.361228585243225, acc: 0.4169999957084656, val_loss: 1.3196722269058228, val_acc: 0.4296000003814697,  lr: 0.00947
step: 948, train_loss: 1.3003565073013306, acc: 0.43619999289512634, val_loss: 1.3271483182907104, val_acc: 0.42500001192092896,  lr: 0.00948
step: 949, train_loss: 1.3277019262313843, acc: 0.42340001463890076, val_loss: 1.31832754611969, val_acc: 0.4339999854564667,  lr: 0.00949
step: 950, train_loss: 1.3105241060256958, acc: 0.4447999894618988, val_loss: 1.314740777015686, val_acc: 0.43619999289512634,  lr: 0.0095
step: 951, train_loss: 1.3397412300109863, acc: 0.4174000024795532, val_loss: 1.3149769306182861, val_acc: 0.428600013256073,  lr: 0.00951
step: 952, train_loss: 1.3044679164886475, acc: 0.43720000982284546, val_loss: 1.3120582103729248, val_acc: 0.421999990940094,  lr: 0.009519999999999999
step: 953, train_loss: 1.3484587669372559, acc: 0.4146000146865845, val_loss: 1.318341851234436, val_acc: 0.42239999771118164,  lr: 0.00953
step: 954, train_loss: 1.309805154800415, acc: 0.4381999969482422, val_loss: 1.3160524368286133, val_acc: 0.42719998955726624,  lr: 0.00954
step: 955, train_loss: 1.2987322807312012, acc: 0.4422000050544739, val_loss: 1.3184500932693481, val_acc: 0.42719998955726624,  lr: 0.00955
step: 956, train_loss: 1.3365225791931152, acc: 0.4275999963283539, val_loss: 1.3227131366729736, val_acc: 0.4262000024318695,  lr: 0.009559999999999999
step: 957, train_loss: 1.3252865076065063, acc: 0.43619999289512634, val_loss: 1.3237799406051636, val_acc: 0.42579999566078186,  lr: 0.00957
step: 958, train_loss: 1.299843430519104, acc: 0.4458000063896179, val_loss: 1.3197022676467896, val_acc: 0.43059998750686646,  lr: 0.00958
step: 959, train_loss: 1.3118466138839722, acc: 0.44359999895095825, val_loss: 1.3221670389175415, val_acc: 0.4277999997138977,  lr: 0.00959
step: 960, train_loss: 1.3217941522598267, acc: 0.42800000309944153, val_loss: 1.3162645101547241, val_acc: 0.4253999888896942,  lr: 0.0096
step: 961, train_loss: 1.3175297975540161, acc: 0.43959999084472656, val_loss: 1.322173833847046, val_acc: 0.4235999882221222,  lr: 0.00961
step: 962, train_loss: 1.3491253852844238, acc: 0.4108000099658966, val_loss: 1.3294328451156616, val_acc: 0.42100000381469727,  lr: 0.00962
step: 963, train_loss: 1.311012864112854, acc: 0.4381999969482422, val_loss: 1.3401625156402588, val_acc: 0.4198000133037567,  lr: 0.00963
step: 964, train_loss: 1.3406656980514526, acc: 0.4205999970436096, val_loss: 1.3333642482757568, val_acc: 0.4212000072002411,  lr: 0.00964
step: 965, train_loss: 1.3128589391708374, acc: 0.4318000078201294, val_loss: 1.3353166580200195, val_acc: 0.4203999936580658,  lr: 0.00965
step: 966, train_loss: 1.3474522829055786, acc: 0.41440001130104065, val_loss: 1.326676607131958, val_acc: 0.42160001397132874,  lr: 0.00966
step: 967, train_loss: 1.3031996488571167, acc: 0.4397999942302704, val_loss: 1.3234530687332153, val_acc: 0.4244000017642975,  lr: 0.00967
step: 968, train_loss: 1.3160369396209717, acc: 0.4339999854564667, val_loss: 1.317196249961853, val_acc: 0.4253999888896942,  lr: 0.00968
step: 969, train_loss: 1.3047521114349365, acc: 0.4350000023841858, val_loss: 1.3230388164520264, val_acc: 0.4251999855041504,  lr: 0.00969
step: 970, train_loss: 1.2923927307128906, acc: 0.44859999418258667, val_loss: 1.3241543769836426, val_acc: 0.4244000017642975,  lr: 0.0097
step: 971, train_loss: 1.3156901597976685, acc: 0.4275999963283539, val_loss: 1.3187568187713623, val_acc: 0.421999990940094,  lr: 0.00971
step: 972, train_loss: 1.2978750467300415, acc: 0.4480000138282776, val_loss: 1.3232365846633911, val_acc: 0.4262000024318695,  lr: 0.00972
step: 973, train_loss: 1.3014609813690186, acc: 0.4429999887943268, val_loss: 1.3226712942123413, val_acc: 0.42500001192092896,  lr: 0.00973
step: 974, train_loss: 1.3230249881744385, acc: 0.43220001459121704, val_loss: 1.320772409439087, val_acc: 0.42899999022483826,  lr: 0.00974
step: 975, train_loss: 1.339505672454834, acc: 0.42160001397132874, val_loss: 1.34493088722229, val_acc: 0.41440001130104065,  lr: 0.00975
step: 976, train_loss: 1.3573591709136963, acc: 0.40939998626708984, val_loss: 1.3509572744369507, val_acc: 0.412200003862381,  lr: 0.00976
step: 977, train_loss: 1.3542708158493042, acc: 0.4115999937057495, val_loss: 1.3489246368408203, val_acc: 0.4163999855518341,  lr: 0.00977
step: 978, train_loss: 1.3347654342651367, acc: 0.4262000024318695, val_loss: 1.3482868671417236, val_acc: 0.41819998621940613,  lr: 0.00978
step: 979, train_loss: 1.3193289041519165, acc: 0.4375999867916107, val_loss: 1.3326634168624878, val_acc: 0.42320001125335693,  lr: 0.00979
step: 980, train_loss: 1.313829779624939, acc: 0.4422000050544739, val_loss: 1.333338737487793, val_acc: 0.4221999943256378,  lr: 0.0098
step: 981, train_loss: 1.325134515762329, acc: 0.44119998812675476, val_loss: 1.3254913091659546, val_acc: 0.4246000051498413,  lr: 0.00981
step: 982, train_loss: 1.3195879459381104, acc: 0.43160000443458557, val_loss: 1.3310126066207886, val_acc: 0.42500001192092896,  lr: 0.00982
step: 983, train_loss: 1.332429051399231, acc: 0.4284000098705292, val_loss: 1.3261613845825195, val_acc: 0.42820000648498535,  lr: 0.00983
step: 984, train_loss: 1.2813847064971924, acc: 0.4474000036716461, val_loss: 1.3302701711654663, val_acc: 0.4251999855041504,  lr: 0.00984
step: 985, train_loss: 1.31775963306427, acc: 0.43160000443458557, val_loss: 1.32891047000885, val_acc: 0.42739999294281006,  lr: 0.00985
step: 986, train_loss: 1.3145869970321655, acc: 0.4323999881744385, val_loss: 1.339216947555542, val_acc: 0.4198000133037567,  lr: 0.00986
step: 987, train_loss: 1.3656861782073975, acc: 0.41019999980926514, val_loss: 1.3267425298690796, val_acc: 0.4251999855041504,  lr: 0.00987
step: 988, train_loss: 1.3284951448440552, acc: 0.4244000017642975, val_loss: 1.3196468353271484, val_acc: 0.4284000098705292,  lr: 0.00988
step: 989, train_loss: 1.3087968826293945, acc: 0.43560001254081726, val_loss: 1.32091224193573, val_acc: 0.4318000078201294,  lr: 0.00989
step: 990, train_loss: 1.2971906661987305, acc: 0.44040000438690186, val_loss: 1.3347991704940796, val_acc: 0.4221999943256378,  lr: 0.0099
step: 991, train_loss: 1.355672836303711, acc: 0.41519999504089355, val_loss: 1.3315881490707397, val_acc: 0.4277999997138977,  lr: 0.00991
step: 992, train_loss: 1.3417880535125732, acc: 0.42739999294281006, val_loss: 1.335937261581421, val_acc: 0.42160001397132874,  lr: 0.00992
step: 993, train_loss: 1.3315587043762207, acc: 0.42260000109672546, val_loss: 1.3347110748291016, val_acc: 0.4189999997615814,  lr: 0.00993
step: 994, train_loss: 1.3118650913238525, acc: 0.44119998812675476, val_loss: 1.3275066614151, val_acc: 0.4228000044822693,  lr: 0.009940000000000001
step: 995, train_loss: 1.2967110872268677, acc: 0.44279998540878296, val_loss: 1.3184136152267456, val_acc: 0.4246000051498413,  lr: 0.00995
step: 996, train_loss: 1.3219977617263794, acc: 0.4284000098705292, val_loss: 1.3283075094223022, val_acc: 0.4212000072002411,  lr: 0.00996
step: 997, train_loss: 1.2955245971679688, acc: 0.4453999996185303, val_loss: 1.3167579174041748, val_acc: 0.4244000017642975,  lr: 0.00997
step: 998, train_loss: 1.3272136449813843, acc: 0.42100000381469727, val_loss: 1.323157548904419, val_acc: 0.42800000309944153,  lr: 0.009980000000000001
step: 999, train_loss: 1.2908765077590942, acc: 0.4447999894618988, val_loss: 1.3235853910446167, val_acc: 0.42660000920295715,  lr: 0.00999
step: 1000, train_loss: 1.311631441116333, acc: 0.4350000023841858, val_loss: 1.3249974250793457, val_acc: 0.42260000109672546,  lr: 0.01
step: 1001, train_loss: 1.2783567905426025, acc: 0.4465999901294708, val_loss: 1.3239606618881226, val_acc: 0.4269999861717224,  lr: 0.00999965551724138
step: 1002, train_loss: 1.2691256999969482, acc: 0.45260000228881836, val_loss: 1.3206737041473389, val_acc: 0.4309999942779541,  lr: 0.009999311034482759
step: 1003, train_loss: 1.3156076669692993, acc: 0.4343999922275543, val_loss: 1.3170552253723145, val_acc: 0.4284000098705292,  lr: 0.009998966551724138
step: 1004, train_loss: 1.2835198640823364, acc: 0.45179998874664307, val_loss: 1.3228418827056885, val_acc: 0.42980000376701355,  lr: 0.009998622068965518
step: 1005, train_loss: 1.3320966958999634, acc: 0.42579999566078186, val_loss: 1.318339467048645, val_acc: 0.4334000051021576,  lr: 0.009998277586206897
step: 1006, train_loss: 1.2944570779800415, acc: 0.44600000977516174, val_loss: 1.3165674209594727, val_acc: 0.4296000003814697,  lr: 0.009997933103448277
step: 1007, train_loss: 1.2777436971664429, acc: 0.45680001378059387, val_loss: 1.3154869079589844, val_acc: 0.421999990940094,  lr: 0.009997588620689656
step: 1008, train_loss: 1.3269586563110352, acc: 0.4198000133037567, val_loss: 1.3207573890686035, val_acc: 0.4189999997615814,  lr: 0.009997244137931035
step: 1009, train_loss: 1.292871117591858, acc: 0.44440001249313354, val_loss: 1.316567301750183, val_acc: 0.4205999970436096,  lr: 0.009996899655172415
step: 1010, train_loss: 1.3197871446609497, acc: 0.4293999969959259, val_loss: 1.3175547122955322, val_acc: 0.42719998955726624,  lr: 0.009996555172413794
step: 1011, train_loss: 1.3320579528808594, acc: 0.42080000042915344, val_loss: 1.3150081634521484, val_acc: 0.4275999963283539,  lr: 0.009996210689655173
step: 1012, train_loss: 1.329485535621643, acc: 0.42399999499320984, val_loss: 1.3073302507400513, val_acc: 0.4309999942779541,  lr: 0.009995866206896553
step: 1013, train_loss: 1.3206111192703247, acc: 0.42579999566078186, val_loss: 1.3003289699554443, val_acc: 0.43880000710487366,  lr: 0.00999552172413793
step: 1014, train_loss: 1.3090919256210327, acc: 0.4291999936103821, val_loss: 1.2981516122817993, val_acc: 0.4431999921798706,  lr: 0.00999517724137931
step: 1015, train_loss: 1.3496694564819336, acc: 0.40799999237060547, val_loss: 1.3069669008255005, val_acc: 0.43700000643730164,  lr: 0.00999483275862069
step: 1016, train_loss: 1.3202019929885864, acc: 0.4334000051021576, val_loss: 1.302909016609192, val_acc: 0.4318000078201294,  lr: 0.009994488275862069
step: 1017, train_loss: 1.303308129310608, acc: 0.4397999942302704, val_loss: 1.3063727617263794, val_acc: 0.43320000171661377,  lr: 0.009994143793103448
step: 1018, train_loss: 1.3141294717788696, acc: 0.43880000710487366, val_loss: 1.2992335557937622, val_acc: 0.44020000100135803,  lr: 0.009993799310344827
step: 1019, train_loss: 1.2962955236434937, acc: 0.43880000710487366, val_loss: 1.302488088607788, val_acc: 0.44440001249313354,  lr: 0.009993454827586207
step: 1020, train_loss: 1.334797739982605, acc: 0.4235999882221222, val_loss: 1.3082596063613892, val_acc: 0.4392000138759613,  lr: 0.009993110344827586
step: 1021, train_loss: 1.3082935810089111, acc: 0.43479999899864197, val_loss: 1.309476613998413, val_acc: 0.43939998745918274,  lr: 0.009992765862068965
step: 1022, train_loss: 1.3766298294067383, acc: 0.4050000011920929, val_loss: 1.3434500694274902, val_acc: 0.43320000171661377,  lr: 0.009992421379310345
step: 1023, train_loss: 1.3442672491073608, acc: 0.42800000309944153, val_loss: 1.3383899927139282, val_acc: 0.420199990272522,  lr: 0.009992076896551724
step: 1024, train_loss: 1.3471639156341553, acc: 0.41679999232292175, val_loss: 1.3353981971740723, val_acc: 0.421999990940094,  lr: 0.009991732413793104
step: 1025, train_loss: 1.3009124994277954, acc: 0.4424000084400177, val_loss: 1.331252932548523, val_acc: 0.4302000105381012,  lr: 0.009991387931034483
step: 1026, train_loss: 1.3225902318954468, acc: 0.42480000853538513, val_loss: 1.3311395645141602, val_acc: 0.4300000071525574,  lr: 0.009991043448275862
step: 1027, train_loss: 1.3513758182525635, acc: 0.42160001397132874, val_loss: 1.328346848487854, val_acc: 0.4291999936103821,  lr: 0.009990698965517242
step: 1028, train_loss: 1.342635154724121, acc: 0.4131999909877777, val_loss: 1.3319228887557983, val_acc: 0.42480000853538513,  lr: 0.009990354482758621
step: 1029, train_loss: 1.3184428215026855, acc: 0.43140000104904175, val_loss: 1.3293533325195312, val_acc: 0.41839998960494995,  lr: 0.00999001
step: 1030, train_loss: 1.3274469375610352, acc: 0.4352000057697296, val_loss: 1.325994849205017, val_acc: 0.4269999861717224,  lr: 0.00998966551724138
step: 1031, train_loss: 1.3129793405532837, acc: 0.44179999828338623, val_loss: 1.3316447734832764, val_acc: 0.421999990940094,  lr: 0.00998932103448276
step: 1032, train_loss: 1.2948946952819824, acc: 0.45419999957084656, val_loss: 1.322874665260315, val_acc: 0.42399999499320984,  lr: 0.009988976551724139
step: 1033, train_loss: 1.286399245262146, acc: 0.45419999957084656, val_loss: 1.3253885507583618, val_acc: 0.428600013256073,  lr: 0.009988632068965518
step: 1034, train_loss: 1.3320651054382324, acc: 0.4259999990463257, val_loss: 1.3216856718063354, val_acc: 0.426800012588501,  lr: 0.009988287586206897
step: 1035, train_loss: 1.3408969640731812, acc: 0.41260001063346863, val_loss: 1.319993495941162, val_acc: 0.4341999888420105,  lr: 0.009987943103448277
step: 1036, train_loss: 1.2797867059707642, acc: 0.45719999074935913, val_loss: 1.323264718055725, val_acc: 0.42800000309944153,  lr: 0.009987598620689656
step: 1037, train_loss: 1.2913585901260376, acc: 0.4474000036716461, val_loss: 1.322523593902588, val_acc: 0.41940000653266907,  lr: 0.009987254137931034
step: 1038, train_loss: 1.3054405450820923, acc: 0.4410000145435333, val_loss: 1.3138247728347778, val_acc: 0.43720000982284546,  lr: 0.009986909655172413
step: 1039, train_loss: 1.3493021726608276, acc: 0.40700000524520874, val_loss: 1.3210532665252686, val_acc: 0.42500001192092896,  lr: 0.009986565172413793
step: 1040, train_loss: 1.3409647941589355, acc: 0.4228000044822693, val_loss: 1.3181911706924438, val_acc: 0.4230000078678131,  lr: 0.009986220689655172
step: 1041, train_loss: 1.315868616104126, acc: 0.43700000643730164, val_loss: 1.3191158771514893, val_acc: 0.42399999499320984,  lr: 0.009985876206896551
step: 1042, train_loss: 1.3205116987228394, acc: 0.42800000309944153, val_loss: 1.326143741607666, val_acc: 0.42399999499320984,  lr: 0.00998553172413793
step: 1043, train_loss: 1.2823477983474731, acc: 0.45179998874664307, val_loss: 1.323814868927002, val_acc: 0.4284000098705292,  lr: 0.00998518724137931
step: 1044, train_loss: 1.3176342248916626, acc: 0.43479999899864197, val_loss: 1.3279752731323242, val_acc: 0.42160001397132874,  lr: 0.00998484275862069
step: 1045, train_loss: 1.3345476388931274, acc: 0.4251999855041504, val_loss: 1.3326255083084106, val_acc: 0.4235999882221222,  lr: 0.009984498275862069
step: 1046, train_loss: 1.3356034755706787, acc: 0.42100000381469727, val_loss: 1.3290408849716187, val_acc: 0.4262000024318695,  lr: 0.009984153793103448
step: 1047, train_loss: 1.2789435386657715, acc: 0.4474000036716461, val_loss: 1.3320142030715942, val_acc: 0.4262000024318695,  lr: 0.009983809310344828
step: 1048, train_loss: 1.3300153017044067, acc: 0.4212000072002411, val_loss: 1.3341608047485352, val_acc: 0.42500001192092896,  lr: 0.009983464827586207
step: 1049, train_loss: 1.333477258682251, acc: 0.4146000146865845, val_loss: 1.324476718902588, val_acc: 0.423799991607666,  lr: 0.009983120344827586
step: 1050, train_loss: 1.3581393957138062, acc: 0.41100001335144043, val_loss: 1.3301198482513428, val_acc: 0.4269999861717224,  lr: 0.009982775862068966
step: 1051, train_loss: 1.3391324281692505, acc: 0.42320001125335693, val_loss: 1.3336158990859985, val_acc: 0.42320001125335693,  lr: 0.009982431379310345
step: 1052, train_loss: 1.304826021194458, acc: 0.4424000084400177, val_loss: 1.3203699588775635, val_acc: 0.4311999976634979,  lr: 0.009982086896551725
step: 1053, train_loss: 1.3360718488693237, acc: 0.42239999771118164, val_loss: 1.3168871402740479, val_acc: 0.43380001187324524,  lr: 0.009981742413793104
step: 1054, train_loss: 1.319175362586975, acc: 0.42980000376701355, val_loss: 1.3196512460708618, val_acc: 0.43459999561309814,  lr: 0.009981397931034483
step: 1055, train_loss: 1.2841694355010986, acc: 0.4480000138282776, val_loss: 1.3208723068237305, val_acc: 0.4302000105381012,  lr: 0.009981053448275863
step: 1056, train_loss: 1.3261737823486328, acc: 0.43779999017715454, val_loss: 1.3146644830703735, val_acc: 0.43779999017715454,  lr: 0.009980708965517242
step: 1057, train_loss: 1.2807255983352661, acc: 0.4578000009059906, val_loss: 1.3221237659454346, val_acc: 0.4269999861717224,  lr: 0.009980364482758622
step: 1058, train_loss: 1.310407280921936, acc: 0.44339999556541443, val_loss: 1.3160098791122437, val_acc: 0.42739999294281006,  lr: 0.009980020000000001
step: 1059, train_loss: 1.3063205480575562, acc: 0.4341999888420105, val_loss: 1.3181047439575195, val_acc: 0.42500001192092896,  lr: 0.00997967551724138
step: 1060, train_loss: 1.3083537817001343, acc: 0.43059998750686646, val_loss: 1.3173835277557373, val_acc: 0.430400013923645,  lr: 0.00997933103448276
step: 1061, train_loss: 1.2819174528121948, acc: 0.4575999975204468, val_loss: 1.309135913848877, val_acc: 0.43560001254081726,  lr: 0.009978986551724137
step: 1062, train_loss: 1.3161683082580566, acc: 0.4327999949455261, val_loss: 1.3162485361099243, val_acc: 0.4311999976634979,  lr: 0.009978642068965517
step: 1063, train_loss: 1.3115019798278809, acc: 0.4302000105381012, val_loss: 1.3111352920532227, val_acc: 0.43619999289512634,  lr: 0.009978297586206896
step: 1064, train_loss: 1.2976398468017578, acc: 0.44119998812675476, val_loss: 1.3216001987457275, val_acc: 0.4259999990463257,  lr: 0.009977953103448275
step: 1065, train_loss: 1.3037760257720947, acc: 0.4357999861240387, val_loss: 1.3237234354019165, val_acc: 0.430400013923645,  lr: 0.009977608620689655
step: 1066, train_loss: 1.3452050685882568, acc: 0.4230000078678131, val_loss: 1.3127535581588745, val_acc: 0.4424000084400177,  lr: 0.009977264137931034
step: 1067, train_loss: 1.2895598411560059, acc: 0.44279998540878296, val_loss: 1.3149961233139038, val_acc: 0.4343999922275543,  lr: 0.009976919655172414
step: 1068, train_loss: 1.2954093217849731, acc: 0.45080000162124634, val_loss: 1.318752646446228, val_acc: 0.4341999888420105,  lr: 0.009976575172413793
step: 1069, train_loss: 1.2993273735046387, acc: 0.4528000056743622, val_loss: 1.3106508255004883, val_acc: 0.4352000057697296,  lr: 0.009976230689655172
step: 1070, train_loss: 1.3095647096633911, acc: 0.43479999899864197, val_loss: 1.3217267990112305, val_acc: 0.4302000105381012,  lr: 0.009975886206896552
step: 1071, train_loss: 1.301433801651001, acc: 0.4368000030517578, val_loss: 1.311365008354187, val_acc: 0.4413999915122986,  lr: 0.009975541724137931
step: 1072, train_loss: 1.3014771938323975, acc: 0.43560001254081726, val_loss: 1.3128440380096436, val_acc: 0.4368000030517578,  lr: 0.00997519724137931
step: 1073, train_loss: 1.3233450651168823, acc: 0.4327999949455261, val_loss: 1.3106423616409302, val_acc: 0.43479999899864197,  lr: 0.00997485275862069
step: 1074, train_loss: 1.3349698781967163, acc: 0.4205999970436096, val_loss: 1.3276516199111938, val_acc: 0.42579999566078186,  lr: 0.00997450827586207
step: 1075, train_loss: 1.2974528074264526, acc: 0.4413999915122986, val_loss: 1.3130477666854858, val_acc: 0.43160000443458557,  lr: 0.009974163793103449
step: 1076, train_loss: 1.3343346118927002, acc: 0.42160001397132874, val_loss: 1.309326410293579, val_acc: 0.43299999833106995,  lr: 0.009973819310344828
step: 1077, train_loss: 1.2834303379058838, acc: 0.4496000111103058, val_loss: 1.3043838739395142, val_acc: 0.4408000111579895,  lr: 0.009973474827586207
step: 1078, train_loss: 1.3025293350219727, acc: 0.43140000104904175, val_loss: 1.3022031784057617, val_acc: 0.43619999289512634,  lr: 0.009973130344827587
step: 1079, train_loss: 1.290633201599121, acc: 0.4562000036239624, val_loss: 1.2936064004898071, val_acc: 0.446399986743927,  lr: 0.009972785862068966
step: 1080, train_loss: 1.2939790487289429, acc: 0.43639999628067017, val_loss: 1.3019660711288452, val_acc: 0.4410000145435333,  lr: 0.009972441379310346
step: 1081, train_loss: 1.2858268022537231, acc: 0.4343999922275543, val_loss: 1.3058884143829346, val_acc: 0.44179999828338623,  lr: 0.009972096896551725
step: 1082, train_loss: 1.3096028566360474, acc: 0.44359999895095825, val_loss: 1.2990314960479736, val_acc: 0.4438000023365021,  lr: 0.009971752413793104
step: 1083, train_loss: 1.2785128355026245, acc: 0.4472000002861023, val_loss: 1.2953708171844482, val_acc: 0.44279998540878296,  lr: 0.009971407931034484
step: 1084, train_loss: 1.3336772918701172, acc: 0.41760000586509705, val_loss: 1.3077690601348877, val_acc: 0.430400013923645,  lr: 0.009971063448275863
step: 1085, train_loss: 1.2863097190856934, acc: 0.45080000162124634, val_loss: 1.304052472114563, val_acc: 0.43720000982284546,  lr: 0.00997071896551724
step: 1086, train_loss: 1.2612524032592773, acc: 0.45719999074935913, val_loss: 1.3057693243026733, val_acc: 0.42899999022483826,  lr: 0.00997037448275862
step: 1087, train_loss: 1.3598860502243042, acc: 0.41100001335144043, val_loss: 1.3104066848754883, val_acc: 0.43479999899864197,  lr: 0.00997003
step: 1088, train_loss: 1.3747403621673584, acc: 0.4034000039100647, val_loss: 1.3113659620285034, val_acc: 0.4341999888420105,  lr: 0.009969685517241379
step: 1089, train_loss: 1.318654179573059, acc: 0.43540000915527344, val_loss: 1.319682240486145, val_acc: 0.43320000171661377,  lr: 0.009969341034482758
step: 1090, train_loss: 1.303515911102295, acc: 0.44040000438690186, val_loss: 1.3081098794937134, val_acc: 0.4352000057697296,  lr: 0.009968996551724138
step: 1091, train_loss: 1.3125877380371094, acc: 0.4275999963283539, val_loss: 1.3076828718185425, val_acc: 0.4339999854564667,  lr: 0.009968652068965517
step: 1092, train_loss: 1.2790178060531616, acc: 0.4537999927997589, val_loss: 1.3120611906051636, val_acc: 0.4325999915599823,  lr: 0.009968307586206896
step: 1093, train_loss: 1.282034993171692, acc: 0.44620001316070557, val_loss: 1.3170442581176758, val_acc: 0.4284000098705292,  lr: 0.009967963103448276
step: 1094, train_loss: 1.2750420570373535, acc: 0.45419999957084656, val_loss: 1.3241000175476074, val_acc: 0.430400013923645,  lr: 0.009967618620689655
step: 1095, train_loss: 1.3178454637527466, acc: 0.43779999017715454, val_loss: 1.3127822875976562, val_acc: 0.4259999990463257,  lr: 0.009967274137931035
step: 1096, train_loss: 1.2714719772338867, acc: 0.4496000111103058, val_loss: 1.3108737468719482, val_acc: 0.4275999963283539,  lr: 0.009966929655172414
step: 1097, train_loss: 1.2856929302215576, acc: 0.4472000002861023, val_loss: 1.3150482177734375, val_acc: 0.430400013923645,  lr: 0.009966585172413793
step: 1098, train_loss: 1.3055729866027832, acc: 0.4399999976158142, val_loss: 1.3121910095214844, val_acc: 0.4307999908924103,  lr: 0.009966240689655173
step: 1099, train_loss: 1.277051329612732, acc: 0.45399999618530273, val_loss: 1.3098156452178955, val_acc: 0.4339999854564667,  lr: 0.009965896206896552
step: 1100, train_loss: 1.2833691835403442, acc: 0.4487999975681305, val_loss: 1.3169493675231934, val_acc: 0.4352000057697296,  lr: 0.009965551724137931
step: 1101, train_loss: 1.2575702667236328, acc: 0.4634000062942505, val_loss: 1.3133469820022583, val_acc: 0.43320000171661377,  lr: 0.009965207241379311
step: 1102, train_loss: 1.262166142463684, acc: 0.454800009727478, val_loss: 1.3111807107925415, val_acc: 0.42980000376701355,  lr: 0.00996486275862069
step: 1103, train_loss: 1.3041653633117676, acc: 0.4449999928474426, val_loss: 1.3165041208267212, val_acc: 0.43160000443458557,  lr: 0.00996451827586207
step: 1104, train_loss: 1.257637619972229, acc: 0.46619999408721924, val_loss: 1.3157819509506226, val_acc: 0.43459999561309814,  lr: 0.009964173793103449
step: 1105, train_loss: 1.253029704093933, acc: 0.4675999879837036, val_loss: 1.3208099603652954, val_acc: 0.4275999963283539,  lr: 0.009963829310344828
step: 1106, train_loss: 1.2674164772033691, acc: 0.459199994802475, val_loss: 1.3249660730361938, val_acc: 0.4269999861717224,  lr: 0.009963484827586208
step: 1107, train_loss: 1.3308689594268799, acc: 0.42480000853538513, val_loss: 1.330320119857788, val_acc: 0.4291999936103821,  lr: 0.009963140344827587
step: 1108, train_loss: 1.2641865015029907, acc: 0.45419999957084656, val_loss: 1.3275420665740967, val_acc: 0.4235999882221222,  lr: 0.009962795862068967
step: 1109, train_loss: 1.2848472595214844, acc: 0.4472000002861023, val_loss: 1.3278669118881226, val_acc: 0.4253999888896942,  lr: 0.009962451379310344
step: 1110, train_loss: 1.3250352144241333, acc: 0.4296000003814697, val_loss: 1.3309431076049805, val_acc: 0.4244000017642975,  lr: 0.009962106896551724
step: 1111, train_loss: 1.253894567489624, acc: 0.4699999988079071, val_loss: 1.3413500785827637, val_acc: 0.4230000078678131,  lr: 0.009961762413793103
step: 1112, train_loss: 1.2790604829788208, acc: 0.45719999074935913, val_loss: 1.3308348655700684, val_acc: 0.4228000044822693,  lr: 0.009961417931034482
step: 1113, train_loss: 1.3380153179168701, acc: 0.42480000853538513, val_loss: 1.3363230228424072, val_acc: 0.4180000126361847,  lr: 0.009961073448275862
step: 1114, train_loss: 1.2767051458358765, acc: 0.45339998602867126, val_loss: 1.3325194120407104, val_acc: 0.42160001397132874,  lr: 0.009960728965517241
step: 1115, train_loss: 1.257851004600525, acc: 0.4593999981880188, val_loss: 1.337593674659729, val_acc: 0.42340001463890076,  lr: 0.00996038448275862
step: 1116, train_loss: 1.3378055095672607, acc: 0.4196000099182129, val_loss: 1.332288384437561, val_acc: 0.4244000017642975,  lr: 0.00996004
step: 1117, train_loss: 1.2769700288772583, acc: 0.4465999901294708, val_loss: 1.3436132669448853, val_acc: 0.41679999232292175,  lr: 0.00995969551724138
step: 1118, train_loss: 1.339300513267517, acc: 0.42800000309944153, val_loss: 1.339438796043396, val_acc: 0.4163999855518341,  lr: 0.009959351034482759
step: 1119, train_loss: 1.312626838684082, acc: 0.4291999936103821, val_loss: 1.3384263515472412, val_acc: 0.42419999837875366,  lr: 0.009959006551724138
step: 1120, train_loss: 1.3148936033248901, acc: 0.43720000982284546, val_loss: 1.3320176601409912, val_acc: 0.4203999936580658,  lr: 0.009958662068965517
step: 1121, train_loss: 1.350294589996338, acc: 0.4124000072479248, val_loss: 1.3197847604751587, val_acc: 0.42340001463890076,  lr: 0.009958317586206897
step: 1122, train_loss: 1.2711113691329956, acc: 0.45899999141693115, val_loss: 1.314179539680481, val_acc: 0.4320000112056732,  lr: 0.009957973103448276
step: 1123, train_loss: 1.276294231414795, acc: 0.4602000117301941, val_loss: 1.3183014392852783, val_acc: 0.43140000104904175,  lr: 0.009957628620689656
step: 1124, train_loss: 1.2685554027557373, acc: 0.4593999981880188, val_loss: 1.3108863830566406, val_acc: 0.4381999969482422,  lr: 0.009957284137931035
step: 1125, train_loss: 1.3051830530166626, acc: 0.4447999894618988, val_loss: 1.3071860074996948, val_acc: 0.4368000030517578,  lr: 0.009956939655172414
step: 1126, train_loss: 1.2854008674621582, acc: 0.453000009059906, val_loss: 1.316552996635437, val_acc: 0.42879998683929443,  lr: 0.009956595172413794
step: 1127, train_loss: 1.312558650970459, acc: 0.43720000982284546, val_loss: 1.320502519607544, val_acc: 0.43619999289512634,  lr: 0.009956250689655173
step: 1128, train_loss: 1.2976083755493164, acc: 0.4490000009536743, val_loss: 1.323662519454956, val_acc: 0.4341999888420105,  lr: 0.009955906206896552
step: 1129, train_loss: 1.2884372472763062, acc: 0.453000009059906, val_loss: 1.3226815462112427, val_acc: 0.43700000643730164,  lr: 0.009955561724137932
step: 1130, train_loss: 1.3032567501068115, acc: 0.4440000057220459, val_loss: 1.3217145204544067, val_acc: 0.426800012588501,  lr: 0.009955217241379311
step: 1131, train_loss: 1.2697393894195557, acc: 0.4602000117301941, val_loss: 1.3105217218399048, val_acc: 0.42879998683929443,  lr: 0.00995487275862069
step: 1132, train_loss: 1.3050848245620728, acc: 0.4323999881744385, val_loss: 1.325332760810852, val_acc: 0.42260000109672546,  lr: 0.00995452827586207
step: 1133, train_loss: 1.3247915506362915, acc: 0.43220001459121704, val_loss: 1.335680365562439, val_acc: 0.420199990272522,  lr: 0.009954183793103448
step: 1134, train_loss: 1.365028738975525, acc: 0.41920000314712524, val_loss: 1.325355052947998, val_acc: 0.42899999022483826,  lr: 0.009953839310344827
step: 1135, train_loss: 1.3417223691940308, acc: 0.41819998621940613, val_loss: 1.3247416019439697, val_acc: 0.43140000104904175,  lr: 0.009953494827586206
step: 1136, train_loss: 1.2915886640548706, acc: 0.446399986743927, val_loss: 1.3077341318130493, val_acc: 0.4323999881744385,  lr: 0.009953150344827586
step: 1137, train_loss: 1.2848068475723267, acc: 0.45019999146461487, val_loss: 1.3045649528503418, val_acc: 0.446399986743927,  lr: 0.009952805862068965
step: 1138, train_loss: 1.306218147277832, acc: 0.45080000162124634, val_loss: 1.3230489492416382, val_acc: 0.4336000084877014,  lr: 0.009952461379310345
step: 1139, train_loss: 1.337260365486145, acc: 0.42559999227523804, val_loss: 1.3068434000015259, val_acc: 0.43479999899864197,  lr: 0.009952116896551724
step: 1140, train_loss: 1.3440183401107788, acc: 0.421999990940094, val_loss: 1.3121323585510254, val_acc: 0.4426000118255615,  lr: 0.009951772413793103
step: 1141, train_loss: 1.3153610229492188, acc: 0.44200000166893005, val_loss: 1.316602110862732, val_acc: 0.4375999867916107,  lr: 0.009951427931034483
step: 1142, train_loss: 1.2958208322525024, acc: 0.4458000063896179, val_loss: 1.3207920789718628, val_acc: 0.4408000111579895,  lr: 0.009951083448275862
step: 1143, train_loss: 1.3238518238067627, acc: 0.42739999294281006, val_loss: 1.3111767768859863, val_acc: 0.44119998812675476,  lr: 0.009950738965517241
step: 1144, train_loss: 1.3402715921401978, acc: 0.4262000024318695, val_loss: 1.3022091388702393, val_acc: 0.4431999921798706,  lr: 0.00995039448275862
step: 1145, train_loss: 1.2829667329788208, acc: 0.45680001378059387, val_loss: 1.2972631454467773, val_acc: 0.4431999921798706,  lr: 0.00995005
step: 1146, train_loss: 1.261706829071045, acc: 0.4634000062942505, val_loss: 1.2962162494659424, val_acc: 0.4431999921798706,  lr: 0.00994970551724138
step: 1147, train_loss: 1.3351224660873413, acc: 0.42559999227523804, val_loss: 1.2940773963928223, val_acc: 0.451200008392334,  lr: 0.009949361034482759
step: 1148, train_loss: 1.2759345769882202, acc: 0.45159998536109924, val_loss: 1.2946062088012695, val_acc: 0.44359999895095825,  lr: 0.009949016551724138
step: 1149, train_loss: 1.3078346252441406, acc: 0.44339999556541443, val_loss: 1.2975400686264038, val_acc: 0.444599986076355,  lr: 0.009948672068965518
step: 1150, train_loss: 1.2696964740753174, acc: 0.4496000111103058, val_loss: 1.299916386604309, val_acc: 0.4429999887943268,  lr: 0.009948327586206897
step: 1151, train_loss: 1.2637755870819092, acc: 0.45820000767707825, val_loss: 1.3018403053283691, val_acc: 0.43939998745918274,  lr: 0.009947983103448277
step: 1152, train_loss: 1.3135896921157837, acc: 0.44200000166893005, val_loss: 1.2984378337860107, val_acc: 0.4381999969482422,  lr: 0.009947638620689656
step: 1153, train_loss: 1.285871148109436, acc: 0.45719999074935913, val_loss: 1.2947043180465698, val_acc: 0.4368000030517578,  lr: 0.009947294137931035
step: 1154, train_loss: 1.2534793615341187, acc: 0.4675999879837036, val_loss: 1.2958316802978516, val_acc: 0.44040000438690186,  lr: 0.009946949655172415
step: 1155, train_loss: 1.315482497215271, acc: 0.420199990272522, val_loss: 1.2947907447814941, val_acc: 0.4424000084400177,  lr: 0.009946605172413794
step: 1156, train_loss: 1.2634568214416504, acc: 0.45879998803138733, val_loss: 1.2901726961135864, val_acc: 0.4458000063896179,  lr: 0.009946260689655173
step: 1157, train_loss: 1.287310004234314, acc: 0.4564000070095062, val_loss: 1.2878191471099854, val_acc: 0.446399986743927,  lr: 0.009945916206896551
step: 1158, train_loss: 1.328446865081787, acc: 0.4318000078201294, val_loss: 1.288563847541809, val_acc: 0.44760000705718994,  lr: 0.00994557172413793
step: 1159, train_loss: 1.2724649906158447, acc: 0.45879998803138733, val_loss: 1.299517035484314, val_acc: 0.44020000100135803,  lr: 0.00994522724137931
step: 1160, train_loss: 1.3167271614074707, acc: 0.42739999294281006, val_loss: 1.2927563190460205, val_acc: 0.44600000977516174,  lr: 0.00994488275862069
step: 1161, train_loss: 1.2550026178359985, acc: 0.4675999879837036, val_loss: 1.2938255071640015, val_acc: 0.4498000144958496,  lr: 0.009944538275862069
step: 1162, train_loss: 1.2923446893692017, acc: 0.44339999556541443, val_loss: 1.2996745109558105, val_acc: 0.4413999915122986,  lr: 0.009944193793103448
step: 1163, train_loss: 1.2602626085281372, acc: 0.459199994802475, val_loss: 1.2885560989379883, val_acc: 0.44620001316070557,  lr: 0.009943849310344827
step: 1164, train_loss: 1.3056769371032715, acc: 0.4318000078201294, val_loss: 1.2939947843551636, val_acc: 0.44859999418258667,  lr: 0.009943504827586207
step: 1165, train_loss: 1.2934772968292236, acc: 0.44119998812675476, val_loss: 1.2955751419067383, val_acc: 0.4453999996185303,  lr: 0.009943160344827586
step: 1166, train_loss: 1.3175489902496338, acc: 0.43880000710487366, val_loss: 1.299673080444336, val_acc: 0.4343999922275543,  lr: 0.009942815862068966
step: 1167, train_loss: 1.2944382429122925, acc: 0.4336000084877014, val_loss: 1.2957732677459717, val_acc: 0.4375999867916107,  lr: 0.009942471379310345
step: 1168, train_loss: 1.2487080097198486, acc: 0.4717999994754791, val_loss: 1.295736312866211, val_acc: 0.4465999901294708,  lr: 0.009942126896551724
step: 1169, train_loss: 1.2973911762237549, acc: 0.4440000057220459, val_loss: 1.2957439422607422, val_acc: 0.44179999828338623,  lr: 0.009941782413793104
step: 1170, train_loss: 1.2889147996902466, acc: 0.4458000063896179, val_loss: 1.3131036758422852, val_acc: 0.436599999666214,  lr: 0.009941437931034483
step: 1171, train_loss: 1.2964346408843994, acc: 0.44339999556541443, val_loss: 1.30446195602417, val_acc: 0.43959999084472656,  lr: 0.009941093448275862
step: 1172, train_loss: 1.2910544872283936, acc: 0.4424000084400177, val_loss: 1.3106052875518799, val_acc: 0.4341999888420105,  lr: 0.009940748965517242
step: 1173, train_loss: 1.2842549085617065, acc: 0.4643999934196472, val_loss: 1.3150396347045898, val_acc: 0.4293999969959259,  lr: 0.009940404482758621
step: 1174, train_loss: 1.2949265241622925, acc: 0.4424000084400177, val_loss: 1.31046724319458, val_acc: 0.4341999888420105,  lr: 0.00994006
step: 1175, train_loss: 1.2940900325775146, acc: 0.4480000138282776, val_loss: 1.3211318254470825, val_acc: 0.43160000443458557,  lr: 0.00993971551724138
step: 1176, train_loss: 1.2692673206329346, acc: 0.454800009727478, val_loss: 1.3104761838912964, val_acc: 0.43380001187324524,  lr: 0.00993937103448276
step: 1177, train_loss: 1.3009512424468994, acc: 0.4374000132083893, val_loss: 1.3096435070037842, val_acc: 0.4410000145435333,  lr: 0.009939026551724139
step: 1178, train_loss: 1.2858062982559204, acc: 0.45320001244544983, val_loss: 1.324413776397705, val_acc: 0.42879998683929443,  lr: 0.009938682068965518
step: 1179, train_loss: 1.2949728965759277, acc: 0.4480000138282776, val_loss: 1.3070344924926758, val_acc: 0.4424000084400177,  lr: 0.009938337586206897
step: 1180, train_loss: 1.2550228834152222, acc: 0.460999995470047, val_loss: 1.317306399345398, val_acc: 0.4325999915599823,  lr: 0.009937993103448277
step: 1181, train_loss: 1.2743394374847412, acc: 0.4505999982357025, val_loss: 1.3093887567520142, val_acc: 0.44020000100135803,  lr: 0.009937648620689656
step: 1182, train_loss: 1.254664421081543, acc: 0.46939998865127563, val_loss: 1.317234754562378, val_acc: 0.43299999833106995,  lr: 0.009937304137931034
step: 1183, train_loss: 1.264813780784607, acc: 0.45739999413490295, val_loss: 1.3058229684829712, val_acc: 0.43860000371932983,  lr: 0.009936959655172413
step: 1184, train_loss: 1.2898414134979248, acc: 0.44179999828338623, val_loss: 1.3154444694519043, val_acc: 0.43059998750686646,  lr: 0.009936615172413793
step: 1185, train_loss: 1.3160161972045898, acc: 0.42179998755455017, val_loss: 1.311774730682373, val_acc: 0.4350000023841858,  lr: 0.009936270689655172
step: 1186, train_loss: 1.3328497409820557, acc: 0.4251999855041504, val_loss: 1.3092535734176636, val_acc: 0.4350000023841858,  lr: 0.009935926206896551
step: 1187, train_loss: 1.2581441402435303, acc: 0.46239998936653137, val_loss: 1.301178216934204, val_acc: 0.4458000063896179,  lr: 0.00993558172413793
step: 1188, train_loss: 1.285059928894043, acc: 0.44920000433921814, val_loss: 1.2979434728622437, val_acc: 0.45080000162124634,  lr: 0.00993523724137931
step: 1189, train_loss: 1.2928425073623657, acc: 0.44600000977516174, val_loss: 1.2943557500839233, val_acc: 0.44519999623298645,  lr: 0.00993489275862069
step: 1190, train_loss: 1.2931067943572998, acc: 0.45579999685287476, val_loss: 1.2976055145263672, val_acc: 0.4474000036716461,  lr: 0.009934548275862069
step: 1191, train_loss: 1.2490642070770264, acc: 0.46560001373291016, val_loss: 1.2943795919418335, val_acc: 0.4480000138282776,  lr: 0.009934203793103448
step: 1192, train_loss: 1.2507688999176025, acc: 0.4553999900817871, val_loss: 1.2982157468795776, val_acc: 0.4415999948978424,  lr: 0.009933859310344828
step: 1193, train_loss: 1.2791566848754883, acc: 0.4521999955177307, val_loss: 1.2935987710952759, val_acc: 0.4397999942302704,  lr: 0.009933514827586207
step: 1194, train_loss: 1.2566657066345215, acc: 0.46700000762939453, val_loss: 1.2965259552001953, val_acc: 0.44519999623298645,  lr: 0.009933170344827586
step: 1195, train_loss: 1.2660822868347168, acc: 0.4560000002384186, val_loss: 1.3000359535217285, val_acc: 0.44760000705718994,  lr: 0.009932825862068966
step: 1196, train_loss: 1.328553318977356, acc: 0.430400013923645, val_loss: 1.3062090873718262, val_acc: 0.444599986076355,  lr: 0.009932481379310345
step: 1197, train_loss: 1.2498424053192139, acc: 0.4625999927520752, val_loss: 1.3012936115264893, val_acc: 0.4490000009536743,  lr: 0.009932136896551725
step: 1198, train_loss: 1.3292251825332642, acc: 0.430400013923645, val_loss: 1.2973977327346802, val_acc: 0.44780001044273376,  lr: 0.009931792413793104
step: 1199, train_loss: 1.29587984085083, acc: 0.4487999975681305, val_loss: 1.2909212112426758, val_acc: 0.4487999975681305,  lr: 0.009931447931034483
step: 1200, train_loss: 1.2546571493148804, acc: 0.4657999873161316, val_loss: 1.2859230041503906, val_acc: 0.44780001044273376,  lr: 0.009931103448275863
step: 1201, train_loss: 1.253594994544983, acc: 0.4657999873161316, val_loss: 1.2822139263153076, val_acc: 0.4487999975681305,  lr: 0.009930758965517242
step: 1202, train_loss: 1.2868324518203735, acc: 0.4438000023365021, val_loss: 1.2852545976638794, val_acc: 0.4465999901294708,  lr: 0.009930414482758622
step: 1203, train_loss: 1.2554932832717896, acc: 0.4668000042438507, val_loss: 1.2805849313735962, val_acc: 0.45080000162124634,  lr: 0.009930070000000001
step: 1204, train_loss: 1.2556155920028687, acc: 0.46459999680519104, val_loss: 1.2848488092422485, val_acc: 0.4553999900817871,  lr: 0.00992972551724138
step: 1205, train_loss: 1.2593132257461548, acc: 0.4643999934196472, val_loss: 1.2817833423614502, val_acc: 0.45320001244544983,  lr: 0.00992938103448276
step: 1206, train_loss: 1.2703901529312134, acc: 0.4519999921321869, val_loss: 1.2755986452102661, val_acc: 0.45100000500679016,  lr: 0.009929036551724137
step: 1207, train_loss: 1.2468912601470947, acc: 0.4706000089645386, val_loss: 1.2871190309524536, val_acc: 0.4447999894618988,  lr: 0.009928692068965517
step: 1208, train_loss: 1.276389241218567, acc: 0.4641999900341034, val_loss: 1.2818723917007446, val_acc: 0.448199987411499,  lr: 0.009928347586206896
step: 1209, train_loss: 1.2974249124526978, acc: 0.4397999942302704, val_loss: 1.288511872291565, val_acc: 0.43860000371932983,  lr: 0.009928003103448275
step: 1210, train_loss: 1.2929192781448364, acc: 0.43700000643730164, val_loss: 1.2875651121139526, val_acc: 0.44780001044273376,  lr: 0.009927658620689655
step: 1211, train_loss: 1.2722586393356323, acc: 0.45500001311302185, val_loss: 1.2969814538955688, val_acc: 0.4442000091075897,  lr: 0.009927314137931034
step: 1212, train_loss: 1.2547266483306885, acc: 0.47200000286102295, val_loss: 1.3008365631103516, val_acc: 0.43860000371932983,  lr: 0.009926969655172414
step: 1213, train_loss: 1.3120137453079224, acc: 0.43619999289512634, val_loss: 1.2966006994247437, val_acc: 0.444599986076355,  lr: 0.009926625172413793
step: 1214, train_loss: 1.2490334510803223, acc: 0.46219998598098755, val_loss: 1.307132601737976, val_acc: 0.43459999561309814,  lr: 0.009926280689655172
step: 1215, train_loss: 1.3211477994918823, acc: 0.42340001463890076, val_loss: 1.2853238582611084, val_acc: 0.45879998803138733,  lr: 0.009925936206896552
step: 1216, train_loss: 1.3053689002990723, acc: 0.4277999997138977, val_loss: 1.2936211824417114, val_acc: 0.45500001311302185,  lr: 0.009925591724137931
step: 1217, train_loss: 1.3123300075531006, acc: 0.4334000051021576, val_loss: 1.2882249355316162, val_acc: 0.45320001244544983,  lr: 0.00992524724137931
step: 1218, train_loss: 1.2492187023162842, acc: 0.4713999927043915, val_loss: 1.2835772037506104, val_acc: 0.4521999955177307,  lr: 0.00992490275862069
step: 1219, train_loss: 1.2610678672790527, acc: 0.462799996137619, val_loss: 1.2856497764587402, val_acc: 0.45239999890327454,  lr: 0.00992455827586207
step: 1220, train_loss: 1.3525437116622925, acc: 0.41839998960494995, val_loss: 1.287423014640808, val_acc: 0.4498000144958496,  lr: 0.009924213793103449
step: 1221, train_loss: 1.2888239622116089, acc: 0.44780001044273376, val_loss: 1.284778118133545, val_acc: 0.4535999894142151,  lr: 0.009923869310344828
step: 1222, train_loss: 1.3134692907333374, acc: 0.4302000105381012, val_loss: 1.285204291343689, val_acc: 0.45260000228881836,  lr: 0.009923524827586207
step: 1223, train_loss: 1.2641057968139648, acc: 0.46160000562667847, val_loss: 1.2874587774276733, val_acc: 0.4496000111103058,  lr: 0.009923180344827587
step: 1224, train_loss: 1.3146073818206787, acc: 0.4275999963283539, val_loss: 1.2852853536605835, val_acc: 0.4496000111103058,  lr: 0.009922835862068966
step: 1225, train_loss: 1.253345012664795, acc: 0.46860000491142273, val_loss: 1.2899572849273682, val_acc: 0.4514000117778778,  lr: 0.009922491379310346
step: 1226, train_loss: 1.2777923345565796, acc: 0.4480000138282776, val_loss: 1.2836112976074219, val_acc: 0.4474000036716461,  lr: 0.009922146896551725
step: 1227, train_loss: 1.2510011196136475, acc: 0.46939998865127563, val_loss: 1.283979892730713, val_acc: 0.44940000772476196,  lr: 0.009921802413793104
step: 1228, train_loss: 1.2502872943878174, acc: 0.4691999852657318, val_loss: 1.2850592136383057, val_acc: 0.44859999418258667,  lr: 0.009921457931034484
step: 1229, train_loss: 1.2414270639419556, acc: 0.4708000123500824, val_loss: 1.2863727807998657, val_acc: 0.44620001316070557,  lr: 0.009921113448275863
step: 1230, train_loss: 1.2244986295700073, acc: 0.4713999927043915, val_loss: 1.2916531562805176, val_acc: 0.4431999921798706,  lr: 0.00992076896551724
step: 1231, train_loss: 1.2716761827468872, acc: 0.46000000834465027, val_loss: 1.3010917901992798, val_acc: 0.44119998812675476,  lr: 0.00992042448275862
step: 1232, train_loss: 1.2993216514587402, acc: 0.428600013256073, val_loss: 1.2985013723373413, val_acc: 0.4397999942302704,  lr: 0.00992008
step: 1233, train_loss: 1.2648292779922485, acc: 0.46380001306533813, val_loss: 1.305237889289856, val_acc: 0.4374000132083893,  lr: 0.009919735517241379
step: 1234, train_loss: 1.2393571138381958, acc: 0.4666000008583069, val_loss: 1.2966387271881104, val_acc: 0.43779999017715454,  lr: 0.009919391034482758
step: 1235, train_loss: 1.2582474946975708, acc: 0.4575999975204468, val_loss: 1.2957298755645752, val_acc: 0.43639999628067017,  lr: 0.009919046551724138
step: 1236, train_loss: 1.2929600477218628, acc: 0.45260000228881836, val_loss: 1.3011478185653687, val_acc: 0.43160000443458557,  lr: 0.009918702068965517
step: 1237, train_loss: 1.2197953462600708, acc: 0.4821999967098236, val_loss: 1.2995243072509766, val_acc: 0.4392000138759613,  lr: 0.009918357586206896
step: 1238, train_loss: 1.3178623914718628, acc: 0.43320000171661377, val_loss: 1.3046369552612305, val_acc: 0.4390000104904175,  lr: 0.009918013103448276
step: 1239, train_loss: 1.3076865673065186, acc: 0.4392000138759613, val_loss: 1.3083009719848633, val_acc: 0.43880000710487366,  lr: 0.009917668620689655
step: 1240, train_loss: 1.2827378511428833, acc: 0.4453999996185303, val_loss: 1.3063435554504395, val_acc: 0.4392000138759613,  lr: 0.009917324137931035
step: 1241, train_loss: 1.2553664445877075, acc: 0.46399998664855957, val_loss: 1.3079590797424316, val_acc: 0.4375999867916107,  lr: 0.009916979655172414
step: 1242, train_loss: 1.2532891035079956, acc: 0.4643999934196472, val_loss: 1.303571105003357, val_acc: 0.43799999356269836,  lr: 0.009916635172413793
step: 1243, train_loss: 1.2287354469299316, acc: 0.4749999940395355, val_loss: 1.3088802099227905, val_acc: 0.4352000057697296,  lr: 0.009916290689655173
step: 1244, train_loss: 1.2793830633163452, acc: 0.4514000117778778, val_loss: 1.3035743236541748, val_acc: 0.4318000078201294,  lr: 0.009915946206896552
step: 1245, train_loss: 1.224844217300415, acc: 0.4796000123023987, val_loss: 1.3054389953613281, val_acc: 0.438400000333786,  lr: 0.009915601724137931
step: 1246, train_loss: 1.2235609292984009, acc: 0.4819999933242798, val_loss: 1.3036515712738037, val_acc: 0.44040000438690186,  lr: 0.009915257241379311
step: 1247, train_loss: 1.235447883605957, acc: 0.47699999809265137, val_loss: 1.3138666152954102, val_acc: 0.4341999888420105,  lr: 0.00991491275862069
step: 1248, train_loss: 1.33792245388031, acc: 0.42480000853538513, val_loss: 1.3004393577575684, val_acc: 0.4350000023841858,  lr: 0.00991456827586207
step: 1249, train_loss: 1.2610985040664673, acc: 0.4607999920845032, val_loss: 1.3056814670562744, val_acc: 0.4350000023841858,  lr: 0.009914223793103449
step: 1250, train_loss: 1.260291576385498, acc: 0.4650000035762787, val_loss: 1.3032588958740234, val_acc: 0.43619999289512634,  lr: 0.009913879310344828
step: 1251, train_loss: 1.2468934059143066, acc: 0.4659999907016754, val_loss: 1.2941378355026245, val_acc: 0.4406000077724457,  lr: 0.009913534827586208
step: 1252, train_loss: 1.2672895193099976, acc: 0.4659999907016754, val_loss: 1.2926807403564453, val_acc: 0.44600000977516174,  lr: 0.009913190344827587
step: 1253, train_loss: 1.2586852312088013, acc: 0.4618000090122223, val_loss: 1.299095630645752, val_acc: 0.4406000077724457,  lr: 0.009912845862068967
step: 1254, train_loss: 1.348039984703064, acc: 0.4259999990463257, val_loss: 1.296046257019043, val_acc: 0.4368000030517578,  lr: 0.009912501379310344
step: 1255, train_loss: 1.2619718313217163, acc: 0.4657999873161316, val_loss: 1.298933744430542, val_acc: 0.4413999915122986,  lr: 0.009912156896551724
step: 1256, train_loss: 1.25796639919281, acc: 0.46140000224113464, val_loss: 1.3049724102020264, val_acc: 0.4399999976158142,  lr: 0.009911812413793103
step: 1257, train_loss: 1.2508940696716309, acc: 0.46720001101493835, val_loss: 1.2979633808135986, val_acc: 0.4426000118255615,  lr: 0.009911467931034482
step: 1258, train_loss: 1.3220789432525635, acc: 0.4327999949455261, val_loss: 1.2965009212493896, val_acc: 0.44279998540878296,  lr: 0.009911123448275862
step: 1259, train_loss: 1.312927484512329, acc: 0.428600013256073, val_loss: 1.2989332675933838, val_acc: 0.44519999623298645,  lr: 0.009910778965517241
step: 1260, train_loss: 1.240760087966919, acc: 0.48100000619888306, val_loss: 1.291364312171936, val_acc: 0.44699999690055847,  lr: 0.00991043448275862
step: 1261, train_loss: 1.330336093902588, acc: 0.42660000920295715, val_loss: 1.279935359954834, val_acc: 0.4514000117778778,  lr: 0.00991009
step: 1262, train_loss: 1.3012797832489014, acc: 0.4374000132083893, val_loss: 1.275086760520935, val_acc: 0.45579999685287476,  lr: 0.00990974551724138
step: 1263, train_loss: 1.312121868133545, acc: 0.43479999899864197, val_loss: 1.280564785003662, val_acc: 0.4551999866962433,  lr: 0.009909401034482759
step: 1264, train_loss: 1.268789529800415, acc: 0.4603999853134155, val_loss: 1.2832916975021362, val_acc: 0.4424000084400177,  lr: 0.009909056551724138
step: 1265, train_loss: 1.3192871809005737, acc: 0.4339999854564667, val_loss: 1.2726305723190308, val_acc: 0.4580000042915344,  lr: 0.009908712068965517
step: 1266, train_loss: 1.3020963668823242, acc: 0.43639999628067017, val_loss: 1.2726579904556274, val_acc: 0.4593999981880188,  lr: 0.009908367586206897
step: 1267, train_loss: 1.2570236921310425, acc: 0.4715999960899353, val_loss: 1.264364242553711, val_acc: 0.4586000144481659,  lr: 0.009908023103448276
step: 1268, train_loss: 1.2844161987304688, acc: 0.4397999942302704, val_loss: 1.2718069553375244, val_acc: 0.4560000002384186,  lr: 0.009907678620689656
step: 1269, train_loss: 1.252517580986023, acc: 0.46059998869895935, val_loss: 1.2603293657302856, val_acc: 0.45820000767707825,  lr: 0.009907334137931035
step: 1270, train_loss: 1.2407125234603882, acc: 0.46799999475479126, val_loss: 1.2728960514068604, val_acc: 0.45419999957084656,  lr: 0.009906989655172414
step: 1271, train_loss: 1.3160804510116577, acc: 0.428600013256073, val_loss: 1.2656577825546265, val_acc: 0.4652000069618225,  lr: 0.009906645172413794
step: 1272, train_loss: 1.2481987476348877, acc: 0.4643999934196472, val_loss: 1.2769193649291992, val_acc: 0.4602000117301941,  lr: 0.009906300689655173
step: 1273, train_loss: 1.2579023838043213, acc: 0.462799996137619, val_loss: 1.2792754173278809, val_acc: 0.4586000144481659,  lr: 0.009905956206896552
step: 1274, train_loss: 1.3272476196289062, acc: 0.43140000104904175, val_loss: 1.2785145044326782, val_acc: 0.4544000029563904,  lr: 0.009905611724137932
step: 1275, train_loss: 1.2616088390350342, acc: 0.4607999920845032, val_loss: 1.2862608432769775, val_acc: 0.45159998536109924,  lr: 0.009905267241379311
step: 1276, train_loss: 1.3058674335479736, acc: 0.43220001459121704, val_loss: 1.272662878036499, val_acc: 0.45579999685287476,  lr: 0.00990492275862069
step: 1277, train_loss: 1.282581090927124, acc: 0.4593999981880188, val_loss: 1.271626353263855, val_acc: 0.448199987411499,  lr: 0.00990457827586207
step: 1278, train_loss: 1.2547577619552612, acc: 0.46540001034736633, val_loss: 1.274857759475708, val_acc: 0.45159998536109924,  lr: 0.009904233793103448
step: 1279, train_loss: 1.2582216262817383, acc: 0.45980000495910645, val_loss: 1.2700130939483643, val_acc: 0.4480000138282776,  lr: 0.009903889310344827
step: 1280, train_loss: 1.2980319261550903, acc: 0.44179999828338623, val_loss: 1.2765229940414429, val_acc: 0.4519999921321869,  lr: 0.009903544827586206
step: 1281, train_loss: 1.2940970659255981, acc: 0.4514000117778778, val_loss: 1.2829580307006836, val_acc: 0.45159998536109924,  lr: 0.009903200344827586
step: 1282, train_loss: 1.2470818758010864, acc: 0.47380000352859497, val_loss: 1.2860080003738403, val_acc: 0.4490000009536743,  lr: 0.009902855862068965
step: 1283, train_loss: 1.282039999961853, acc: 0.4503999948501587, val_loss: 1.2849031686782837, val_acc: 0.44200000166893005,  lr: 0.009902511379310345
step: 1284, train_loss: 1.2565913200378418, acc: 0.4659999907016754, val_loss: 1.2881828546524048, val_acc: 0.4442000091075897,  lr: 0.009902166896551724
step: 1285, train_loss: 1.3060178756713867, acc: 0.4431999921798706, val_loss: 1.280729055404663, val_acc: 0.4505999982357025,  lr: 0.009901822413793103
step: 1286, train_loss: 1.2821789979934692, acc: 0.4505999982357025, val_loss: 1.2862223386764526, val_acc: 0.4537999927997589,  lr: 0.009901477931034483
step: 1287, train_loss: 1.2916415929794312, acc: 0.4458000063896179, val_loss: 1.286039113998413, val_acc: 0.4442000091075897,  lr: 0.009901133448275862
step: 1288, train_loss: 1.2896374464035034, acc: 0.4456000030040741, val_loss: 1.289772629737854, val_acc: 0.4410000145435333,  lr: 0.009900788965517241
step: 1289, train_loss: 1.268708348274231, acc: 0.4620000123977661, val_loss: 1.3007112741470337, val_acc: 0.4392000138759613,  lr: 0.00990044448275862
step: 1290, train_loss: 1.275454044342041, acc: 0.46299999952316284, val_loss: 1.2887221574783325, val_acc: 0.4449999928474426,  lr: 0.0099001
step: 1291, train_loss: 1.3003122806549072, acc: 0.4325999915599823, val_loss: 1.2943246364593506, val_acc: 0.4399999976158142,  lr: 0.00989975551724138
step: 1292, train_loss: 1.274335503578186, acc: 0.4625999927520752, val_loss: 1.286952257156372, val_acc: 0.4415999948978424,  lr: 0.009899411034482759
step: 1293, train_loss: 1.304137945175171, acc: 0.430400013923645, val_loss: 1.2858467102050781, val_acc: 0.4487999975681305,  lr: 0.009899066551724138
step: 1294, train_loss: 1.2457811832427979, acc: 0.48019999265670776, val_loss: 1.2836825847625732, val_acc: 0.4514000117778778,  lr: 0.009898722068965518
step: 1295, train_loss: 1.252560019493103, acc: 0.4674000144004822, val_loss: 1.2732707262039185, val_acc: 0.4560000002384186,  lr: 0.009898377586206897
step: 1296, train_loss: 1.2210021018981934, acc: 0.4747999906539917, val_loss: 1.2801929712295532, val_acc: 0.4514000117778778,  lr: 0.009898033103448277
step: 1297, train_loss: 1.2773936986923218, acc: 0.4465999901294708, val_loss: 1.2783459424972534, val_acc: 0.45419999957084656,  lr: 0.009897688620689656
step: 1298, train_loss: 1.30696439743042, acc: 0.4375999867916107, val_loss: 1.2732073068618774, val_acc: 0.45260000228881836,  lr: 0.009897344137931035
step: 1299, train_loss: 1.2390233278274536, acc: 0.4749999940395355, val_loss: 1.2841869592666626, val_acc: 0.4413999915122986,  lr: 0.009896999655172415
step: 1300, train_loss: 1.2726417779922485, acc: 0.4505999982357025, val_loss: 1.275606393814087, val_acc: 0.44679999351501465,  lr: 0.009896655172413794
step: 1301, train_loss: 1.2885277271270752, acc: 0.4429999887943268, val_loss: 1.281356692314148, val_acc: 0.44679999351501465,  lr: 0.009896310689655173
step: 1302, train_loss: 1.2445838451385498, acc: 0.4708000123500824, val_loss: 1.2780288457870483, val_acc: 0.45260000228881836,  lr: 0.009895966206896551
step: 1303, train_loss: 1.2639786005020142, acc: 0.4643999934196472, val_loss: 1.2798508405685425, val_acc: 0.44920000433921814,  lr: 0.00989562172413793
step: 1304, train_loss: 1.2489935159683228, acc: 0.4697999954223633, val_loss: 1.2808939218521118, val_acc: 0.4519999921321869,  lr: 0.00989527724137931
step: 1305, train_loss: 1.2905404567718506, acc: 0.4413999915122986, val_loss: 1.2799400091171265, val_acc: 0.45100000500679016,  lr: 0.00989493275862069
step: 1306, train_loss: 1.2641701698303223, acc: 0.4578000009059906, val_loss: 1.2837262153625488, val_acc: 0.4440000057220459,  lr: 0.009894588275862069
step: 1307, train_loss: 1.2653928995132446, acc: 0.4634000062942505, val_loss: 1.2946884632110596, val_acc: 0.4442000091075897,  lr: 0.009894243793103448
step: 1308, train_loss: 1.271409273147583, acc: 0.4596000015735626, val_loss: 1.2807520627975464, val_acc: 0.446399986743927,  lr: 0.009893899310344827
step: 1309, train_loss: 1.2406340837478638, acc: 0.46779999136924744, val_loss: 1.2875151634216309, val_acc: 0.45100000500679016,  lr: 0.009893554827586207
step: 1310, train_loss: 1.286482572555542, acc: 0.453000009059906, val_loss: 1.287743091583252, val_acc: 0.444599986076355,  lr: 0.009893210344827586
step: 1311, train_loss: 1.3146225214004517, acc: 0.4318000078201294, val_loss: 1.286036491394043, val_acc: 0.4442000091075897,  lr: 0.009892865862068966
step: 1312, train_loss: 1.2693268060684204, acc: 0.4453999996185303, val_loss: 1.2982455492019653, val_acc: 0.44040000438690186,  lr: 0.009892521379310345
step: 1313, train_loss: 1.2661762237548828, acc: 0.4569999873638153, val_loss: 1.290656566619873, val_acc: 0.4410000145435333,  lr: 0.009892176896551724
step: 1314, train_loss: 1.240522027015686, acc: 0.46860000491142273, val_loss: 1.2869842052459717, val_acc: 0.44279998540878296,  lr: 0.009891832413793104
step: 1315, train_loss: 1.2158770561218262, acc: 0.48420000076293945, val_loss: 1.2894957065582275, val_acc: 0.4406000077724457,  lr: 0.009891487931034483
step: 1316, train_loss: 1.2442584037780762, acc: 0.4643999934196472, val_loss: 1.2839553356170654, val_acc: 0.4465999901294708,  lr: 0.009891143448275862
step: 1317, train_loss: 1.2395514249801636, acc: 0.47839999198913574, val_loss: 1.2917217016220093, val_acc: 0.4406000077724457,  lr: 0.009890798965517242
step: 1318, train_loss: 1.2487674951553345, acc: 0.4620000123977661, val_loss: 1.3003710508346558, val_acc: 0.4374000132083893,  lr: 0.009890454482758621
step: 1319, train_loss: 1.1996990442276, acc: 0.48919999599456787, val_loss: 1.2957638502120972, val_acc: 0.43619999289512634,  lr: 0.00989011
step: 1320, train_loss: 1.267781138420105, acc: 0.4474000036716461, val_loss: 1.2929600477218628, val_acc: 0.44359999895095825,  lr: 0.00988976551724138
step: 1321, train_loss: 1.2618931531906128, acc: 0.46860000491142273, val_loss: 1.295536994934082, val_acc: 0.4374000132083893,  lr: 0.00988942103448276
step: 1322, train_loss: 1.2620290517807007, acc: 0.46320000290870667, val_loss: 1.2945754528045654, val_acc: 0.4440000057220459,  lr: 0.009889076551724139
step: 1323, train_loss: 1.2483463287353516, acc: 0.4724000096321106, val_loss: 1.293257474899292, val_acc: 0.44359999895095825,  lr: 0.009888732068965518
step: 1324, train_loss: 1.236801028251648, acc: 0.4726000130176544, val_loss: 1.2953816652297974, val_acc: 0.44440001249313354,  lr: 0.009888387586206897
step: 1325, train_loss: 1.2435791492462158, acc: 0.4666000008583069, val_loss: 1.3021694421768188, val_acc: 0.4413999915122986,  lr: 0.009888043103448277
step: 1326, train_loss: 1.236764669418335, acc: 0.47839999198913574, val_loss: 1.2972917556762695, val_acc: 0.44179999828338623,  lr: 0.009887698620689656
step: 1327, train_loss: 1.2643895149230957, acc: 0.45680001378059387, val_loss: 1.2962827682495117, val_acc: 0.4413999915122986,  lr: 0.009887354137931034
step: 1328, train_loss: 1.2670754194259644, acc: 0.46160000562667847, val_loss: 1.3017995357513428, val_acc: 0.4426000118255615,  lr: 0.009887009655172413
step: 1329, train_loss: 1.2242331504821777, acc: 0.47620001435279846, val_loss: 1.2963075637817383, val_acc: 0.4408000111579895,  lr: 0.009886665172413793
step: 1330, train_loss: 1.2072011232376099, acc: 0.4941999912261963, val_loss: 1.3050014972686768, val_acc: 0.4375999867916107,  lr: 0.009886320689655172
step: 1331, train_loss: 1.2463772296905518, acc: 0.47859999537467957, val_loss: 1.3046174049377441, val_acc: 0.43700000643730164,  lr: 0.009885976206896551
step: 1332, train_loss: 1.3193641901016235, acc: 0.4244000017642975, val_loss: 1.31404447555542, val_acc: 0.43160000443458557,  lr: 0.00988563172413793
step: 1333, train_loss: 1.2443034648895264, acc: 0.4747999906539917, val_loss: 1.3208870887756348, val_acc: 0.4368000030517578,  lr: 0.00988528724137931
step: 1334, train_loss: 1.356036901473999, acc: 0.4221999943256378, val_loss: 1.305572748184204, val_acc: 0.44339999556541443,  lr: 0.00988494275862069
step: 1335, train_loss: 1.3049696683883667, acc: 0.4458000063896179, val_loss: 1.3124463558197021, val_acc: 0.4424000084400177,  lr: 0.009884598275862069
step: 1336, train_loss: 1.3056989908218384, acc: 0.4440000057220459, val_loss: 1.3011629581451416, val_acc: 0.4505999982357025,  lr: 0.009884253793103448
step: 1337, train_loss: 1.2946219444274902, acc: 0.4498000144958496, val_loss: 1.3041014671325684, val_acc: 0.4474000036716461,  lr: 0.009883909310344828
step: 1338, train_loss: 1.2968782186508179, acc: 0.44679999351501465, val_loss: 1.3012317419052124, val_acc: 0.4429999887943268,  lr: 0.009883564827586207
step: 1339, train_loss: 1.232817530632019, acc: 0.4814000129699707, val_loss: 1.2988802194595337, val_acc: 0.4381999969482422,  lr: 0.009883220344827586
step: 1340, train_loss: 1.2627851963043213, acc: 0.46700000762939453, val_loss: 1.2982817888259888, val_acc: 0.43959999084472656,  lr: 0.009882875862068966
step: 1341, train_loss: 1.2632951736450195, acc: 0.45739999413490295, val_loss: 1.2993215322494507, val_acc: 0.4447999894618988,  lr: 0.009882531379310345
step: 1342, train_loss: 1.2468881607055664, acc: 0.4722000062465668, val_loss: 1.2895803451538086, val_acc: 0.4535999894142151,  lr: 0.009882186896551725
step: 1343, train_loss: 1.3296979665756226, acc: 0.43799999356269836, val_loss: 1.2914154529571533, val_acc: 0.45100000500679016,  lr: 0.009881842413793104
step: 1344, train_loss: 1.2001750469207764, acc: 0.4966000020503998, val_loss: 1.2941209077835083, val_acc: 0.4474000036716461,  lr: 0.009881497931034483
step: 1345, train_loss: 1.327461838722229, acc: 0.4320000112056732, val_loss: 1.3000128269195557, val_acc: 0.44679999351501465,  lr: 0.009881153448275863
step: 1346, train_loss: 1.298386573791504, acc: 0.45100000500679016, val_loss: 1.2937268018722534, val_acc: 0.44780001044273376,  lr: 0.009880808965517242
step: 1347, train_loss: 1.2932769060134888, acc: 0.43959999084472656, val_loss: 1.2874765396118164, val_acc: 0.44999998807907104,  lr: 0.009880464482758622
step: 1348, train_loss: 1.2965385913848877, acc: 0.4474000036716461, val_loss: 1.2863398790359497, val_acc: 0.45320001244544983,  lr: 0.009880120000000001
step: 1349, train_loss: 1.2833610773086548, acc: 0.451200008392334, val_loss: 1.287915587425232, val_acc: 0.4544000029563904,  lr: 0.00987977551724138
step: 1350, train_loss: 1.324143648147583, acc: 0.4336000084877014, val_loss: 1.2794642448425293, val_acc: 0.46140000224113464,  lr: 0.00987943103448276
step: 1351, train_loss: 1.2733354568481445, acc: 0.45719999074935913, val_loss: 1.2735141515731812, val_acc: 0.46380001306533813,  lr: 0.009879086551724137
step: 1352, train_loss: 1.2422360181808472, acc: 0.47519999742507935, val_loss: 1.2745976448059082, val_acc: 0.46619999408721924,  lr: 0.009878742068965517
step: 1353, train_loss: 1.2532453536987305, acc: 0.4620000123977661, val_loss: 1.272026777267456, val_acc: 0.46059998869895935,  lr: 0.009878397586206896
step: 1354, train_loss: 1.2905696630477905, acc: 0.45179998874664307, val_loss: 1.2775055170059204, val_acc: 0.4586000144481659,  lr: 0.009878053103448275
step: 1355, train_loss: 1.2997379302978516, acc: 0.44519999623298645, val_loss: 1.2828775644302368, val_acc: 0.45159998536109924,  lr: 0.009877708620689655
step: 1356, train_loss: 1.2948907613754272, acc: 0.4503999948501587, val_loss: 1.2742059230804443, val_acc: 0.4611999988555908,  lr: 0.009877364137931034
step: 1357, train_loss: 1.2962253093719482, acc: 0.4438000023365021, val_loss: 1.2736947536468506, val_acc: 0.46140000224113464,  lr: 0.009877019655172414
step: 1358, train_loss: 1.2917985916137695, acc: 0.45159998536109924, val_loss: 1.2667109966278076, val_acc: 0.4634000062942505,  lr: 0.009876675172413793
step: 1359, train_loss: 1.2515580654144287, acc: 0.46959999203681946, val_loss: 1.2701170444488525, val_acc: 0.459199994802475,  lr: 0.009876330689655172
step: 1360, train_loss: 1.2420215606689453, acc: 0.4684000015258789, val_loss: 1.2667304277420044, val_acc: 0.46299999952316284,  lr: 0.009875986206896552
step: 1361, train_loss: 1.2942464351654053, acc: 0.4397999942302704, val_loss: 1.2665835618972778, val_acc: 0.46059998869895935,  lr: 0.009875641724137931
step: 1362, train_loss: 1.2050213813781738, acc: 0.4936000108718872, val_loss: 1.2643327713012695, val_acc: 0.46239998936653137,  lr: 0.00987529724137931
step: 1363, train_loss: 1.2526016235351562, acc: 0.4643999934196472, val_loss: 1.2613496780395508, val_acc: 0.46459999680519104,  lr: 0.00987495275862069
step: 1364, train_loss: 1.2502150535583496, acc: 0.4666000008583069, val_loss: 1.2664297819137573, val_acc: 0.45820000767707825,  lr: 0.00987460827586207
step: 1365, train_loss: 1.2655513286590576, acc: 0.4634000062942505, val_loss: 1.2741833925247192, val_acc: 0.45159998536109924,  lr: 0.009874263793103449
step: 1366, train_loss: 1.2069587707519531, acc: 0.4909999966621399, val_loss: 1.2631292343139648, val_acc: 0.4602000117301941,  lr: 0.009873919310344828
step: 1367, train_loss: 1.1862940788269043, acc: 0.4975999891757965, val_loss: 1.2669804096221924, val_acc: 0.45339998602867126,  lr: 0.009873574827586207
step: 1368, train_loss: 1.239027500152588, acc: 0.4745999872684479, val_loss: 1.2589120864868164, val_acc: 0.4586000144481659,  lr: 0.009873230344827587
step: 1369, train_loss: 1.2542436122894287, acc: 0.45980000495910645, val_loss: 1.2681950330734253, val_acc: 0.4514000117778778,  lr: 0.009872885862068966
step: 1370, train_loss: 1.2482842206954956, acc: 0.46799999475479126, val_loss: 1.2663967609405518, val_acc: 0.453000009059906,  lr: 0.009872541379310346
step: 1371, train_loss: 1.2472862005233765, acc: 0.4668000042438507, val_loss: 1.2696319818496704, val_acc: 0.451200008392334,  lr: 0.009872196896551725
step: 1372, train_loss: 1.2607327699661255, acc: 0.46299999952316284, val_loss: 1.276680827140808, val_acc: 0.44920000433921814,  lr: 0.009871852413793104
step: 1373, train_loss: 1.2193665504455566, acc: 0.4781999886035919, val_loss: 1.2865902185440063, val_acc: 0.4449999928474426,  lr: 0.009871507931034484
step: 1374, train_loss: 1.3101022243499756, acc: 0.4359999895095825, val_loss: 1.2854255437850952, val_acc: 0.44519999623298645,  lr: 0.009871163448275863
step: 1375, train_loss: 1.227921485900879, acc: 0.47679999470710754, val_loss: 1.2793433666229248, val_acc: 0.4519999921321869,  lr: 0.00987081896551724
step: 1376, train_loss: 1.303507924079895, acc: 0.4397999942302704, val_loss: 1.287505030632019, val_acc: 0.44940000772476196,  lr: 0.00987047448275862
step: 1377, train_loss: 1.2587354183197021, acc: 0.46219998598098755, val_loss: 1.2908514738082886, val_acc: 0.4458000063896179,  lr: 0.00987013
step: 1378, train_loss: 1.2308599948883057, acc: 0.4763999879360199, val_loss: 1.2868595123291016, val_acc: 0.44839999079704285,  lr: 0.009869785517241379
step: 1379, train_loss: 1.213053584098816, acc: 0.4787999987602234, val_loss: 1.2975674867630005, val_acc: 0.43880000710487366,  lr: 0.009869441034482758
step: 1380, train_loss: 1.2713786363601685, acc: 0.4618000090122223, val_loss: 1.3006576299667358, val_acc: 0.4413999915122986,  lr: 0.009869096551724138
step: 1381, train_loss: 1.329558253288269, acc: 0.4318000078201294, val_loss: 1.2927907705307007, val_acc: 0.4447999894618988,  lr: 0.009868752068965517
step: 1382, train_loss: 1.3244223594665527, acc: 0.43459999561309814, val_loss: 1.2851626873016357, val_acc: 0.44440001249313354,  lr: 0.009868407586206896
step: 1383, train_loss: 1.246913194656372, acc: 0.4690000116825104, val_loss: 1.2753454446792603, val_acc: 0.4562000036239624,  lr: 0.009868063103448276
step: 1384, train_loss: 1.2575896978378296, acc: 0.4625999927520752, val_loss: 1.2836980819702148, val_acc: 0.45820000767707825,  lr: 0.009867718620689655
step: 1385, train_loss: 1.2585479021072388, acc: 0.4596000015735626, val_loss: 1.2734904289245605, val_acc: 0.4580000042915344,  lr: 0.009867374137931035
step: 1386, train_loss: 1.314963459968567, acc: 0.4318000078201294, val_loss: 1.2672569751739502, val_acc: 0.4641999900341034,  lr: 0.009867029655172414
step: 1387, train_loss: 1.2257137298583984, acc: 0.47859999537467957, val_loss: 1.2675213813781738, val_acc: 0.4659999907016754,  lr: 0.009866685172413793
step: 1388, train_loss: 1.3127930164337158, acc: 0.42980000376701355, val_loss: 1.257428526878357, val_acc: 0.4684000015258789,  lr: 0.009866340689655173
step: 1389, train_loss: 1.2874456644058228, acc: 0.44040000438690186, val_loss: 1.2602548599243164, val_acc: 0.4681999981403351,  lr: 0.009865996206896552
step: 1390, train_loss: 1.276869535446167, acc: 0.4537999927997589, val_loss: 1.2577433586120605, val_acc: 0.4681999981403351,  lr: 0.009865651724137931
step: 1391, train_loss: 1.2430152893066406, acc: 0.4731999933719635, val_loss: 1.2636357545852661, val_acc: 0.4684000015258789,  lr: 0.009865307241379311
step: 1392, train_loss: 1.267897605895996, acc: 0.4560000002384186, val_loss: 1.2600936889648438, val_acc: 0.4702000021934509,  lr: 0.00986496275862069
step: 1393, train_loss: 1.2417476177215576, acc: 0.4713999927043915, val_loss: 1.2655788660049438, val_acc: 0.46639999747276306,  lr: 0.00986461827586207
step: 1394, train_loss: 1.2467361688613892, acc: 0.4668000042438507, val_loss: 1.2574493885040283, val_acc: 0.4657999873161316,  lr: 0.009864273793103449
step: 1395, train_loss: 1.2598462104797363, acc: 0.46619999408721924, val_loss: 1.2662749290466309, val_acc: 0.4713999927043915,  lr: 0.009863929310344828
step: 1396, train_loss: 1.2646024227142334, acc: 0.4717999994754791, val_loss: 1.2560831308364868, val_acc: 0.47040000557899475,  lr: 0.009863584827586208
step: 1397, train_loss: 1.2360965013504028, acc: 0.4812000095844269, val_loss: 1.261309027671814, val_acc: 0.4625999927520752,  lr: 0.009863240344827587
step: 1398, train_loss: 1.2386914491653442, acc: 0.47600001096725464, val_loss: 1.2695242166519165, val_acc: 0.4603999853134155,  lr: 0.009862895862068967
step: 1399, train_loss: 1.2549233436584473, acc: 0.46939998865127563, val_loss: 1.2647078037261963, val_acc: 0.4603999853134155,  lr: 0.009862551379310344
step: 1400, train_loss: 1.2991145849227905, acc: 0.44440001249313354, val_loss: 1.270019292831421, val_acc: 0.45739999413490295,  lr: 0.009862206896551724
step: 1401, train_loss: 1.2379132509231567, acc: 0.477400004863739, val_loss: 1.2648314237594604, val_acc: 0.4564000070095062,  lr: 0.009861862413793103
step: 1402, train_loss: 1.2128046751022339, acc: 0.4880000054836273, val_loss: 1.2613857984542847, val_acc: 0.4607999920845032,  lr: 0.009861517931034482
step: 1403, train_loss: 1.2402143478393555, acc: 0.4690000116825104, val_loss: 1.262203574180603, val_acc: 0.46239998936653137,  lr: 0.009861173448275862
step: 1404, train_loss: 1.220139980316162, acc: 0.47699999809265137, val_loss: 1.2547709941864014, val_acc: 0.4652000069618225,  lr: 0.009860828965517241
step: 1405, train_loss: 1.238036870956421, acc: 0.4731999933719635, val_loss: 1.2666879892349243, val_acc: 0.45840001106262207,  lr: 0.00986048448275862
step: 1406, train_loss: 1.2040153741836548, acc: 0.4957999885082245, val_loss: 1.2647379636764526, val_acc: 0.4593999981880188,  lr: 0.00986014
step: 1407, train_loss: 1.275209665298462, acc: 0.4625999927520752, val_loss: 1.270796537399292, val_acc: 0.4528000056743622,  lr: 0.00985979551724138
step: 1408, train_loss: 1.2537484169006348, acc: 0.45980000495910645, val_loss: 1.2717299461364746, val_acc: 0.4596000015735626,  lr: 0.009859451034482759
step: 1409, train_loss: 1.2335002422332764, acc: 0.47999998927116394, val_loss: 1.2713954448699951, val_acc: 0.4620000123977661,  lr: 0.009859106551724138
step: 1410, train_loss: 1.2292993068695068, acc: 0.4828000068664551, val_loss: 1.2675734758377075, val_acc: 0.45239999890327454,  lr: 0.009858762068965517
step: 1411, train_loss: 1.2052397727966309, acc: 0.4896000027656555, val_loss: 1.2720513343811035, val_acc: 0.4498000144958496,  lr: 0.009858417586206897
step: 1412, train_loss: 1.223968505859375, acc: 0.4731999933719635, val_loss: 1.2740460634231567, val_acc: 0.46140000224113464,  lr: 0.009858073103448276
step: 1413, train_loss: 1.2189289331436157, acc: 0.48559999465942383, val_loss: 1.271742343902588, val_acc: 0.4578000009059906,  lr: 0.009857728620689656
step: 1414, train_loss: 1.209670066833496, acc: 0.49239999055862427, val_loss: 1.2807351350784302, val_acc: 0.45239999890327454,  lr: 0.009857384137931035
step: 1415, train_loss: 1.206146240234375, acc: 0.48019999265670776, val_loss: 1.2839261293411255, val_acc: 0.4551999866962433,  lr: 0.009857039655172414
step: 1416, train_loss: 1.278520107269287, acc: 0.45399999618530273, val_loss: 1.2884798049926758, val_acc: 0.4519999921321869,  lr: 0.009856695172413794
step: 1417, train_loss: 1.3054395914077759, acc: 0.45239999890327454, val_loss: 1.2789483070373535, val_acc: 0.45680001378059387,  lr: 0.009856350689655173
step: 1418, train_loss: 1.3161773681640625, acc: 0.4350000023841858, val_loss: 1.276162028312683, val_acc: 0.4553999900817871,  lr: 0.009856006206896552
step: 1419, train_loss: 1.2447052001953125, acc: 0.4887999892234802, val_loss: 1.2704418897628784, val_acc: 0.46000000834465027,  lr: 0.009855661724137932
step: 1420, train_loss: 1.3241652250289917, acc: 0.42480000853538513, val_loss: 1.2646870613098145, val_acc: 0.4618000090122223,  lr: 0.009855317241379311
step: 1421, train_loss: 1.2510794401168823, acc: 0.4702000021934509, val_loss: 1.257919430732727, val_acc: 0.4674000144004822,  lr: 0.00985497275862069
step: 1422, train_loss: 1.2415783405303955, acc: 0.4733999967575073, val_loss: 1.2527568340301514, val_acc: 0.4722000062465668,  lr: 0.00985462827586207
step: 1423, train_loss: 1.2196115255355835, acc: 0.47839999198913574, val_loss: 1.254525065422058, val_acc: 0.4650000035762787,  lr: 0.009854283793103448
step: 1424, train_loss: 1.2158994674682617, acc: 0.48159998655319214, val_loss: 1.2498117685317993, val_acc: 0.4745999872684479,  lr: 0.009853939310344827
step: 1425, train_loss: 1.227772831916809, acc: 0.47940000891685486, val_loss: 1.247988224029541, val_acc: 0.47099998593330383,  lr: 0.009853594827586206
step: 1426, train_loss: 1.2083635330200195, acc: 0.49059998989105225, val_loss: 1.2522426843643188, val_acc: 0.4666000008583069,  lr: 0.009853250344827586
step: 1427, train_loss: 1.2170506715774536, acc: 0.47360000014305115, val_loss: 1.2548209428787231, val_acc: 0.460999995470047,  lr: 0.009852905862068965
step: 1428, train_loss: 1.298463225364685, acc: 0.4528000056743622, val_loss: 1.2487318515777588, val_acc: 0.4643999934196472,  lr: 0.009852561379310345
step: 1429, train_loss: 1.2608397006988525, acc: 0.4528000056743622, val_loss: 1.2459290027618408, val_acc: 0.46720001101493835,  lr: 0.009852216896551724
step: 1430, train_loss: 1.288849949836731, acc: 0.4406000077724457, val_loss: 1.2463759183883667, val_acc: 0.4708000123500824,  lr: 0.009851872413793103
step: 1431, train_loss: 1.1991047859191895, acc: 0.4934000074863434, val_loss: 1.2449984550476074, val_acc: 0.4675999879837036,  lr: 0.009851527931034483
step: 1432, train_loss: 1.2912993431091309, acc: 0.4503999948501587, val_loss: 1.2412322759628296, val_acc: 0.4684000015258789,  lr: 0.009851183448275862
step: 1433, train_loss: 1.266435980796814, acc: 0.4562000036239624, val_loss: 1.2554138898849487, val_acc: 0.4620000123977661,  lr: 0.009850838965517241
step: 1434, train_loss: 1.2547935247421265, acc: 0.46639999747276306, val_loss: 1.2545180320739746, val_acc: 0.4675999879837036,  lr: 0.00985049448275862
step: 1435, train_loss: 1.3150237798690796, acc: 0.42980000376701355, val_loss: 1.2636319398880005, val_acc: 0.4731999933719635,  lr: 0.00985015
step: 1436, train_loss: 1.2768974304199219, acc: 0.45179998874664307, val_loss: 1.2601709365844727, val_acc: 0.46619999408721924,  lr: 0.00984980551724138
step: 1437, train_loss: 1.213561773300171, acc: 0.48500001430511475, val_loss: 1.2789671421051025, val_acc: 0.45879998803138733,  lr: 0.009849461034482759
step: 1438, train_loss: 1.2402288913726807, acc: 0.46959999203681946, val_loss: 1.2588796615600586, val_acc: 0.46459999680519104,  lr: 0.009849116551724138
step: 1439, train_loss: 1.2938703298568726, acc: 0.44359999895095825, val_loss: 1.2591676712036133, val_acc: 0.4706000089645386,  lr: 0.009848772068965518
step: 1440, train_loss: 1.2377244234085083, acc: 0.4674000144004822, val_loss: 1.2446438074111938, val_acc: 0.4722000062465668,  lr: 0.009848427586206897
step: 1441, train_loss: 1.2189267873764038, acc: 0.47279998660087585, val_loss: 1.258228063583374, val_acc: 0.46299999952316284,  lr: 0.009848083103448277
step: 1442, train_loss: 1.25715970993042, acc: 0.4666000008583069, val_loss: 1.2491443157196045, val_acc: 0.4699999988079071,  lr: 0.009847738620689656
step: 1443, train_loss: 1.2353415489196777, acc: 0.4869999885559082, val_loss: 1.243111491203308, val_acc: 0.47999998927116394,  lr: 0.009847394137931035
step: 1444, train_loss: 1.272525668144226, acc: 0.4474000036716461, val_loss: 1.23940908908844, val_acc: 0.47760000824928284,  lr: 0.009847049655172415
step: 1445, train_loss: 1.2262591123580933, acc: 0.47380000352859497, val_loss: 1.2429267168045044, val_acc: 0.47119998931884766,  lr: 0.009846705172413794
step: 1446, train_loss: 1.247990608215332, acc: 0.477400004863739, val_loss: 1.243855595588684, val_acc: 0.47679999470710754,  lr: 0.009846360689655173
step: 1447, train_loss: 1.2514667510986328, acc: 0.4690000116825104, val_loss: 1.2455626726150513, val_acc: 0.4706000089645386,  lr: 0.009846016206896551
step: 1448, train_loss: 1.2630860805511475, acc: 0.45899999141693115, val_loss: 1.2457473278045654, val_acc: 0.4699999988079071,  lr: 0.00984567172413793
step: 1449, train_loss: 1.2920960187911987, acc: 0.4505999982357025, val_loss: 1.2473359107971191, val_acc: 0.4713999927043915,  lr: 0.00984532724137931
step: 1450, train_loss: 1.2862801551818848, acc: 0.4413999915122986, val_loss: 1.2472902536392212, val_acc: 0.46939998865127563,  lr: 0.00984498275862069
step: 1451, train_loss: 1.2396308183670044, acc: 0.4659999907016754, val_loss: 1.2515978813171387, val_acc: 0.47380000352859497,  lr: 0.009844638275862069
step: 1452, train_loss: 1.2329236268997192, acc: 0.4819999933242798, val_loss: 1.2401585578918457, val_acc: 0.475600004196167,  lr: 0.009844293793103448
step: 1453, train_loss: 1.2358953952789307, acc: 0.477400004863739, val_loss: 1.2561906576156616, val_acc: 0.46320000290870667,  lr: 0.009843949310344827
step: 1454, train_loss: 1.2730337381362915, acc: 0.46140000224113464, val_loss: 1.247887372970581, val_acc: 0.4699999988079071,  lr: 0.009843604827586207
step: 1455, train_loss: 1.2891600131988525, acc: 0.45399999618530273, val_loss: 1.247406005859375, val_acc: 0.47679999470710754,  lr: 0.009843260344827586
step: 1456, train_loss: 1.2749017477035522, acc: 0.4575999975204468, val_loss: 1.2571778297424316, val_acc: 0.4733999967575073,  lr: 0.009842915862068966
step: 1457, train_loss: 1.218186855316162, acc: 0.4844000041484833, val_loss: 1.257858157157898, val_acc: 0.4724000096321106,  lr: 0.009842571379310345
step: 1458, train_loss: 1.25730562210083, acc: 0.4674000144004822, val_loss: 1.2545498609542847, val_acc: 0.4643999934196472,  lr: 0.009842226896551724
step: 1459, train_loss: 1.2467113733291626, acc: 0.46860000491142273, val_loss: 1.2561452388763428, val_acc: 0.4731999933719635,  lr: 0.009841882413793104
step: 1460, train_loss: 1.242287278175354, acc: 0.4749999940395355, val_loss: 1.2497423887252808, val_acc: 0.4740000069141388,  lr: 0.009841537931034483
step: 1461, train_loss: 1.2473170757293701, acc: 0.47200000286102295, val_loss: 1.253255844116211, val_acc: 0.4636000096797943,  lr: 0.009841193448275862
step: 1462, train_loss: 1.251617431640625, acc: 0.4684000015258789, val_loss: 1.2471909523010254, val_acc: 0.4681999981403351,  lr: 0.009840848965517242
step: 1463, train_loss: 1.2477258443832397, acc: 0.47540000081062317, val_loss: 1.2529981136322021, val_acc: 0.46720001101493835,  lr: 0.009840504482758621
step: 1464, train_loss: 1.2049282789230347, acc: 0.49380001425743103, val_loss: 1.2542476654052734, val_acc: 0.4717999994754791,  lr: 0.00984016
step: 1465, train_loss: 1.207409143447876, acc: 0.48980000615119934, val_loss: 1.2563130855560303, val_acc: 0.4602000117301941,  lr: 0.00983981551724138
step: 1466, train_loss: 1.2980581521987915, acc: 0.4442000091075897, val_loss: 1.2590861320495605, val_acc: 0.45840001106262207,  lr: 0.00983947103448276
step: 1467, train_loss: 1.2284283638000488, acc: 0.48159998655319214, val_loss: 1.2482887506484985, val_acc: 0.460999995470047,  lr: 0.009839126551724139
step: 1468, train_loss: 1.2235702276229858, acc: 0.4797999858856201, val_loss: 1.251092791557312, val_acc: 0.4580000042915344,  lr: 0.009838782068965518
step: 1469, train_loss: 1.2264404296875, acc: 0.47999998927116394, val_loss: 1.2515642642974854, val_acc: 0.4636000096797943,  lr: 0.009838437586206897
step: 1470, train_loss: 1.2161439657211304, acc: 0.4837999939918518, val_loss: 1.2502340078353882, val_acc: 0.4625999927520752,  lr: 0.009838093103448277
step: 1471, train_loss: 1.288266658782959, acc: 0.45559999346733093, val_loss: 1.2480164766311646, val_acc: 0.45899999141693115,  lr: 0.009837748620689656
step: 1472, train_loss: 1.3005858659744263, acc: 0.44440001249313354, val_loss: 1.24783194065094, val_acc: 0.46880000829696655,  lr: 0.009837404137931034
step: 1473, train_loss: 1.2488746643066406, acc: 0.4708000123500824, val_loss: 1.2510969638824463, val_acc: 0.4733999967575073,  lr: 0.009837059655172413
step: 1474, train_loss: 1.2673059701919556, acc: 0.4553999900817871, val_loss: 1.2425915002822876, val_acc: 0.4772000014781952,  lr: 0.009836715172413793
step: 1475, train_loss: 1.2015472650527954, acc: 0.49079999327659607, val_loss: 1.2416883707046509, val_acc: 0.47360000014305115,  lr: 0.009836370689655172
step: 1476, train_loss: 1.293927788734436, acc: 0.43959999084472656, val_loss: 1.2407623529434204, val_acc: 0.475600004196167,  lr: 0.009836026206896551
step: 1477, train_loss: 1.1861169338226318, acc: 0.5019999742507935, val_loss: 1.2437256574630737, val_acc: 0.4717999994754791,  lr: 0.00983568172413793
step: 1478, train_loss: 1.3038277626037598, acc: 0.4334000051021576, val_loss: 1.2474395036697388, val_acc: 0.4724000096321106,  lr: 0.00983533724137931
step: 1479, train_loss: 1.2166801691055298, acc: 0.48159998655319214, val_loss: 1.2492440938949585, val_acc: 0.47119998931884766,  lr: 0.00983499275862069
step: 1480, train_loss: 1.1866376399993896, acc: 0.4957999885082245, val_loss: 1.2430347204208374, val_acc: 0.47279998660087585,  lr: 0.009834648275862069
step: 1481, train_loss: 1.252251386642456, acc: 0.4519999921321869, val_loss: 1.2469960451126099, val_acc: 0.47279998660087585,  lr: 0.009834303793103448
step: 1482, train_loss: 1.2704108953475952, acc: 0.454800009727478, val_loss: 1.246265172958374, val_acc: 0.4758000075817108,  lr: 0.009833959310344828
step: 1483, train_loss: 1.2062132358551025, acc: 0.4925999939441681, val_loss: 1.24919855594635, val_acc: 0.46939998865127563,  lr: 0.009833614827586207
step: 1484, train_loss: 1.2477446794509888, acc: 0.4729999899864197, val_loss: 1.245361089706421, val_acc: 0.4697999954223633,  lr: 0.009833270344827586
step: 1485, train_loss: 1.2167243957519531, acc: 0.48539999127388, val_loss: 1.2453593015670776, val_acc: 0.46779999136924744,  lr: 0.009832925862068966
step: 1486, train_loss: 1.2427608966827393, acc: 0.4740000069141388, val_loss: 1.2532432079315186, val_acc: 0.46639999747276306,  lr: 0.009832581379310345
step: 1487, train_loss: 1.2079384326934814, acc: 0.48159998655319214, val_loss: 1.2550289630889893, val_acc: 0.4666000008583069,  lr: 0.009832236896551725
step: 1488, train_loss: 1.269378662109375, acc: 0.4643999934196472, val_loss: 1.2516686916351318, val_acc: 0.47519999742507935,  lr: 0.009831892413793104
step: 1489, train_loss: 1.248618245124817, acc: 0.46860000491142273, val_loss: 1.2507388591766357, val_acc: 0.47040000557899475,  lr: 0.009831547931034483
step: 1490, train_loss: 1.2365138530731201, acc: 0.4724000096321106, val_loss: 1.259152889251709, val_acc: 0.46619999408721924,  lr: 0.009831203448275863
step: 1491, train_loss: 1.2591354846954346, acc: 0.4681999981403351, val_loss: 1.2589765787124634, val_acc: 0.46619999408721924,  lr: 0.009830858965517242
step: 1492, train_loss: 1.2840675115585327, acc: 0.4490000009536743, val_loss: 1.2626339197158813, val_acc: 0.4607999920845032,  lr: 0.009830514482758622
step: 1493, train_loss: 1.318190097808838, acc: 0.43959999084472656, val_loss: 1.2644342184066772, val_acc: 0.4634000062942505,  lr: 0.009830170000000001
step: 1494, train_loss: 1.2206006050109863, acc: 0.4973999857902527, val_loss: 1.2641974687576294, val_acc: 0.45719999074935913,  lr: 0.00982982551724138
step: 1495, train_loss: 1.2787667512893677, acc: 0.45080000162124634, val_loss: 1.2697865962982178, val_acc: 0.45559999346733093,  lr: 0.009829481034482758
step: 1496, train_loss: 1.244231939315796, acc: 0.46459999680519104, val_loss: 1.2734792232513428, val_acc: 0.4569999873638153,  lr: 0.009829136551724137
step: 1497, train_loss: 1.282924771308899, acc: 0.448199987411499, val_loss: 1.272226333618164, val_acc: 0.45500001311302185,  lr: 0.009828792068965517
step: 1498, train_loss: 1.2082784175872803, acc: 0.4925999939441681, val_loss: 1.2694153785705566, val_acc: 0.45899999141693115,  lr: 0.009828447586206896
step: 1499, train_loss: 1.2503772974014282, acc: 0.4702000021934509, val_loss: 1.2861605882644653, val_acc: 0.4465999901294708,  lr: 0.009828103103448275
step: 1500, train_loss: 1.3055649995803833, acc: 0.44200000166893005, val_loss: 1.2815349102020264, val_acc: 0.44999998807907104,  lr: 0.009827758620689655
step: 1501, train_loss: 1.2129411697387695, acc: 0.48820000886917114, val_loss: 1.2847224473953247, val_acc: 0.4487999975681305,  lr: 0.009827414137931034
step: 1502, train_loss: 1.22127103805542, acc: 0.48240000009536743, val_loss: 1.295969009399414, val_acc: 0.4514000117778778,  lr: 0.009827069655172414
step: 1503, train_loss: 1.3229728937149048, acc: 0.4259999990463257, val_loss: 1.2763490676879883, val_acc: 0.45320001244544983,  lr: 0.009826725172413793
step: 1504, train_loss: 1.2412298917770386, acc: 0.45980000495910645, val_loss: 1.2805922031402588, val_acc: 0.44859999418258667,  lr: 0.009826380689655172
step: 1505, train_loss: 1.2149101495742798, acc: 0.48660001158714294, val_loss: 1.2724344730377197, val_acc: 0.4514000117778778,  lr: 0.009826036206896552
step: 1506, train_loss: 1.2096190452575684, acc: 0.4821999967098236, val_loss: 1.2797585725784302, val_acc: 0.44839999079704285,  lr: 0.009825691724137931
step: 1507, train_loss: 1.3140146732330322, acc: 0.44519999623298645, val_loss: 1.2678030729293823, val_acc: 0.45260000228881836,  lr: 0.00982534724137931
step: 1508, train_loss: 1.234026551246643, acc: 0.47040000557899475, val_loss: 1.2685192823410034, val_acc: 0.45820000767707825,  lr: 0.00982500275862069
step: 1509, train_loss: 1.2432969808578491, acc: 0.47519999742507935, val_loss: 1.2600635290145874, val_acc: 0.4636000096797943,  lr: 0.00982465827586207
step: 1510, train_loss: 1.3073430061340332, acc: 0.44040000438690186, val_loss: 1.2601752281188965, val_acc: 0.4657999873161316,  lr: 0.009824313793103449
step: 1511, train_loss: 1.2186665534973145, acc: 0.48500001430511475, val_loss: 1.2581932544708252, val_acc: 0.46639999747276306,  lr: 0.009823969310344828
step: 1512, train_loss: 1.2589318752288818, acc: 0.46619999408721924, val_loss: 1.2539438009262085, val_acc: 0.46779999136924744,  lr: 0.009823624827586207
step: 1513, train_loss: 1.2144074440002441, acc: 0.48399999737739563, val_loss: 1.2578648328781128, val_acc: 0.46299999952316284,  lr: 0.009823280344827587
step: 1514, train_loss: 1.2353371381759644, acc: 0.4636000096797943, val_loss: 1.2548435926437378, val_acc: 0.4681999981403351,  lr: 0.009822935862068966
step: 1515, train_loss: 1.1967527866363525, acc: 0.4878000020980835, val_loss: 1.2520076036453247, val_acc: 0.46299999952316284,  lr: 0.009822591379310346
step: 1516, train_loss: 1.2041730880737305, acc: 0.48539999127388, val_loss: 1.2582741975784302, val_acc: 0.460999995470047,  lr: 0.009822246896551725
step: 1517, train_loss: 1.2879947423934937, acc: 0.4453999996185303, val_loss: 1.2500641345977783, val_acc: 0.4674000144004822,  lr: 0.009821902413793104
step: 1518, train_loss: 1.2211579084396362, acc: 0.4832000136375427, val_loss: 1.247909426689148, val_acc: 0.4717999994754791,  lr: 0.009821557931034484
step: 1519, train_loss: 1.2617461681365967, acc: 0.45500001311302185, val_loss: 1.24885892868042, val_acc: 0.47620001435279846,  lr: 0.009821213448275863
step: 1520, train_loss: 1.198105812072754, acc: 0.4970000088214874, val_loss: 1.248858094215393, val_acc: 0.47099998593330383,  lr: 0.00982086896551724
step: 1521, train_loss: 1.2218297719955444, acc: 0.4880000054836273, val_loss: 1.2503254413604736, val_acc: 0.4726000130176544,  lr: 0.00982052448275862
step: 1522, train_loss: 1.2042348384857178, acc: 0.4887999892234802, val_loss: 1.262200117111206, val_acc: 0.46860000491142273,  lr: 0.00982018
step: 1523, train_loss: 1.2293800115585327, acc: 0.4749999940395355, val_loss: 1.2588478326797485, val_acc: 0.46540001034736633,  lr: 0.009819835517241379
step: 1524, train_loss: 1.1873959302902222, acc: 0.5044000148773193, val_loss: 1.2664856910705566, val_acc: 0.462799996137619,  lr: 0.009819491034482758
step: 1525, train_loss: 1.193153977394104, acc: 0.492000013589859, val_loss: 1.2632631063461304, val_acc: 0.4636000096797943,  lr: 0.009819146551724138
step: 1526, train_loss: 1.3096528053283691, acc: 0.4424000084400177, val_loss: 1.2692244052886963, val_acc: 0.4607999920845032,  lr: 0.009818802068965517
step: 1527, train_loss: 1.2656663656234741, acc: 0.46000000834465027, val_loss: 1.2712488174438477, val_acc: 0.46160000562667847,  lr: 0.009818457586206896
step: 1528, train_loss: 1.2002307176589966, acc: 0.49059998989105225, val_loss: 1.2770112752914429, val_acc: 0.46059998869895935,  lr: 0.009818113103448276
step: 1529, train_loss: 1.3127611875534058, acc: 0.43560001254081726, val_loss: 1.2658648490905762, val_acc: 0.45579999685287476,  lr: 0.009817768620689655
step: 1530, train_loss: 1.1984195709228516, acc: 0.487199991941452, val_loss: 1.2478395700454712, val_acc: 0.46939998865127563,  lr: 0.009817424137931035
step: 1531, train_loss: 1.1925055980682373, acc: 0.49779999256134033, val_loss: 1.2545356750488281, val_acc: 0.46380001306533813,  lr: 0.009817079655172414
step: 1532, train_loss: 1.2077289819717407, acc: 0.4814000129699707, val_loss: 1.255947470664978, val_acc: 0.46480000019073486,  lr: 0.009816735172413793
step: 1533, train_loss: 1.257723093032837, acc: 0.4596000015735626, val_loss: 1.2558659315109253, val_acc: 0.46000000834465027,  lr: 0.009816390689655173
step: 1534, train_loss: 1.2566988468170166, acc: 0.46219998598098755, val_loss: 1.2547699213027954, val_acc: 0.4681999981403351,  lr: 0.009816046206896552
step: 1535, train_loss: 1.2602906227111816, acc: 0.46160000562667847, val_loss: 1.254583716392517, val_acc: 0.46779999136924744,  lr: 0.009815701724137932
step: 1536, train_loss: 1.2372418642044067, acc: 0.4796000123023987, val_loss: 1.2575815916061401, val_acc: 0.47099998593330383,  lr: 0.009815357241379311
step: 1537, train_loss: 1.258454442024231, acc: 0.462799996137619, val_loss: 1.2569985389709473, val_acc: 0.4715999960899353,  lr: 0.00981501275862069
step: 1538, train_loss: 1.1965539455413818, acc: 0.4918000102043152, val_loss: 1.2451794147491455, val_acc: 0.4747999906539917,  lr: 0.00981466827586207
step: 1539, train_loss: 1.250188946723938, acc: 0.4593999981880188, val_loss: 1.2451415061950684, val_acc: 0.4733999967575073,  lr: 0.009814323793103449
step: 1540, train_loss: 1.2327460050582886, acc: 0.47119998931884766, val_loss: 1.2450499534606934, val_acc: 0.47099998593330383,  lr: 0.009813979310344828
step: 1541, train_loss: 1.2025727033615112, acc: 0.49619999527931213, val_loss: 1.2474061250686646, val_acc: 0.4715999960899353,  lr: 0.009813634827586208
step: 1542, train_loss: 1.250171184539795, acc: 0.46779999136924744, val_loss: 1.243010401725769, val_acc: 0.4702000021934509,  lr: 0.009813290344827587
step: 1543, train_loss: 1.2144224643707275, acc: 0.4803999960422516, val_loss: 1.234283447265625, val_acc: 0.47519999742507935,  lr: 0.009812945862068967
step: 1544, train_loss: 1.240628719329834, acc: 0.46459999680519104, val_loss: 1.2357358932495117, val_acc: 0.4708000123500824,  lr: 0.009812601379310344
step: 1545, train_loss: 1.2511639595031738, acc: 0.4803999960422516, val_loss: 1.239069938659668, val_acc: 0.47920000553131104,  lr: 0.009812256896551724
step: 1546, train_loss: 1.1748772859573364, acc: 0.5081999897956848, val_loss: 1.2416365146636963, val_acc: 0.4765999913215637,  lr: 0.009811912413793103
step: 1547, train_loss: 1.2106685638427734, acc: 0.49000000953674316, val_loss: 1.242742896080017, val_acc: 0.4742000102996826,  lr: 0.009811567931034482
step: 1548, train_loss: 1.2570359706878662, acc: 0.4575999975204468, val_loss: 1.2409027814865112, val_acc: 0.47920000553131104,  lr: 0.009811223448275862
step: 1549, train_loss: 1.2713381052017212, acc: 0.45399999618530273, val_loss: 1.2477667331695557, val_acc: 0.4706000089645386,  lr: 0.009810878965517241
step: 1550, train_loss: 1.2028602361679077, acc: 0.49239999055862427, val_loss: 1.2433357238769531, val_acc: 0.4715999960899353,  lr: 0.00981053448275862
step: 1551, train_loss: 1.2677031755447388, acc: 0.45820000767707825, val_loss: 1.2433780431747437, val_acc: 0.4747999906539917,  lr: 0.00981019
step: 1552, train_loss: 1.226154088973999, acc: 0.4828000068664551, val_loss: 1.2494292259216309, val_acc: 0.4699999988079071,  lr: 0.00980984551724138
step: 1553, train_loss: 1.2888718843460083, acc: 0.4424000084400177, val_loss: 1.251252293586731, val_acc: 0.4691999852657318,  lr: 0.009809501034482759
step: 1554, train_loss: 1.2623854875564575, acc: 0.45739999413490295, val_loss: 1.2506786584854126, val_acc: 0.4641999900341034,  lr: 0.009809156551724138
step: 1555, train_loss: 1.2328429222106934, acc: 0.4765999913215637, val_loss: 1.2520816326141357, val_acc: 0.46540001034736633,  lr: 0.009808812068965517
step: 1556, train_loss: 1.2585409879684448, acc: 0.4643999934196472, val_loss: 1.2551472187042236, val_acc: 0.46619999408721924,  lr: 0.009808467586206897
step: 1557, train_loss: 1.2337335348129272, acc: 0.4726000130176544, val_loss: 1.2451003789901733, val_acc: 0.4747999906539917,  lr: 0.009808123103448276
step: 1558, train_loss: 1.206351399421692, acc: 0.48820000886917114, val_loss: 1.2480828762054443, val_acc: 0.4772000014781952,  lr: 0.009807778620689656
step: 1559, train_loss: 1.1830683946609497, acc: 0.4941999912261963, val_loss: 1.2462937831878662, val_acc: 0.47620001435279846,  lr: 0.009807434137931035
step: 1560, train_loss: 1.1579054594039917, acc: 0.5121999979019165, val_loss: 1.244157075881958, val_acc: 0.47920000553131104,  lr: 0.009807089655172414
step: 1561, train_loss: 1.2391608953475952, acc: 0.4731999933719635, val_loss: 1.2499877214431763, val_acc: 0.47200000286102295,  lr: 0.009806745172413794
step: 1562, train_loss: 1.2272855043411255, acc: 0.48399999737739563, val_loss: 1.2383249998092651, val_acc: 0.4708000123500824,  lr: 0.009806400689655173
step: 1563, train_loss: 1.229980707168579, acc: 0.47780001163482666, val_loss: 1.2453134059906006, val_acc: 0.4641999900341034,  lr: 0.009806056206896552
step: 1564, train_loss: 1.2205772399902344, acc: 0.4821999967098236, val_loss: 1.2426942586898804, val_acc: 0.4745999872684479,  lr: 0.009805711724137932
step: 1565, train_loss: 1.194528579711914, acc: 0.5004000067710876, val_loss: 1.2460156679153442, val_acc: 0.4733999967575073,  lr: 0.009805367241379311
step: 1566, train_loss: 1.1702650785446167, acc: 0.5081999897956848, val_loss: 1.2420430183410645, val_acc: 0.47600001096725464,  lr: 0.00980502275862069
step: 1567, train_loss: 1.2244287729263306, acc: 0.48500001430511475, val_loss: 1.2452279329299927, val_acc: 0.4747999906539917,  lr: 0.00980467827586207
step: 1568, train_loss: 1.2406682968139648, acc: 0.47440001368522644, val_loss: 1.240505576133728, val_acc: 0.47620001435279846,  lr: 0.009804333793103448
step: 1569, train_loss: 1.2327136993408203, acc: 0.47600001096725464, val_loss: 1.2559744119644165, val_acc: 0.46639999747276306,  lr: 0.009803989310344827
step: 1570, train_loss: 1.2672003507614136, acc: 0.46320000290870667, val_loss: 1.2367737293243408, val_acc: 0.4733999967575073,  lr: 0.009803644827586206
step: 1571, train_loss: 1.2107446193695068, acc: 0.4880000054836273, val_loss: 1.2469360828399658, val_acc: 0.47540000081062317,  lr: 0.009803300344827586
step: 1572, train_loss: 1.3349826335906982, acc: 0.4296000003814697, val_loss: 1.2445907592773438, val_acc: 0.4740000069141388,  lr: 0.009802955862068965
step: 1573, train_loss: 1.2016475200653076, acc: 0.5, val_loss: 1.2443856000900269, val_acc: 0.4747999906539917,  lr: 0.009802611379310345
step: 1574, train_loss: 1.1987717151641846, acc: 0.49639999866485596, val_loss: 1.2445722818374634, val_acc: 0.4674000144004822,  lr: 0.009802266896551724
step: 1575, train_loss: 1.1956015825271606, acc: 0.4966000020503998, val_loss: 1.245689868927002, val_acc: 0.4715999960899353,  lr: 0.009801922413793103
step: 1576, train_loss: 1.2412196397781372, acc: 0.4706000089645386, val_loss: 1.2492420673370361, val_acc: 0.4724000096321106,  lr: 0.009801577931034483
step: 1577, train_loss: 1.2678495645523071, acc: 0.45899999141693115, val_loss: 1.250181794166565, val_acc: 0.4726000130176544,  lr: 0.009801233448275862
step: 1578, train_loss: 1.2677531242370605, acc: 0.46299999952316284, val_loss: 1.2564268112182617, val_acc: 0.462799996137619,  lr: 0.009800888965517241
step: 1579, train_loss: 1.2663500308990479, acc: 0.46140000224113464, val_loss: 1.2671873569488525, val_acc: 0.459199994802475,  lr: 0.00980054448275862
step: 1580, train_loss: 1.3127573728561401, acc: 0.44020000100135803, val_loss: 1.252774715423584, val_acc: 0.4733999967575073,  lr: 0.0098002
step: 1581, train_loss: 1.1835894584655762, acc: 0.49959999322891235, val_loss: 1.2540271282196045, val_acc: 0.4668000042438507,  lr: 0.00979985551724138
step: 1582, train_loss: 1.2620830535888672, acc: 0.4699999988079071, val_loss: 1.2482858896255493, val_acc: 0.46939998865127563,  lr: 0.009799511034482759
step: 1583, train_loss: 1.2389668226242065, acc: 0.4717999994754791, val_loss: 1.2537784576416016, val_acc: 0.4657999873161316,  lr: 0.009799166551724138
step: 1584, train_loss: 1.2908916473388672, acc: 0.46000000834465027, val_loss: 1.256099820137024, val_acc: 0.46140000224113464,  lr: 0.009798822068965518
step: 1585, train_loss: 1.1993308067321777, acc: 0.4991999864578247, val_loss: 1.2576807737350464, val_acc: 0.46219998598098755,  lr: 0.009798477586206897
step: 1586, train_loss: 1.3028696775436401, acc: 0.43639999628067017, val_loss: 1.2554162740707397, val_acc: 0.4666000008583069,  lr: 0.009798133103448277
step: 1587, train_loss: 1.250675916671753, acc: 0.47699999809265137, val_loss: 1.2537895441055298, val_acc: 0.4634000062942505,  lr: 0.009797788620689656
step: 1588, train_loss: 1.2889227867126465, acc: 0.4453999996185303, val_loss: 1.2553449869155884, val_acc: 0.46320000290870667,  lr: 0.009797444137931035
step: 1589, train_loss: 1.3045244216918945, acc: 0.44359999895095825, val_loss: 1.2590664625167847, val_acc: 0.46540001034736633,  lr: 0.009797099655172415
step: 1590, train_loss: 1.2150025367736816, acc: 0.48339998722076416, val_loss: 1.250421166419983, val_acc: 0.47040000557899475,  lr: 0.009796755172413794
step: 1591, train_loss: 1.2445368766784668, acc: 0.4708000123500824, val_loss: 1.2469700574874878, val_acc: 0.4684000015258789,  lr: 0.009796410689655173
step: 1592, train_loss: 1.2749727964401245, acc: 0.44999998807907104, val_loss: 1.2523493766784668, val_acc: 0.4666000008583069,  lr: 0.009796066206896551
step: 1593, train_loss: 1.2128735780715942, acc: 0.4893999993801117, val_loss: 1.2538846731185913, val_acc: 0.4675999879837036,  lr: 0.00979572172413793
step: 1594, train_loss: 1.2460453510284424, acc: 0.4740000069141388, val_loss: 1.2562212944030762, val_acc: 0.46540001034736633,  lr: 0.00979537724137931
step: 1595, train_loss: 1.1902353763580322, acc: 0.4943999946117401, val_loss: 1.2556393146514893, val_acc: 0.4618000090122223,  lr: 0.00979503275862069
step: 1596, train_loss: 1.2019529342651367, acc: 0.49540001153945923, val_loss: 1.2501161098480225, val_acc: 0.4634000062942505,  lr: 0.009794688275862069
step: 1597, train_loss: 1.2575984001159668, acc: 0.4560000002384186, val_loss: 1.2680023908615112, val_acc: 0.4641999900341034,  lr: 0.009794343793103448
step: 1598, train_loss: 1.2242745161056519, acc: 0.48159998655319214, val_loss: 1.2602452039718628, val_acc: 0.46880000829696655,  lr: 0.009793999310344827
step: 1599, train_loss: 1.199016809463501, acc: 0.4893999993801117, val_loss: 1.2605619430541992, val_acc: 0.46799999475479126,  lr: 0.009793654827586207
step: 1600, train_loss: 1.205140471458435, acc: 0.4887999892234802, val_loss: 1.2668259143829346, val_acc: 0.4657999873161316,  lr: 0.009793310344827586
step: 1601, train_loss: 1.1988921165466309, acc: 0.4846000075340271, val_loss: 1.2615753412246704, val_acc: 0.4618000090122223,  lr: 0.009792965862068966
step: 1602, train_loss: 1.224061131477356, acc: 0.49059998989105225, val_loss: 1.2645843029022217, val_acc: 0.4625999927520752,  lr: 0.009792621379310345
step: 1603, train_loss: 1.2648297548294067, acc: 0.4528000056743622, val_loss: 1.269238829612732, val_acc: 0.4560000002384186,  lr: 0.009792276896551724
step: 1604, train_loss: 1.1904805898666382, acc: 0.4973999857902527, val_loss: 1.2723687887191772, val_acc: 0.46480000019073486,  lr: 0.009791932413793104
step: 1605, train_loss: 1.181444525718689, acc: 0.49959999322891235, val_loss: 1.2560958862304688, val_acc: 0.4580000042915344,  lr: 0.009791587931034483
step: 1606, train_loss: 1.2049574851989746, acc: 0.4973999857902527, val_loss: 1.2631008625030518, val_acc: 0.4602000117301941,  lr: 0.009791243448275862
step: 1607, train_loss: 1.197967529296875, acc: 0.4959999918937683, val_loss: 1.257157564163208, val_acc: 0.4650000035762787,  lr: 0.009790898965517242
step: 1608, train_loss: 1.3081908226013184, acc: 0.43939998745918274, val_loss: 1.2527000904083252, val_acc: 0.46160000562667847,  lr: 0.009790554482758621
step: 1609, train_loss: 1.3089971542358398, acc: 0.4374000132083893, val_loss: 1.2648539543151855, val_acc: 0.46239998936653137,  lr: 0.00979021
step: 1610, train_loss: 1.233045220375061, acc: 0.48240000009536743, val_loss: 1.2632317543029785, val_acc: 0.4666000008583069,  lr: 0.00978986551724138
step: 1611, train_loss: 1.1856437921524048, acc: 0.503000020980835, val_loss: 1.248906135559082, val_acc: 0.46239998936653137,  lr: 0.00978952103448276
step: 1612, train_loss: 1.245848298072815, acc: 0.47279998660087585, val_loss: 1.2393553256988525, val_acc: 0.46860000491142273,  lr: 0.009789176551724139
step: 1613, train_loss: 1.2797685861587524, acc: 0.4643999934196472, val_loss: 1.2565524578094482, val_acc: 0.4659999907016754,  lr: 0.009788832068965518
step: 1614, train_loss: 1.279369831085205, acc: 0.45680001378059387, val_loss: 1.2454538345336914, val_acc: 0.4713999927043915,  lr: 0.009788487586206897
step: 1615, train_loss: 1.2742234468460083, acc: 0.4575999975204468, val_loss: 1.2417495250701904, val_acc: 0.47099998593330383,  lr: 0.009788143103448277
step: 1616, train_loss: 1.2349491119384766, acc: 0.4851999878883362, val_loss: 1.2352699041366577, val_acc: 0.4724000096321106,  lr: 0.009787798620689655
step: 1617, train_loss: 1.1836134195327759, acc: 0.5023999810218811, val_loss: 1.2513967752456665, val_acc: 0.46779999136924744,  lr: 0.009787454137931034
step: 1618, train_loss: 1.2787995338439941, acc: 0.459199994802475, val_loss: 1.2468664646148682, val_acc: 0.475600004196167,  lr: 0.009787109655172413
step: 1619, train_loss: 1.2100180387496948, acc: 0.49059998989105225, val_loss: 1.2413461208343506, val_acc: 0.47859999537467957,  lr: 0.009786765172413793
step: 1620, train_loss: 1.2462819814682007, acc: 0.4699999988079071, val_loss: 1.2443116903305054, val_acc: 0.4731999933719635,  lr: 0.009786420689655172
step: 1621, train_loss: 1.3151066303253174, acc: 0.4392000138759613, val_loss: 1.234772801399231, val_acc: 0.46480000019073486,  lr: 0.009786076206896551
step: 1622, train_loss: 1.2808653116226196, acc: 0.453000009059906, val_loss: 1.2300273180007935, val_acc: 0.4787999987602234,  lr: 0.00978573172413793
step: 1623, train_loss: 1.184057354927063, acc: 0.5091999769210815, val_loss: 1.2299110889434814, val_acc: 0.48159998655319214,  lr: 0.00978538724137931
step: 1624, train_loss: 1.2540010213851929, acc: 0.46299999952316284, val_loss: 1.2278270721435547, val_acc: 0.47679999470710754,  lr: 0.00978504275862069
step: 1625, train_loss: 1.2064257860183716, acc: 0.4927999973297119, val_loss: 1.2313650846481323, val_acc: 0.4787999987602234,  lr: 0.009784698275862069
step: 1626, train_loss: 1.1998751163482666, acc: 0.49239999055862427, val_loss: 1.2310516834259033, val_acc: 0.48260000348091125,  lr: 0.009784353793103448
step: 1627, train_loss: 1.2319269180297852, acc: 0.47360000014305115, val_loss: 1.2276959419250488, val_acc: 0.48080000281333923,  lr: 0.009784009310344828
step: 1628, train_loss: 1.2622404098510742, acc: 0.4607999920845032, val_loss: 1.2295818328857422, val_acc: 0.4787999987602234,  lr: 0.009783664827586207
step: 1629, train_loss: 1.1970365047454834, acc: 0.4927999973297119, val_loss: 1.2325183153152466, val_acc: 0.4722000062465668,  lr: 0.009783320344827586
step: 1630, train_loss: 1.2762562036514282, acc: 0.4593999981880188, val_loss: 1.241480827331543, val_acc: 0.4713999927043915,  lr: 0.009782975862068966
step: 1631, train_loss: 1.2326124906539917, acc: 0.477400004863739, val_loss: 1.2331764698028564, val_acc: 0.47999998927116394,  lr: 0.009782631379310345
step: 1632, train_loss: 1.2216756343841553, acc: 0.48660001158714294, val_loss: 1.2336078882217407, val_acc: 0.47699999809265137,  lr: 0.009782286896551725
step: 1633, train_loss: 1.2541431188583374, acc: 0.46399998664855957, val_loss: 1.2398607730865479, val_acc: 0.47279998660087585,  lr: 0.009781942413793104
step: 1634, train_loss: 1.2923966646194458, acc: 0.4474000036716461, val_loss: 1.2443169355392456, val_acc: 0.4742000102996826,  lr: 0.009781597931034483
step: 1635, train_loss: 1.2150120735168457, acc: 0.4950000047683716, val_loss: 1.234187126159668, val_acc: 0.47920000553131104,  lr: 0.009781253448275863
step: 1636, train_loss: 1.2780910730361938, acc: 0.4490000009536743, val_loss: 1.2291648387908936, val_acc: 0.47940000891685486,  lr: 0.009780908965517242
step: 1637, train_loss: 1.2439244985580444, acc: 0.46380001306533813, val_loss: 1.235408902168274, val_acc: 0.4765999913215637,  lr: 0.009780564482758622
step: 1638, train_loss: 1.1976323127746582, acc: 0.5044000148773193, val_loss: 1.2448660135269165, val_acc: 0.4729999899864197,  lr: 0.009780220000000001
step: 1639, train_loss: 1.3053169250488281, acc: 0.4413999915122986, val_loss: 1.2449144124984741, val_acc: 0.4708000123500824,  lr: 0.00977987551724138
step: 1640, train_loss: 1.2140398025512695, acc: 0.4878000020980835, val_loss: 1.2469850778579712, val_acc: 0.4668000042438507,  lr: 0.009779531034482758
step: 1641, train_loss: 1.2113797664642334, acc: 0.483599990606308, val_loss: 1.2455400228500366, val_acc: 0.46860000491142273,  lr: 0.009779186551724137
step: 1642, train_loss: 1.2222888469696045, acc: 0.4864000082015991, val_loss: 1.2493834495544434, val_acc: 0.4684000015258789,  lr: 0.009778842068965517
step: 1643, train_loss: 1.2473787069320679, acc: 0.4681999981403351, val_loss: 1.2460578680038452, val_acc: 0.4681999981403351,  lr: 0.009778497586206896
step: 1644, train_loss: 1.2632414102554321, acc: 0.46939998865127563, val_loss: 1.2534055709838867, val_acc: 0.46459999680519104,  lr: 0.009778153103448275
step: 1645, train_loss: 1.2092255353927612, acc: 0.48820000886917114, val_loss: 1.25200355052948, val_acc: 0.4708000123500824,  lr: 0.009777808620689655
step: 1646, train_loss: 1.2902624607086182, acc: 0.4537999927997589, val_loss: 1.2604511976242065, val_acc: 0.46219998598098755,  lr: 0.009777464137931034
step: 1647, train_loss: 1.2250409126281738, acc: 0.47920000553131104, val_loss: 1.2433840036392212, val_acc: 0.4729999899864197,  lr: 0.009777119655172414
step: 1648, train_loss: 1.2166060209274292, acc: 0.4878000020980835, val_loss: 1.238928198814392, val_acc: 0.47620001435279846,  lr: 0.009776775172413793
step: 1649, train_loss: 1.2732384204864502, acc: 0.44999998807907104, val_loss: 1.2298635244369507, val_acc: 0.4803999960422516,  lr: 0.009776430689655172
step: 1650, train_loss: 1.2532362937927246, acc: 0.4618000090122223, val_loss: 1.231048822402954, val_acc: 0.477400004863739,  lr: 0.009776086206896552
step: 1651, train_loss: 1.2613446712493896, acc: 0.4580000042915344, val_loss: 1.2319204807281494, val_acc: 0.47940000891685486,  lr: 0.009775741724137931
step: 1652, train_loss: 1.2427902221679688, acc: 0.4684000015258789, val_loss: 1.2347593307495117, val_acc: 0.4745999872684479,  lr: 0.00977539724137931
step: 1653, train_loss: 1.2377614974975586, acc: 0.47999998927116394, val_loss: 1.234298586845398, val_acc: 0.4706000089645386,  lr: 0.00977505275862069
step: 1654, train_loss: 1.1817060708999634, acc: 0.5054000020027161, val_loss: 1.2333327531814575, val_acc: 0.47679999470710754,  lr: 0.00977470827586207
step: 1655, train_loss: 1.2206602096557617, acc: 0.4832000136375427, val_loss: 1.2359894514083862, val_acc: 0.4733999967575073,  lr: 0.009774363793103449
step: 1656, train_loss: 1.2390615940093994, acc: 0.4796000123023987, val_loss: 1.2363386154174805, val_acc: 0.4713999927043915,  lr: 0.009774019310344828
step: 1657, train_loss: 1.1802537441253662, acc: 0.5044000148773193, val_loss: 1.239405870437622, val_acc: 0.4634000062942505,  lr: 0.009773674827586207
step: 1658, train_loss: 1.2106367349624634, acc: 0.49000000953674316, val_loss: 1.2385667562484741, val_acc: 0.4681999981403351,  lr: 0.009773330344827587
step: 1659, train_loss: 1.2252254486083984, acc: 0.4726000130176544, val_loss: 1.2443331480026245, val_acc: 0.4681999981403351,  lr: 0.009772985862068966
step: 1660, train_loss: 1.2337255477905273, acc: 0.475600004196167, val_loss: 1.2338160276412964, val_acc: 0.4749999940395355,  lr: 0.009772641379310346
step: 1661, train_loss: 1.1612541675567627, acc: 0.5109999775886536, val_loss: 1.239452838897705, val_acc: 0.4666000008583069,  lr: 0.009772296896551725
step: 1662, train_loss: 1.2594521045684814, acc: 0.46720001101493835, val_loss: 1.2448290586471558, val_acc: 0.47119998931884766,  lr: 0.009771952413793104
step: 1663, train_loss: 1.2185043096542358, acc: 0.4837999939918518, val_loss: 1.240913987159729, val_acc: 0.4675999879837036,  lr: 0.009771607931034484
step: 1664, train_loss: 1.2598474025726318, acc: 0.46700000762939453, val_loss: 1.2437524795532227, val_acc: 0.4713999927043915,  lr: 0.009771263448275863
step: 1665, train_loss: 1.2645306587219238, acc: 0.4553999900817871, val_loss: 1.2463347911834717, val_acc: 0.4745999872684479,  lr: 0.00977091896551724
step: 1666, train_loss: 1.302546739578247, acc: 0.45100000500679016, val_loss: 1.2463916540145874, val_acc: 0.4708000123500824,  lr: 0.00977057448275862
step: 1667, train_loss: 1.2406219244003296, acc: 0.4805999994277954, val_loss: 1.2372691631317139, val_acc: 0.4702000021934509,  lr: 0.00977023
step: 1668, train_loss: 1.2056611776351929, acc: 0.49459999799728394, val_loss: 1.2447222471237183, val_acc: 0.4634000062942505,  lr: 0.009769885517241379
step: 1669, train_loss: 1.2388195991516113, acc: 0.4715999960899353, val_loss: 1.2468950748443604, val_acc: 0.46239998936653137,  lr: 0.009769541034482758
step: 1670, train_loss: 1.238013505935669, acc: 0.47920000553131104, val_loss: 1.2405375242233276, val_acc: 0.4715999960899353,  lr: 0.009769196551724138
step: 1671, train_loss: 1.2238813638687134, acc: 0.47920000553131104, val_loss: 1.2463414669036865, val_acc: 0.4675999879837036,  lr: 0.009768852068965517
step: 1672, train_loss: 1.276793122291565, acc: 0.45719999074935913, val_loss: 1.243683934211731, val_acc: 0.4765999913215637,  lr: 0.009768507586206896
step: 1673, train_loss: 1.239634394645691, acc: 0.4740000069141388, val_loss: 1.2463009357452393, val_acc: 0.4742000102996826,  lr: 0.009768163103448276
step: 1674, train_loss: 1.2234368324279785, acc: 0.47519999742507935, val_loss: 1.2327206134796143, val_acc: 0.4781999886035919,  lr: 0.009767818620689655
step: 1675, train_loss: 1.218551516532898, acc: 0.48500001430511475, val_loss: 1.2365959882736206, val_acc: 0.4666000008583069,  lr: 0.009767474137931035
step: 1676, train_loss: 1.2198253870010376, acc: 0.4851999878883362, val_loss: 1.2422130107879639, val_acc: 0.4706000089645386,  lr: 0.009767129655172414
step: 1677, train_loss: 1.2238366603851318, acc: 0.4797999858856201, val_loss: 1.2354158163070679, val_acc: 0.4749999940395355,  lr: 0.009766785172413793
step: 1678, train_loss: 1.2317205667495728, acc: 0.47620001435279846, val_loss: 1.2357892990112305, val_acc: 0.4726000130176544,  lr: 0.009766440689655173
step: 1679, train_loss: 1.197489619255066, acc: 0.49239999055862427, val_loss: 1.2403621673583984, val_acc: 0.4699999988079071,  lr: 0.009766096206896552
step: 1680, train_loss: 1.2139910459518433, acc: 0.49079999327659607, val_loss: 1.2477338314056396, val_acc: 0.4697999954223633,  lr: 0.009765751724137932
step: 1681, train_loss: 1.2161504030227661, acc: 0.48579999804496765, val_loss: 1.2509596347808838, val_acc: 0.4733999967575073,  lr: 0.009765407241379311
step: 1682, train_loss: 1.2177526950836182, acc: 0.48539999127388, val_loss: 1.242973804473877, val_acc: 0.4690000116825104,  lr: 0.00976506275862069
step: 1683, train_loss: 1.1937628984451294, acc: 0.490200012922287, val_loss: 1.2348670959472656, val_acc: 0.4722000062465668,  lr: 0.00976471827586207
step: 1684, train_loss: 1.2039645910263062, acc: 0.4925999939441681, val_loss: 1.2343497276306152, val_acc: 0.4726000130176544,  lr: 0.009764373793103449
step: 1685, train_loss: 1.1786808967590332, acc: 0.504800021648407, val_loss: 1.2341927289962769, val_acc: 0.4731999933719635,  lr: 0.009764029310344828
step: 1686, train_loss: 1.157845377922058, acc: 0.5198000073432922, val_loss: 1.2308440208435059, val_acc: 0.47279998660087585,  lr: 0.009763684827586208
step: 1687, train_loss: 1.2079755067825317, acc: 0.4975999891757965, val_loss: 1.2372496128082275, val_acc: 0.4745999872684479,  lr: 0.009763340344827587
step: 1688, train_loss: 1.1797056198120117, acc: 0.4973999857902527, val_loss: 1.241227388381958, val_acc: 0.4699999988079071,  lr: 0.009762995862068967
step: 1689, train_loss: 1.2730779647827148, acc: 0.4636000096797943, val_loss: 1.2399098873138428, val_acc: 0.46860000491142273,  lr: 0.009762651379310344
step: 1690, train_loss: 1.2435929775238037, acc: 0.47620001435279846, val_loss: 1.2481757402420044, val_acc: 0.4690000116825104,  lr: 0.009762306896551724
step: 1691, train_loss: 1.2013684511184692, acc: 0.4925999939441681, val_loss: 1.2500890493392944, val_acc: 0.46540001034736633,  lr: 0.009761962413793103
step: 1692, train_loss: 1.2815897464752197, acc: 0.44279998540878296, val_loss: 1.2531206607818604, val_acc: 0.4659999907016754,  lr: 0.009761617931034482
step: 1693, train_loss: 1.2106438875198364, acc: 0.4832000136375427, val_loss: 1.251749873161316, val_acc: 0.46639999747276306,  lr: 0.009761273448275862
step: 1694, train_loss: 1.2426522970199585, acc: 0.4731999933719635, val_loss: 1.252519965171814, val_acc: 0.46799999475479126,  lr: 0.009760928965517241
step: 1695, train_loss: 1.251358151435852, acc: 0.4713999927043915, val_loss: 1.257506251335144, val_acc: 0.46959999203681946,  lr: 0.00976058448275862
step: 1696, train_loss: 1.188191533088684, acc: 0.4936000108718872, val_loss: 1.2596997022628784, val_acc: 0.46140000224113464,  lr: 0.00976024
step: 1697, train_loss: 1.2810015678405762, acc: 0.4447999894618988, val_loss: 1.2622976303100586, val_acc: 0.46140000224113464,  lr: 0.00975989551724138
step: 1698, train_loss: 1.2162388563156128, acc: 0.47780001163482666, val_loss: 1.272123098373413, val_acc: 0.46219998598098755,  lr: 0.009759551034482759
step: 1699, train_loss: 1.2027044296264648, acc: 0.4968000054359436, val_loss: 1.274651050567627, val_acc: 0.4564000070095062,  lr: 0.009759206551724138
step: 1700, train_loss: 1.1859878301620483, acc: 0.5009999871253967, val_loss: 1.2696139812469482, val_acc: 0.46459999680519104,  lr: 0.009758862068965517
step: 1701, train_loss: 1.3195158243179321, acc: 0.43160000443458557, val_loss: 1.2744112014770508, val_acc: 0.46320000290870667,  lr: 0.009758517586206897
step: 1702, train_loss: 1.1849323511123657, acc: 0.49219998717308044, val_loss: 1.2593603134155273, val_acc: 0.4625999927520752,  lr: 0.009758173103448276
step: 1703, train_loss: 1.2716999053955078, acc: 0.45879998803138733, val_loss: 1.2613282203674316, val_acc: 0.4607999920845032,  lr: 0.009757828620689656
step: 1704, train_loss: 1.2941457033157349, acc: 0.4438000023365021, val_loss: 1.2566150426864624, val_acc: 0.46560001373291016,  lr: 0.009757484137931035
step: 1705, train_loss: 1.2285782098770142, acc: 0.4805999994277954, val_loss: 1.261251449584961, val_acc: 0.46239998936653137,  lr: 0.009757139655172414
step: 1706, train_loss: 1.1969691514968872, acc: 0.49480000138282776, val_loss: 1.261154055595398, val_acc: 0.4731999933719635,  lr: 0.009756795172413794
step: 1707, train_loss: 1.272065281867981, acc: 0.4553999900817871, val_loss: 1.2594600915908813, val_acc: 0.46480000019073486,  lr: 0.009756450689655173
step: 1708, train_loss: 1.2530487775802612, acc: 0.46720001101493835, val_loss: 1.2620443105697632, val_acc: 0.4675999879837036,  lr: 0.009756106206896552
step: 1709, train_loss: 1.2049261331558228, acc: 0.490200012922287, val_loss: 1.2451114654541016, val_acc: 0.4758000075817108,  lr: 0.009755761724137932
step: 1710, train_loss: 1.269987940788269, acc: 0.46380001306533813, val_loss: 1.2579165697097778, val_acc: 0.46560001373291016,  lr: 0.009755417241379311
step: 1711, train_loss: 1.22111976146698, acc: 0.4875999987125397, val_loss: 1.2467995882034302, val_acc: 0.4607999920845032,  lr: 0.00975507275862069
step: 1712, train_loss: 1.2409683465957642, acc: 0.4729999899864197, val_loss: 1.2570202350616455, val_acc: 0.45579999685287476,  lr: 0.00975472827586207
step: 1713, train_loss: 1.2523863315582275, acc: 0.46959999203681946, val_loss: 1.245333194732666, val_acc: 0.46700000762939453,  lr: 0.009754383793103448
step: 1714, train_loss: 1.2408921718597412, acc: 0.4697999954223633, val_loss: 1.2602639198303223, val_acc: 0.4684000015258789,  lr: 0.009754039310344827
step: 1715, train_loss: 1.1958377361297607, acc: 0.49239999055862427, val_loss: 1.2544236183166504, val_acc: 0.46959999203681946,  lr: 0.009753694827586206
step: 1716, train_loss: 1.2825736999511719, acc: 0.44600000977516174, val_loss: 1.2502375841140747, val_acc: 0.46540001034736633,  lr: 0.009753350344827586
step: 1717, train_loss: 1.25547456741333, acc: 0.46560001373291016, val_loss: 1.2539873123168945, val_acc: 0.4643999934196472,  lr: 0.009753005862068965
step: 1718, train_loss: 1.2984964847564697, acc: 0.44119998812675476, val_loss: 1.246852159500122, val_acc: 0.46299999952316284,  lr: 0.009752661379310345
step: 1719, train_loss: 1.2490272521972656, acc: 0.4675999879837036, val_loss: 1.2435601949691772, val_acc: 0.4668000042438507,  lr: 0.009752316896551724
step: 1720, train_loss: 1.2025806903839111, acc: 0.49239999055862427, val_loss: 1.2447293996810913, val_acc: 0.46639999747276306,  lr: 0.009751972413793103
step: 1721, train_loss: 1.2173105478286743, acc: 0.49320000410079956, val_loss: 1.2420847415924072, val_acc: 0.4724000096321106,  lr: 0.009751627931034483
step: 1722, train_loss: 1.2559587955474854, acc: 0.4729999899864197, val_loss: 1.2457987070083618, val_acc: 0.46799999475479126,  lr: 0.009751283448275862
step: 1723, train_loss: 1.1982964277267456, acc: 0.4952000081539154, val_loss: 1.2422137260437012, val_acc: 0.46959999203681946,  lr: 0.009750938965517241
step: 1724, train_loss: 1.2240792512893677, acc: 0.4828000068664551, val_loss: 1.2465790510177612, val_acc: 0.4706000089645386,  lr: 0.00975059448275862
step: 1725, train_loss: 1.2226694822311401, acc: 0.47600001096725464, val_loss: 1.2482516765594482, val_acc: 0.46459999680519104,  lr: 0.00975025
step: 1726, train_loss: 1.1889621019363403, acc: 0.4966000020503998, val_loss: 1.2390196323394775, val_acc: 0.4731999933719635,  lr: 0.00974990551724138
step: 1727, train_loss: 1.288468837738037, acc: 0.45019999146461487, val_loss: 1.2352229356765747, val_acc: 0.4790000021457672,  lr: 0.009749561034482759
step: 1728, train_loss: 1.1815106868743896, acc: 0.4903999865055084, val_loss: 1.2305116653442383, val_acc: 0.4765999913215637,  lr: 0.009749216551724138
step: 1729, train_loss: 1.246093511581421, acc: 0.47099998593330383, val_loss: 1.228724718093872, val_acc: 0.4779999852180481,  lr: 0.009748872068965518
step: 1730, train_loss: 1.2193936109542847, acc: 0.4878000020980835, val_loss: 1.228712558746338, val_acc: 0.47839999198913574,  lr: 0.009748527586206897
step: 1731, train_loss: 1.2428245544433594, acc: 0.4740000069141388, val_loss: 1.2400635480880737, val_acc: 0.47679999470710754,  lr: 0.009748183103448277
step: 1732, train_loss: 1.1713799238204956, acc: 0.5070000290870667, val_loss: 1.2305567264556885, val_acc: 0.4844000041484833,  lr: 0.009747838620689656
step: 1733, train_loss: 1.2709747552871704, acc: 0.4620000123977661, val_loss: 1.2473000288009644, val_acc: 0.4763999879360199,  lr: 0.009747494137931035
step: 1734, train_loss: 1.2600185871124268, acc: 0.46459999680519104, val_loss: 1.2386000156402588, val_acc: 0.4749999940395355,  lr: 0.009747149655172415
step: 1735, train_loss: 1.1867791414260864, acc: 0.4957999885082245, val_loss: 1.2345707416534424, val_acc: 0.4742000102996826,  lr: 0.009746805172413794
step: 1736, train_loss: 1.20847749710083, acc: 0.4986000061035156, val_loss: 1.2370750904083252, val_acc: 0.4729999899864197,  lr: 0.009746460689655173
step: 1737, train_loss: 1.2035492658615112, acc: 0.4925999939441681, val_loss: 1.2494961023330688, val_acc: 0.4699999988079071,  lr: 0.009746116206896551
step: 1738, train_loss: 1.200706124305725, acc: 0.4912000000476837, val_loss: 1.2399176359176636, val_acc: 0.47200000286102295,  lr: 0.00974577172413793
step: 1739, train_loss: 1.181776762008667, acc: 0.5027999877929688, val_loss: 1.2425676584243774, val_acc: 0.46860000491142273,  lr: 0.00974542724137931
step: 1740, train_loss: 1.2626678943634033, acc: 0.45879998803138733, val_loss: 1.2430315017700195, val_acc: 0.4717999994754791,  lr: 0.00974508275862069
step: 1741, train_loss: 1.2524232864379883, acc: 0.4717999994754791, val_loss: 1.241275668144226, val_acc: 0.47679999470710754,  lr: 0.009744738275862069
step: 1742, train_loss: 1.1776014566421509, acc: 0.5031999945640564, val_loss: 1.2332773208618164, val_acc: 0.47679999470710754,  lr: 0.009744393793103448
step: 1743, train_loss: 1.1931183338165283, acc: 0.4848000109195709, val_loss: 1.2297054529190063, val_acc: 0.48019999265670776,  lr: 0.009744049310344827
step: 1744, train_loss: 1.2046878337860107, acc: 0.4959999918937683, val_loss: 1.2385586500167847, val_acc: 0.46959999203681946,  lr: 0.009743704827586207
step: 1745, train_loss: 1.1885706186294556, acc: 0.4925999939441681, val_loss: 1.2397129535675049, val_acc: 0.47679999470710754,  lr: 0.009743360344827586
step: 1746, train_loss: 1.2834033966064453, acc: 0.4580000042915344, val_loss: 1.2372790575027466, val_acc: 0.47699999809265137,  lr: 0.009743015862068966
step: 1747, train_loss: 1.2802300453186035, acc: 0.4634000062942505, val_loss: 1.2292547225952148, val_acc: 0.4862000048160553,  lr: 0.009742671379310345
step: 1748, train_loss: 1.2528523206710815, acc: 0.4731999933719635, val_loss: 1.2232716083526611, val_acc: 0.47999998927116394,  lr: 0.009742326896551724
step: 1749, train_loss: 1.1697120666503906, acc: 0.503000020980835, val_loss: 1.2250356674194336, val_acc: 0.48420000076293945,  lr: 0.009741982413793104
step: 1750, train_loss: 1.177017331123352, acc: 0.503000020980835, val_loss: 1.21336030960083, val_acc: 0.4846000075340271,  lr: 0.009741637931034483
step: 1751, train_loss: 1.232430338859558, acc: 0.4765999913215637, val_loss: 1.2150332927703857, val_acc: 0.4912000000476837,  lr: 0.009741293448275862
step: 1752, train_loss: 1.18137788772583, acc: 0.5045999884605408, val_loss: 1.2144442796707153, val_acc: 0.4909999966621399,  lr: 0.009740948965517242
step: 1753, train_loss: 1.1837921142578125, acc: 0.5034000277519226, val_loss: 1.2059069871902466, val_acc: 0.49399998784065247,  lr: 0.009740604482758621
step: 1754, train_loss: 1.2258987426757812, acc: 0.4851999878883362, val_loss: 1.2058948278427124, val_acc: 0.49540001153945923,  lr: 0.00974026
step: 1755, train_loss: 1.247654914855957, acc: 0.46860000491142273, val_loss: 1.2083604335784912, val_acc: 0.49459999799728394,  lr: 0.00973991551724138
step: 1756, train_loss: 1.1813983917236328, acc: 0.5012000203132629, val_loss: 1.2122743129730225, val_acc: 0.49540001153945923,  lr: 0.00973957103448276
step: 1757, train_loss: 1.1662100553512573, acc: 0.5171999931335449, val_loss: 1.2208040952682495, val_acc: 0.4851999878883362,  lr: 0.009739226551724139
step: 1758, train_loss: 1.249120831489563, acc: 0.47519999742507935, val_loss: 1.224624514579773, val_acc: 0.4803999960422516,  lr: 0.009738882068965518
step: 1759, train_loss: 1.2134215831756592, acc: 0.49459999799728394, val_loss: 1.228552222251892, val_acc: 0.4812000095844269,  lr: 0.009738537586206897
step: 1760, train_loss: 1.2466156482696533, acc: 0.4706000089645386, val_loss: 1.2400121688842773, val_acc: 0.47780001163482666,  lr: 0.009738193103448277
step: 1761, train_loss: 1.1823838949203491, acc: 0.4941999912261963, val_loss: 1.2332602739334106, val_acc: 0.47620001435279846,  lr: 0.009737848620689656
step: 1762, train_loss: 1.256006121635437, acc: 0.4675999879837036, val_loss: 1.2377793788909912, val_acc: 0.47699999809265137,  lr: 0.009737504137931034
step: 1763, train_loss: 1.230934739112854, acc: 0.47679999470710754, val_loss: 1.2335920333862305, val_acc: 0.4731999933719635,  lr: 0.009737159655172413
step: 1764, train_loss: 1.2191590070724487, acc: 0.4869999885559082, val_loss: 1.2376424074172974, val_acc: 0.47200000286102295,  lr: 0.009736815172413793
step: 1765, train_loss: 1.2063579559326172, acc: 0.4880000054836273, val_loss: 1.2308834791183472, val_acc: 0.47279998660087585,  lr: 0.009736470689655172
step: 1766, train_loss: 1.1936098337173462, acc: 0.49880000948905945, val_loss: 1.2373093366622925, val_acc: 0.47540000081062317,  lr: 0.009736126206896551
step: 1767, train_loss: 1.2065068483352661, acc: 0.4830000102519989, val_loss: 1.239745855331421, val_acc: 0.4731999933719635,  lr: 0.00973578172413793
step: 1768, train_loss: 1.251242995262146, acc: 0.462799996137619, val_loss: 1.2361114025115967, val_acc: 0.4779999852180481,  lr: 0.00973543724137931
step: 1769, train_loss: 1.2342933416366577, acc: 0.4702000021934509, val_loss: 1.2400190830230713, val_acc: 0.4722000062465668,  lr: 0.00973509275862069
step: 1770, train_loss: 1.1815112829208374, acc: 0.5067999958992004, val_loss: 1.239522099494934, val_acc: 0.47200000286102295,  lr: 0.009734748275862069
step: 1771, train_loss: 1.253896713256836, acc: 0.45739999413490295, val_loss: 1.242445468902588, val_acc: 0.4724000096321106,  lr: 0.009734403793103448
step: 1772, train_loss: 1.148829698562622, acc: 0.5130000114440918, val_loss: 1.2459585666656494, val_acc: 0.46619999408721924,  lr: 0.009734059310344828
step: 1773, train_loss: 1.2078520059585571, acc: 0.49000000953674316, val_loss: 1.246923565864563, val_acc: 0.46880000829696655,  lr: 0.009733714827586207
step: 1774, train_loss: 1.1875793933868408, acc: 0.49480000138282776, val_loss: 1.254042387008667, val_acc: 0.4641999900341034,  lr: 0.009733370344827586
step: 1775, train_loss: 1.1675446033477783, acc: 0.5073999762535095, val_loss: 1.2512192726135254, val_acc: 0.46639999747276306,  lr: 0.009733025862068966
step: 1776, train_loss: 1.1795966625213623, acc: 0.4986000061035156, val_loss: 1.2522832155227661, val_acc: 0.46619999408721924,  lr: 0.009732681379310345
step: 1777, train_loss: 1.2485558986663818, acc: 0.4706000089645386, val_loss: 1.2505511045455933, val_acc: 0.4666000008583069,  lr: 0.009732336896551725
step: 1778, train_loss: 1.23857843875885, acc: 0.46480000019073486, val_loss: 1.2453670501708984, val_acc: 0.46700000762939453,  lr: 0.009731992413793104
step: 1779, train_loss: 1.1701444387435913, acc: 0.5027999877929688, val_loss: 1.2494350671768188, val_acc: 0.4697999954223633,  lr: 0.009731647931034483
step: 1780, train_loss: 1.29463529586792, acc: 0.453000009059906, val_loss: 1.2498499155044556, val_acc: 0.46619999408721924,  lr: 0.009731303448275863
step: 1781, train_loss: 1.2100517749786377, acc: 0.4869999885559082, val_loss: 1.2538729906082153, val_acc: 0.46619999408721924,  lr: 0.009730958965517242
step: 1782, train_loss: 1.1951149702072144, acc: 0.4959999918937683, val_loss: 1.2572134733200073, val_acc: 0.46939998865127563,  lr: 0.009730614482758622
step: 1783, train_loss: 1.1751371622085571, acc: 0.503000020980835, val_loss: 1.2466062307357788, val_acc: 0.4724000096321106,  lr: 0.009730270000000001
step: 1784, train_loss: 1.2181928157806396, acc: 0.48579999804496765, val_loss: 1.2456505298614502, val_acc: 0.46140000224113464,  lr: 0.00972992551724138
step: 1785, train_loss: 1.1933906078338623, acc: 0.4893999993801117, val_loss: 1.2426939010620117, val_acc: 0.4722000062465668,  lr: 0.009729581034482758
step: 1786, train_loss: 1.223158359527588, acc: 0.4772000014781952, val_loss: 1.2348970174789429, val_acc: 0.47540000081062317,  lr: 0.009729236551724137
step: 1787, train_loss: 1.2415753602981567, acc: 0.46799999475479126, val_loss: 1.232001543045044, val_acc: 0.47360000014305115,  lr: 0.009728892068965517
step: 1788, train_loss: 1.176532506942749, acc: 0.5121999979019165, val_loss: 1.230277419090271, val_acc: 0.48019999265670776,  lr: 0.009728547586206896
step: 1789, train_loss: 1.2154425382614136, acc: 0.48899999260902405, val_loss: 1.2487266063690186, val_acc: 0.4740000069141388,  lr: 0.009728203103448275
step: 1790, train_loss: 1.2020844221115112, acc: 0.5016000270843506, val_loss: 1.2414522171020508, val_acc: 0.47360000014305115,  lr: 0.009727858620689655
step: 1791, train_loss: 1.2590351104736328, acc: 0.4575999975204468, val_loss: 1.2427273988723755, val_acc: 0.4717999994754791,  lr: 0.009727514137931034
step: 1792, train_loss: 1.215145468711853, acc: 0.47940000891685486, val_loss: 1.244056224822998, val_acc: 0.477400004863739,  lr: 0.009727169655172414
step: 1793, train_loss: 1.2577486038208008, acc: 0.4697999954223633, val_loss: 1.2445740699768066, val_acc: 0.46880000829696655,  lr: 0.009726825172413793
step: 1794, train_loss: 1.1773861646652222, acc: 0.5080000162124634, val_loss: 1.2384977340698242, val_acc: 0.47519999742507935,  lr: 0.009726480689655172
step: 1795, train_loss: 1.1614128351211548, acc: 0.5121999979019165, val_loss: 1.2353368997573853, val_acc: 0.4781999886035919,  lr: 0.009726136206896552
step: 1796, train_loss: 1.1771835088729858, acc: 0.5077999830245972, val_loss: 1.2380503416061401, val_acc: 0.4814000129699707,  lr: 0.009725791724137931
step: 1797, train_loss: 1.2153793573379517, acc: 0.4797999858856201, val_loss: 1.2344893217086792, val_acc: 0.477400004863739,  lr: 0.00972544724137931
step: 1798, train_loss: 1.3055223226547241, acc: 0.4453999996185303, val_loss: 1.2390385866165161, val_acc: 0.4772000014781952,  lr: 0.00972510275862069
step: 1799, train_loss: 1.1838867664337158, acc: 0.5023999810218811, val_loss: 1.2370731830596924, val_acc: 0.4724000096321106,  lr: 0.00972475827586207
step: 1800, train_loss: 1.2312902212142944, acc: 0.47920000553131104, val_loss: 1.2410224676132202, val_acc: 0.4715999960899353,  lr: 0.009724413793103449
step: 1801, train_loss: 1.2364264726638794, acc: 0.4702000021934509, val_loss: 1.237353801727295, val_acc: 0.47760000824928284,  lr: 0.009724069310344828
step: 1802, train_loss: 1.2133629322052002, acc: 0.4927999973297119, val_loss: 1.2370316982269287, val_acc: 0.4724000096321106,  lr: 0.009723724827586207
step: 1803, train_loss: 1.1594154834747314, acc: 0.5127999782562256, val_loss: 1.2356516122817993, val_acc: 0.4731999933719635,  lr: 0.009723380344827587
step: 1804, train_loss: 1.2255350351333618, acc: 0.4749999940395355, val_loss: 1.2418758869171143, val_acc: 0.4796000123023987,  lr: 0.009723035862068966
step: 1805, train_loss: 1.2322542667388916, acc: 0.48019999265670776, val_loss: 1.246208906173706, val_acc: 0.47699999809265137,  lr: 0.009722691379310346
step: 1806, train_loss: 1.2193266153335571, acc: 0.48080000281333923, val_loss: 1.2325421571731567, val_acc: 0.47940000891685486,  lr: 0.009722346896551725
step: 1807, train_loss: 1.205949068069458, acc: 0.48500001430511475, val_loss: 1.2295178174972534, val_acc: 0.48579999804496765,  lr: 0.009722002413793104
step: 1808, train_loss: 1.219759225845337, acc: 0.4805999994277954, val_loss: 1.225213646888733, val_acc: 0.4903999865055084,  lr: 0.009721657931034484
step: 1809, train_loss: 1.1919125318527222, acc: 0.4860000014305115, val_loss: 1.2272541522979736, val_acc: 0.4878000020980835,  lr: 0.009721313448275863
step: 1810, train_loss: 1.1823806762695312, acc: 0.49959999322891235, val_loss: 1.2246716022491455, val_acc: 0.48559999465942383,  lr: 0.00972096896551724
step: 1811, train_loss: 1.1894031763076782, acc: 0.49639999866485596, val_loss: 1.2250880002975464, val_acc: 0.4830000102519989,  lr: 0.00972062448275862
step: 1812, train_loss: 1.1842782497406006, acc: 0.5005999803543091, val_loss: 1.2292386293411255, val_acc: 0.47839999198913574,  lr: 0.00972028
step: 1813, train_loss: 1.1940419673919678, acc: 0.5013999938964844, val_loss: 1.2283504009246826, val_acc: 0.4796000123023987,  lr: 0.009719935517241379
step: 1814, train_loss: 1.193182110786438, acc: 0.5009999871253967, val_loss: 1.2274658679962158, val_acc: 0.4851999878883362,  lr: 0.009719591034482758
step: 1815, train_loss: 1.2498723268508911, acc: 0.4749999940395355, val_loss: 1.2248823642730713, val_acc: 0.4812000095844269,  lr: 0.009719246551724138
step: 1816, train_loss: 1.2581751346588135, acc: 0.46959999203681946, val_loss: 1.2221202850341797, val_acc: 0.4851999878883362,  lr: 0.009718902068965517
step: 1817, train_loss: 1.1826130151748657, acc: 0.4975999891757965, val_loss: 1.2240179777145386, val_acc: 0.48240000009536743,  lr: 0.009718557586206896
step: 1818, train_loss: 1.2682960033416748, acc: 0.4562000036239624, val_loss: 1.2205653190612793, val_acc: 0.48559999465942383,  lr: 0.009718213103448276
step: 1819, train_loss: 1.1992741823196411, acc: 0.49720001220703125, val_loss: 1.2142763137817383, val_acc: 0.48399999737739563,  lr: 0.009717868620689655
step: 1820, train_loss: 1.1754206418991089, acc: 0.4950000047683716, val_loss: 1.2177554368972778, val_acc: 0.48660001158714294,  lr: 0.009717524137931035
step: 1821, train_loss: 1.1652672290802002, acc: 0.5027999877929688, val_loss: 1.2110204696655273, val_acc: 0.48919999599456787,  lr: 0.009717179655172414
step: 1822, train_loss: 1.191239595413208, acc: 0.4968000054359436, val_loss: 1.2141321897506714, val_acc: 0.48500001430511475,  lr: 0.009716835172413793
step: 1823, train_loss: 1.1577883958816528, acc: 0.522599995136261, val_loss: 1.2128897905349731, val_acc: 0.4887999892234802,  lr: 0.009716490689655173
step: 1824, train_loss: 1.18179452419281, acc: 0.5005999803543091, val_loss: 1.203980803489685, val_acc: 0.4909999966621399,  lr: 0.009716146206896552
step: 1825, train_loss: 1.210455298423767, acc: 0.48820000886917114, val_loss: 1.2115310430526733, val_acc: 0.48739999532699585,  lr: 0.009715801724137932
step: 1826, train_loss: 1.1682602167129517, acc: 0.5034000277519226, val_loss: 1.2123078107833862, val_acc: 0.48899999260902405,  lr: 0.009715457241379311
step: 1827, train_loss: 1.2211040258407593, acc: 0.4844000041484833, val_loss: 1.2159239053726196, val_acc: 0.48739999532699585,  lr: 0.00971511275862069
step: 1828, train_loss: 1.2311053276062012, acc: 0.4729999899864197, val_loss: 1.21365225315094, val_acc: 0.49000000953674316,  lr: 0.00971476827586207
step: 1829, train_loss: 1.181991696357727, acc: 0.5001999735832214, val_loss: 1.2225037813186646, val_acc: 0.4844000041484833,  lr: 0.009714423793103449
step: 1830, train_loss: 1.181220531463623, acc: 0.5157999992370605, val_loss: 1.2109384536743164, val_acc: 0.4959999918937683,  lr: 0.009714079310344828
step: 1831, train_loss: 1.1534761190414429, acc: 0.520799994468689, val_loss: 1.2158080339431763, val_acc: 0.4867999851703644,  lr: 0.009713734827586208
step: 1832, train_loss: 1.161476969718933, acc: 0.5221999883651733, val_loss: 1.2173634767532349, val_acc: 0.4848000109195709,  lr: 0.009713390344827587
step: 1833, train_loss: 1.1661460399627686, acc: 0.5027999877929688, val_loss: 1.2227996587753296, val_acc: 0.48260000348091125,  lr: 0.009713045862068965
step: 1834, train_loss: 1.29163658618927, acc: 0.4546000063419342, val_loss: 1.2233275175094604, val_acc: 0.48019999265670776,  lr: 0.009712701379310344
step: 1835, train_loss: 1.1730501651763916, acc: 0.5037999749183655, val_loss: 1.2217835187911987, val_acc: 0.4803999960422516,  lr: 0.009712356896551724
step: 1836, train_loss: 1.2353765964508057, acc: 0.4740000069141388, val_loss: 1.2221310138702393, val_acc: 0.4779999852180481,  lr: 0.009712012413793103
step: 1837, train_loss: 1.163958191871643, acc: 0.5081999897956848, val_loss: 1.2183400392532349, val_acc: 0.4875999987125397,  lr: 0.009711667931034482
step: 1838, train_loss: 1.2965691089630127, acc: 0.4426000118255615, val_loss: 1.2297546863555908, val_acc: 0.4814000129699707,  lr: 0.009711323448275862
step: 1839, train_loss: 1.2197648286819458, acc: 0.490200012922287, val_loss: 1.2280017137527466, val_acc: 0.48399999737739563,  lr: 0.009710978965517241
step: 1840, train_loss: 1.2560813426971436, acc: 0.45739999413490295, val_loss: 1.2293474674224854, val_acc: 0.48159998655319214,  lr: 0.00971063448275862
step: 1841, train_loss: 1.2181200981140137, acc: 0.48019999265670776, val_loss: 1.2279123067855835, val_acc: 0.4846000075340271,  lr: 0.00971029
step: 1842, train_loss: 1.2211803197860718, acc: 0.47940000891685486, val_loss: 1.2200654745101929, val_acc: 0.47760000824928284,  lr: 0.00970994551724138
step: 1843, train_loss: 1.2275550365447998, acc: 0.4819999933242798, val_loss: 1.2205390930175781, val_acc: 0.47679999470710754,  lr: 0.009709601034482759
step: 1844, train_loss: 1.1948615312576294, acc: 0.5009999871253967, val_loss: 1.2288532257080078, val_acc: 0.477400004863739,  lr: 0.009709256551724138
step: 1845, train_loss: 1.212733268737793, acc: 0.48739999532699585, val_loss: 1.2221653461456299, val_acc: 0.48080000281333923,  lr: 0.009708912068965517
step: 1846, train_loss: 1.1858474016189575, acc: 0.5131999850273132, val_loss: 1.2246553897857666, val_acc: 0.4787999987602234,  lr: 0.009708567586206897
step: 1847, train_loss: 1.1599993705749512, acc: 0.5077999830245972, val_loss: 1.2268102169036865, val_acc: 0.47940000891685486,  lr: 0.009708223103448276
step: 1848, train_loss: 1.2552193403244019, acc: 0.4722000062465668, val_loss: 1.2245333194732666, val_acc: 0.48339998722076416,  lr: 0.009707878620689656
step: 1849, train_loss: 1.2215317487716675, acc: 0.4779999852180481, val_loss: 1.2247555255889893, val_acc: 0.47920000553131104,  lr: 0.009707534137931035
step: 1850, train_loss: 1.202470302581787, acc: 0.48739999532699585, val_loss: 1.2278977632522583, val_acc: 0.48100000619888306,  lr: 0.009707189655172414
step: 1851, train_loss: 1.1679505109786987, acc: 0.5121999979019165, val_loss: 1.2295196056365967, val_acc: 0.4803999960422516,  lr: 0.009706845172413794
step: 1852, train_loss: 1.2691168785095215, acc: 0.4611999988555908, val_loss: 1.216967225074768, val_acc: 0.49000000953674316,  lr: 0.009706500689655173
step: 1853, train_loss: 1.2031779289245605, acc: 0.4909999966621399, val_loss: 1.2185180187225342, val_acc: 0.4862000048160553,  lr: 0.009706156206896552
step: 1854, train_loss: 1.2673217058181763, acc: 0.4797999858856201, val_loss: 1.2300559282302856, val_acc: 0.4851999878883362,  lr: 0.009705811724137932
step: 1855, train_loss: 1.2427425384521484, acc: 0.46799999475479126, val_loss: 1.231557011604309, val_acc: 0.4745999872684479,  lr: 0.009705467241379311
step: 1856, train_loss: 1.2154054641723633, acc: 0.4878000020980835, val_loss: 1.2461766004562378, val_acc: 0.477400004863739,  lr: 0.00970512275862069
step: 1857, train_loss: 1.1634502410888672, acc: 0.5174000263214111, val_loss: 1.2458652257919312, val_acc: 0.47360000014305115,  lr: 0.00970477827586207
step: 1858, train_loss: 1.265031099319458, acc: 0.45739999413490295, val_loss: 1.2523995637893677, val_acc: 0.4666000008583069,  lr: 0.009704433793103448
step: 1859, train_loss: 1.1932687759399414, acc: 0.49239999055862427, val_loss: 1.2493609189987183, val_acc: 0.4702000021934509,  lr: 0.009704089310344827
step: 1860, train_loss: 1.1717485189437866, acc: 0.49900001287460327, val_loss: 1.245772123336792, val_acc: 0.47360000014305115,  lr: 0.009703744827586206
step: 1861, train_loss: 1.174272894859314, acc: 0.5009999871253967, val_loss: 1.2561776638031006, val_acc: 0.4668000042438507,  lr: 0.009703400344827586
step: 1862, train_loss: 1.2796909809112549, acc: 0.4560000002384186, val_loss: 1.2472352981567383, val_acc: 0.47119998931884766,  lr: 0.009703055862068965
step: 1863, train_loss: 1.2523058652877808, acc: 0.4699999988079071, val_loss: 1.2582381963729858, val_acc: 0.4724000096321106,  lr: 0.009702711379310345
step: 1864, train_loss: 1.1996046304702759, acc: 0.49079999327659607, val_loss: 1.25325608253479, val_acc: 0.47360000014305115,  lr: 0.009702366896551724
step: 1865, train_loss: 1.2324053049087524, acc: 0.4706000089645386, val_loss: 1.249843955039978, val_acc: 0.4634000062942505,  lr: 0.009702022413793103
step: 1866, train_loss: 1.162034511566162, acc: 0.5188000202178955, val_loss: 1.245302677154541, val_acc: 0.4722000062465668,  lr: 0.009701677931034483
step: 1867, train_loss: 1.193219780921936, acc: 0.4975999891757965, val_loss: 1.247631311416626, val_acc: 0.4745999872684479,  lr: 0.009701333448275862
step: 1868, train_loss: 1.2079206705093384, acc: 0.49559998512268066, val_loss: 1.2522841691970825, val_acc: 0.47540000081062317,  lr: 0.009700988965517241
step: 1869, train_loss: 1.2206087112426758, acc: 0.48840001225471497, val_loss: 1.2539739608764648, val_acc: 0.47600001096725464,  lr: 0.00970064448275862
step: 1870, train_loss: 1.1572012901306152, acc: 0.5098000168800354, val_loss: 1.253757119178772, val_acc: 0.46880000829696655,  lr: 0.0097003
step: 1871, train_loss: 1.2699869871139526, acc: 0.46140000224113464, val_loss: 1.2557334899902344, val_acc: 0.4674000144004822,  lr: 0.00969995551724138
step: 1872, train_loss: 1.1633703708648682, acc: 0.5189999938011169, val_loss: 1.2563602924346924, val_acc: 0.4708000123500824,  lr: 0.009699611034482759
step: 1873, train_loss: 1.1845359802246094, acc: 0.49720001220703125, val_loss: 1.27022123336792, val_acc: 0.46000000834465027,  lr: 0.009699266551724138
step: 1874, train_loss: 1.201858639717102, acc: 0.4966000020503998, val_loss: 1.2567118406295776, val_acc: 0.4674000144004822,  lr: 0.009698922068965518
step: 1875, train_loss: 1.2311205863952637, acc: 0.4803999960422516, val_loss: 1.252358078956604, val_acc: 0.4724000096321106,  lr: 0.009698577586206897
step: 1876, train_loss: 1.2265746593475342, acc: 0.47839999198913574, val_loss: 1.2570692300796509, val_acc: 0.47679999470710754,  lr: 0.009698233103448277
step: 1877, train_loss: 1.143647313117981, acc: 0.5185999870300293, val_loss: 1.2557810544967651, val_acc: 0.46799999475479126,  lr: 0.009697888620689656
step: 1878, train_loss: 1.1847399473190308, acc: 0.5, val_loss: 1.2476863861083984, val_acc: 0.46540001034736633,  lr: 0.009697544137931035
step: 1879, train_loss: 1.1781238317489624, acc: 0.5037999749183655, val_loss: 1.2599223852157593, val_acc: 0.4657999873161316,  lr: 0.009697199655172415
step: 1880, train_loss: 1.1677073240280151, acc: 0.5052000284194946, val_loss: 1.2479101419448853, val_acc: 0.47360000014305115,  lr: 0.009696855172413794
step: 1881, train_loss: 1.2329645156860352, acc: 0.48919999599456787, val_loss: 1.2541563510894775, val_acc: 0.4681999981403351,  lr: 0.009696510689655173
step: 1882, train_loss: 1.1728447675704956, acc: 0.49959999322891235, val_loss: 1.2589750289916992, val_acc: 0.46140000224113464,  lr: 0.009696166206896551
step: 1883, train_loss: 1.3104439973831177, acc: 0.4410000145435333, val_loss: 1.2494206428527832, val_acc: 0.4717999994754791,  lr: 0.00969582172413793
step: 1884, train_loss: 1.1789286136627197, acc: 0.506600022315979, val_loss: 1.2414655685424805, val_acc: 0.4740000069141388,  lr: 0.00969547724137931
step: 1885, train_loss: 1.266314148902893, acc: 0.45899999141693115, val_loss: 1.233811378479004, val_acc: 0.47620001435279846,  lr: 0.00969513275862069
step: 1886, train_loss: 1.245743989944458, acc: 0.47600001096725464, val_loss: 1.2336305379867554, val_acc: 0.4805999994277954,  lr: 0.009694788275862069
step: 1887, train_loss: 1.2135231494903564, acc: 0.490200012922287, val_loss: 1.2275422811508179, val_acc: 0.477400004863739,  lr: 0.009694443793103448
step: 1888, train_loss: 1.1437597274780273, acc: 0.517799973487854, val_loss: 1.2315397262573242, val_acc: 0.4763999879360199,  lr: 0.009694099310344827
step: 1889, train_loss: 1.2427074909210205, acc: 0.4666000008583069, val_loss: 1.2311229705810547, val_acc: 0.47679999470710754,  lr: 0.009693754827586207
step: 1890, train_loss: 1.1589423418045044, acc: 0.5145999789237976, val_loss: 1.2394604682922363, val_acc: 0.4763999879360199,  lr: 0.009693410344827586
step: 1891, train_loss: 1.2081891298294067, acc: 0.49779999256134033, val_loss: 1.2219897508621216, val_acc: 0.47679999470710754,  lr: 0.009693065862068966
step: 1892, train_loss: 1.2793898582458496, acc: 0.45719999074935913, val_loss: 1.221229076385498, val_acc: 0.4729999899864197,  lr: 0.009692721379310345
step: 1893, train_loss: 1.2412075996398926, acc: 0.48080000281333923, val_loss: 1.2246967554092407, val_acc: 0.47620001435279846,  lr: 0.009692376896551724
step: 1894, train_loss: 1.2417432069778442, acc: 0.4713999927043915, val_loss: 1.2199069261550903, val_acc: 0.4796000123023987,  lr: 0.009692032413793104
step: 1895, train_loss: 1.1916004419326782, acc: 0.5001999735832214, val_loss: 1.2119382619857788, val_acc: 0.4885999858379364,  lr: 0.009691687931034483
step: 1896, train_loss: 1.1841392517089844, acc: 0.5044000148773193, val_loss: 1.2125494480133057, val_acc: 0.4869999885559082,  lr: 0.009691343448275862
step: 1897, train_loss: 1.2253291606903076, acc: 0.4867999851703644, val_loss: 1.2071648836135864, val_acc: 0.49160000681877136,  lr: 0.009690998965517242
step: 1898, train_loss: 1.2098615169525146, acc: 0.47780001163482666, val_loss: 1.2143492698669434, val_acc: 0.48980000615119934,  lr: 0.009690654482758621
step: 1899, train_loss: 1.1944494247436523, acc: 0.49000000953674316, val_loss: 1.215555191040039, val_acc: 0.477400004863739,  lr: 0.00969031
step: 1900, train_loss: 1.186669945716858, acc: 0.5037999749183655, val_loss: 1.2201249599456787, val_acc: 0.48019999265670776,  lr: 0.00968996551724138
step: 1901, train_loss: 1.2148089408874512, acc: 0.4867999851703644, val_loss: 1.216298222541809, val_acc: 0.4803999960422516,  lr: 0.00968962103448276
step: 1902, train_loss: 1.1907416582107544, acc: 0.4950000047683716, val_loss: 1.2326864004135132, val_acc: 0.4772000014781952,  lr: 0.009689276551724139
step: 1903, train_loss: 1.1873772144317627, acc: 0.5076000094413757, val_loss: 1.22352933883667, val_acc: 0.47839999198913574,  lr: 0.009688932068965518
step: 1904, train_loss: 1.1938271522521973, acc: 0.4973999857902527, val_loss: 1.2391571998596191, val_acc: 0.46959999203681946,  lr: 0.009688587586206898
step: 1905, train_loss: 1.2551056146621704, acc: 0.459199994802475, val_loss: 1.2459166049957275, val_acc: 0.4742000102996826,  lr: 0.009688243103448277
step: 1906, train_loss: 1.1600797176361084, acc: 0.5130000114440918, val_loss: 1.23944091796875, val_acc: 0.4731999933719635,  lr: 0.009687898620689655
step: 1907, train_loss: 1.157531499862671, acc: 0.504800021648407, val_loss: 1.241111159324646, val_acc: 0.4708000123500824,  lr: 0.009687554137931034
step: 1908, train_loss: 1.2181493043899536, acc: 0.4864000082015991, val_loss: 1.250936508178711, val_acc: 0.4722000062465668,  lr: 0.009687209655172413
step: 1909, train_loss: 1.2324483394622803, acc: 0.4726000130176544, val_loss: 1.248748779296875, val_acc: 0.4724000096321106,  lr: 0.009686865172413793
step: 1910, train_loss: 1.200804591178894, acc: 0.4909999966621399, val_loss: 1.2415434122085571, val_acc: 0.47859999537467957,  lr: 0.009686520689655172
step: 1911, train_loss: 1.2535451650619507, acc: 0.47099998593330383, val_loss: 1.2557897567749023, val_acc: 0.4742000102996826,  lr: 0.009686176206896551
step: 1912, train_loss: 1.2388070821762085, acc: 0.48559999465942383, val_loss: 1.2328077554702759, val_acc: 0.4772000014781952,  lr: 0.00968583172413793
step: 1913, train_loss: 1.199141263961792, acc: 0.4896000027656555, val_loss: 1.2477288246154785, val_acc: 0.47679999470710754,  lr: 0.00968548724137931
step: 1914, train_loss: 1.2030142545700073, acc: 0.48820000886917114, val_loss: 1.2306214570999146, val_acc: 0.4814000129699707,  lr: 0.00968514275862069
step: 1915, train_loss: 1.2320986986160278, acc: 0.4787999987602234, val_loss: 1.2493560314178467, val_acc: 0.4717999994754791,  lr: 0.009684798275862069
step: 1916, train_loss: 1.2301874160766602, acc: 0.4819999933242798, val_loss: 1.2378367185592651, val_acc: 0.4772000014781952,  lr: 0.009684453793103448
step: 1917, train_loss: 1.2822494506835938, acc: 0.46219998598098755, val_loss: 1.2413302659988403, val_acc: 0.47279998660087585,  lr: 0.009684109310344828
step: 1918, train_loss: 1.2107677459716797, acc: 0.49300000071525574, val_loss: 1.2442739009857178, val_acc: 0.4690000116825104,  lr: 0.009683764827586207
step: 1919, train_loss: 1.199530005455017, acc: 0.4925999939441681, val_loss: 1.2295339107513428, val_acc: 0.4821999967098236,  lr: 0.009683420344827586
step: 1920, train_loss: 1.152597427368164, acc: 0.5170000195503235, val_loss: 1.2331064939498901, val_acc: 0.4814000129699707,  lr: 0.009683075862068966
step: 1921, train_loss: 1.2081680297851562, acc: 0.48420000076293945, val_loss: 1.2270148992538452, val_acc: 0.4885999858379364,  lr: 0.009682731379310345
step: 1922, train_loss: 1.2203757762908936, acc: 0.46959999203681946, val_loss: 1.228936791419983, val_acc: 0.48100000619888306,  lr: 0.009682386896551725
step: 1923, train_loss: 1.1843270063400269, acc: 0.498199999332428, val_loss: 1.2269906997680664, val_acc: 0.4772000014781952,  lr: 0.009682042413793104
step: 1924, train_loss: 1.2101316452026367, acc: 0.4887999892234802, val_loss: 1.222150206565857, val_acc: 0.47859999537467957,  lr: 0.009681697931034483
step: 1925, train_loss: 1.1600416898727417, acc: 0.5116000175476074, val_loss: 1.2233760356903076, val_acc: 0.483599990606308,  lr: 0.009681353448275863
step: 1926, train_loss: 1.1462135314941406, acc: 0.5131999850273132, val_loss: 1.2168539762496948, val_acc: 0.4830000102519989,  lr: 0.009681008965517242
step: 1927, train_loss: 1.1547082662582397, acc: 0.5109999775886536, val_loss: 1.2156434059143066, val_acc: 0.48500001430511475,  lr: 0.009680664482758622
step: 1928, train_loss: 1.183014988899231, acc: 0.5, val_loss: 1.230587124824524, val_acc: 0.48080000281333923,  lr: 0.009680320000000001
step: 1929, train_loss: 1.1954840421676636, acc: 0.4997999966144562, val_loss: 1.232484221458435, val_acc: 0.48100000619888306,  lr: 0.00967997551724138
step: 1930, train_loss: 1.2466925382614136, acc: 0.47279998660087585, val_loss: 1.2267847061157227, val_acc: 0.4828000068664551,  lr: 0.009679631034482758
step: 1931, train_loss: 1.1795166730880737, acc: 0.4997999966144562, val_loss: 1.224737286567688, val_acc: 0.48339998722076416,  lr: 0.009679286551724137
step: 1932, train_loss: 1.2321584224700928, acc: 0.4844000041484833, val_loss: 1.2251111268997192, val_acc: 0.475600004196167,  lr: 0.009678942068965517
step: 1933, train_loss: 1.2104711532592773, acc: 0.49059998989105225, val_loss: 1.2188304662704468, val_acc: 0.4803999960422516,  lr: 0.009678597586206896
step: 1934, train_loss: 1.1987916231155396, acc: 0.492000013589859, val_loss: 1.2264004945755005, val_acc: 0.4796000123023987,  lr: 0.009678253103448275
step: 1935, train_loss: 1.2230308055877686, acc: 0.48539999127388, val_loss: 1.2245550155639648, val_acc: 0.4814000129699707,  lr: 0.009677908620689655
step: 1936, train_loss: 1.1435586214065552, acc: 0.517799973487854, val_loss: 1.225793480873108, val_acc: 0.4821999967098236,  lr: 0.009677564137931034
step: 1937, train_loss: 1.2784520387649536, acc: 0.4607999920845032, val_loss: 1.2301450967788696, val_acc: 0.4796000123023987,  lr: 0.009677219655172414
step: 1938, train_loss: 1.1785739660263062, acc: 0.5004000067710876, val_loss: 1.2328218221664429, val_acc: 0.47780001163482666,  lr: 0.009676875172413793
step: 1939, train_loss: 1.2298732995986938, acc: 0.4796000123023987, val_loss: 1.2304387092590332, val_acc: 0.4790000021457672,  lr: 0.009676530689655172
step: 1940, train_loss: 1.222042441368103, acc: 0.4864000082015991, val_loss: 1.234805703163147, val_acc: 0.4779999852180481,  lr: 0.009676186206896552
step: 1941, train_loss: 1.2029086351394653, acc: 0.4864000082015991, val_loss: 1.2282119989395142, val_acc: 0.4821999967098236,  lr: 0.009675841724137931
step: 1942, train_loss: 1.2275992631912231, acc: 0.4878000020980835, val_loss: 1.2357418537139893, val_acc: 0.47760000824928284,  lr: 0.00967549724137931
step: 1943, train_loss: 1.2008434534072876, acc: 0.4896000027656555, val_loss: 1.2323769330978394, val_acc: 0.4796000123023987,  lr: 0.00967515275862069
step: 1944, train_loss: 1.1765223741531372, acc: 0.5108000040054321, val_loss: 1.234674096107483, val_acc: 0.4803999960422516,  lr: 0.00967480827586207
step: 1945, train_loss: 1.1689350605010986, acc: 0.5080000162124634, val_loss: 1.2375415563583374, val_acc: 0.4758000075817108,  lr: 0.009674463793103449
step: 1946, train_loss: 1.2690633535385132, acc: 0.45820000767707825, val_loss: 1.2289448976516724, val_acc: 0.47859999537467957,  lr: 0.009674119310344828
step: 1947, train_loss: 1.2541546821594238, acc: 0.4675999879837036, val_loss: 1.22750985622406, val_acc: 0.47699999809265137,  lr: 0.009673774827586207
step: 1948, train_loss: 1.2234125137329102, acc: 0.47679999470710754, val_loss: 1.2268662452697754, val_acc: 0.48019999265670776,  lr: 0.009673430344827587
step: 1949, train_loss: 1.2000211477279663, acc: 0.4970000088214874, val_loss: 1.226660132408142, val_acc: 0.4867999851703644,  lr: 0.009673085862068966
step: 1950, train_loss: 1.1708382368087769, acc: 0.5167999863624573, val_loss: 1.2265524864196777, val_acc: 0.4814000129699707,  lr: 0.009672741379310346
step: 1951, train_loss: 1.3043012619018555, acc: 0.44279998540878296, val_loss: 1.249019980430603, val_acc: 0.46799999475479126,  lr: 0.009672396896551725
step: 1952, train_loss: 1.2321903705596924, acc: 0.4787999987602234, val_loss: 1.2322206497192383, val_acc: 0.4729999899864197,  lr: 0.009672052413793104
step: 1953, train_loss: 1.2817836999893188, acc: 0.46160000562667847, val_loss: 1.2351384162902832, val_acc: 0.4781999886035919,  lr: 0.009671707931034484
step: 1954, train_loss: 1.2160329818725586, acc: 0.48420000076293945, val_loss: 1.2248209714889526, val_acc: 0.48919999599456787,  lr: 0.009671363448275863
step: 1955, train_loss: 1.2207831144332886, acc: 0.4880000054836273, val_loss: 1.2303498983383179, val_acc: 0.47999998927116394,  lr: 0.00967101896551724
step: 1956, train_loss: 1.2345938682556152, acc: 0.47839999198913574, val_loss: 1.2132067680358887, val_acc: 0.4887999892234802,  lr: 0.00967067448275862
step: 1957, train_loss: 1.1944743394851685, acc: 0.4975999891757965, val_loss: 1.2245595455169678, val_acc: 0.4864000082015991,  lr: 0.00967033
step: 1958, train_loss: 1.1636015176773071, acc: 0.5149999856948853, val_loss: 1.2204408645629883, val_acc: 0.48739999532699585,  lr: 0.009669985517241379
step: 1959, train_loss: 1.2075403928756714, acc: 0.4851999878883362, val_loss: 1.2166032791137695, val_acc: 0.4878000020980835,  lr: 0.009669641034482758
step: 1960, train_loss: 1.2722268104553223, acc: 0.45419999957084656, val_loss: 1.2343806028366089, val_acc: 0.4814000129699707,  lr: 0.009669296551724138
step: 1961, train_loss: 1.1922296285629272, acc: 0.487199991941452, val_loss: 1.2249740362167358, val_acc: 0.48240000009536743,  lr: 0.009668952068965517
step: 1962, train_loss: 1.1957722902297974, acc: 0.4862000048160553, val_loss: 1.2224253416061401, val_acc: 0.4805999994277954,  lr: 0.009668607586206896
step: 1963, train_loss: 1.2434606552124023, acc: 0.4819999933242798, val_loss: 1.2326078414916992, val_acc: 0.4790000021457672,  lr: 0.009668263103448276
step: 1964, train_loss: 1.2167161703109741, acc: 0.4880000054836273, val_loss: 1.2262928485870361, val_acc: 0.47839999198913574,  lr: 0.009667918620689655
step: 1965, train_loss: 1.1775896549224854, acc: 0.49959999322891235, val_loss: 1.2246228456497192, val_acc: 0.49000000953674316,  lr: 0.009667574137931035
step: 1966, train_loss: 1.194601058959961, acc: 0.4968000054359436, val_loss: 1.2216644287109375, val_acc: 0.4844000041484833,  lr: 0.009667229655172414
step: 1967, train_loss: 1.2624443769454956, acc: 0.46619999408721924, val_loss: 1.2325814962387085, val_acc: 0.47540000081062317,  lr: 0.009666885172413793
step: 1968, train_loss: 1.187958836555481, acc: 0.5013999938964844, val_loss: 1.2235983610153198, val_acc: 0.4821999967098236,  lr: 0.009666540689655173
step: 1969, train_loss: 1.1764423847198486, acc: 0.5023999810218811, val_loss: 1.223611831665039, val_acc: 0.4772000014781952,  lr: 0.009666196206896552
step: 1970, train_loss: 1.1545898914337158, acc: 0.5171999931335449, val_loss: 1.2322360277175903, val_acc: 0.47099998593330383,  lr: 0.009665851724137932
step: 1971, train_loss: 1.191498875617981, acc: 0.4997999966144562, val_loss: 1.2391732931137085, val_acc: 0.4747999906539917,  lr: 0.009665507241379311
step: 1972, train_loss: 1.2510448694229126, acc: 0.47380000352859497, val_loss: 1.2382652759552002, val_acc: 0.47839999198913574,  lr: 0.00966516275862069
step: 1973, train_loss: 1.185976505279541, acc: 0.4991999864578247, val_loss: 1.2434393167495728, val_acc: 0.4781999886035919,  lr: 0.00966481827586207
step: 1974, train_loss: 1.1926952600479126, acc: 0.492000013589859, val_loss: 1.2337998151779175, val_acc: 0.4803999960422516,  lr: 0.009664473793103449
step: 1975, train_loss: 1.2797194719314575, acc: 0.4607999920845032, val_loss: 1.2275989055633545, val_acc: 0.4860000014305115,  lr: 0.009664129310344828
step: 1976, train_loss: 1.1711640357971191, acc: 0.5080000162124634, val_loss: 1.2318826913833618, val_acc: 0.4796000123023987,  lr: 0.009663784827586208
step: 1977, train_loss: 1.174364686012268, acc: 0.5109999775886536, val_loss: 1.2313899993896484, val_acc: 0.4729999899864197,  lr: 0.009663440344827587
step: 1978, train_loss: 1.2200316190719604, acc: 0.49459999799728394, val_loss: 1.2292076349258423, val_acc: 0.47600001096725464,  lr: 0.009663095862068965
step: 1979, train_loss: 1.2412221431732178, acc: 0.4724000096321106, val_loss: 1.2369831800460815, val_acc: 0.475600004196167,  lr: 0.009662751379310344
step: 1980, train_loss: 1.1686561107635498, acc: 0.498199999332428, val_loss: 1.243685007095337, val_acc: 0.47279998660087585,  lr: 0.009662406896551724
step: 1981, train_loss: 1.1862510442733765, acc: 0.5031999945640564, val_loss: 1.2405753135681152, val_acc: 0.4675999879837036,  lr: 0.009662062413793103
step: 1982, train_loss: 1.272154688835144, acc: 0.46399998664855957, val_loss: 1.2498000860214233, val_acc: 0.4724000096321106,  lr: 0.009661717931034482
step: 1983, train_loss: 1.2517297267913818, acc: 0.4765999913215637, val_loss: 1.245697259902954, val_acc: 0.4691999852657318,  lr: 0.009661373448275862
step: 1984, train_loss: 1.154532551765442, acc: 0.5163999795913696, val_loss: 1.24177086353302, val_acc: 0.4690000116825104,  lr: 0.009661028965517241
step: 1985, train_loss: 1.1661361455917358, acc: 0.5094000101089478, val_loss: 1.2439466714859009, val_acc: 0.47119998931884766,  lr: 0.00966068448275862
step: 1986, train_loss: 1.1576595306396484, acc: 0.5072000026702881, val_loss: 1.2475908994674683, val_acc: 0.46459999680519104,  lr: 0.00966034
step: 1987, train_loss: 1.2787460088729858, acc: 0.4625999927520752, val_loss: 1.2475155591964722, val_acc: 0.4666000008583069,  lr: 0.00965999551724138
step: 1988, train_loss: 1.213396668434143, acc: 0.4959999918937683, val_loss: 1.2476277351379395, val_acc: 0.46880000829696655,  lr: 0.009659651034482759
step: 1989, train_loss: 1.271591067314148, acc: 0.4560000002384186, val_loss: 1.2441729307174683, val_acc: 0.47540000081062317,  lr: 0.009659306551724138
step: 1990, train_loss: 1.1807714700698853, acc: 0.5005999803543091, val_loss: 1.2383496761322021, val_acc: 0.4699999988079071,  lr: 0.009658962068965517
step: 1991, train_loss: 1.1581944227218628, acc: 0.5212000012397766, val_loss: 1.2307943105697632, val_acc: 0.4763999879360199,  lr: 0.009658617586206897
step: 1992, train_loss: 1.2840055227279663, acc: 0.448199987411499, val_loss: 1.2215949296951294, val_acc: 0.47920000553131104,  lr: 0.009658273103448276
step: 1993, train_loss: 1.17765474319458, acc: 0.5063999891281128, val_loss: 1.2176116704940796, val_acc: 0.4828000068664551,  lr: 0.009657928620689656
step: 1994, train_loss: 1.2358920574188232, acc: 0.4814000129699707, val_loss: 1.210479974746704, val_acc: 0.47699999809265137,  lr: 0.009657584137931035
step: 1995, train_loss: 1.246341586112976, acc: 0.46799999475479126, val_loss: 1.2184934616088867, val_acc: 0.4790000021457672,  lr: 0.009657239655172414
step: 1996, train_loss: 1.1653658151626587, acc: 0.5162000060081482, val_loss: 1.2107337713241577, val_acc: 0.4864000082015991,  lr: 0.009656895172413794
step: 1997, train_loss: 1.1634690761566162, acc: 0.5109999775886536, val_loss: 1.2079681158065796, val_acc: 0.48919999599456787,  lr: 0.009656550689655173
step: 1998, train_loss: 1.1363416910171509, acc: 0.525600016117096, val_loss: 1.2057206630706787, val_acc: 0.48559999465942383,  lr: 0.009656206206896552
step: 1999, train_loss: 1.2216533422470093, acc: 0.4862000048160553, val_loss: 1.2114877700805664, val_acc: 0.4747999906539917,  lr: 0.009655861724137932
step: 2000, train_loss: 1.1670904159545898, acc: 0.5131999850273132, val_loss: 1.2189675569534302, val_acc: 0.4814000129699707,  lr: 0.009655517241379311
step: 2001, train_loss: 1.2561088800430298, acc: 0.4787999987602234, val_loss: 1.2135651111602783, val_acc: 0.4805999994277954,  lr: 0.00965517275862069
step: 2002, train_loss: 1.1571460962295532, acc: 0.5095999836921692, val_loss: 1.2246215343475342, val_acc: 0.4772000014781952,  lr: 0.00965482827586207
step: 2003, train_loss: 1.2222238779067993, acc: 0.4832000136375427, val_loss: 1.2111952304840088, val_acc: 0.47940000891685486,  lr: 0.009654483793103448
step: 2004, train_loss: 1.2388744354248047, acc: 0.4772000014781952, val_loss: 1.2167190313339233, val_acc: 0.4805999994277954,  lr: 0.009654139310344827
step: 2005, train_loss: 1.2636014223098755, acc: 0.4641999900341034, val_loss: 1.2118278741836548, val_acc: 0.4918000102043152,  lr: 0.009653794827586206
step: 2006, train_loss: 1.126424789428711, acc: 0.526199996471405, val_loss: 1.2210694551467896, val_acc: 0.48559999465942383,  lr: 0.009653450344827586
step: 2007, train_loss: 1.1851059198379517, acc: 0.4975999891757965, val_loss: 1.2131915092468262, val_acc: 0.4869999885559082,  lr: 0.009653105862068965
step: 2008, train_loss: 1.2424578666687012, acc: 0.4772000014781952, val_loss: 1.2186625003814697, val_acc: 0.477400004863739,  lr: 0.009652761379310345
step: 2009, train_loss: 1.1680701971054077, acc: 0.504800021648407, val_loss: 1.2111306190490723, val_acc: 0.4781999886035919,  lr: 0.009652416896551724
step: 2010, train_loss: 1.1757720708847046, acc: 0.508400022983551, val_loss: 1.2163617610931396, val_acc: 0.47440001368522644,  lr: 0.009652072413793103
step: 2011, train_loss: 1.13123619556427, acc: 0.5321999788284302, val_loss: 1.2065255641937256, val_acc: 0.48019999265670776,  lr: 0.009651727931034483
step: 2012, train_loss: 1.1468875408172607, acc: 0.517799973487854, val_loss: 1.2032101154327393, val_acc: 0.48399999737739563,  lr: 0.009651383448275862
step: 2013, train_loss: 1.2296992540359497, acc: 0.4803999960422516, val_loss: 1.2106796503067017, val_acc: 0.4812000095844269,  lr: 0.009651038965517241
step: 2014, train_loss: 1.1104565858840942, acc: 0.5333999991416931, val_loss: 1.2065850496292114, val_acc: 0.48339998722076416,  lr: 0.00965069448275862
step: 2015, train_loss: 1.2376781702041626, acc: 0.47360000014305115, val_loss: 1.2062419652938843, val_acc: 0.48399999737739563,  lr: 0.00965035
step: 2016, train_loss: 1.1205166578292847, acc: 0.5375999808311462, val_loss: 1.2032506465911865, val_acc: 0.4832000136375427,  lr: 0.00965000551724138
step: 2017, train_loss: 1.1446033716201782, acc: 0.5220000147819519, val_loss: 1.210583209991455, val_acc: 0.48159998655319214,  lr: 0.009649661034482759
step: 2018, train_loss: 1.1654349565505981, acc: 0.5076000094413757, val_loss: 1.2115271091461182, val_acc: 0.4862000048160553,  lr: 0.009649316551724138
step: 2019, train_loss: 1.2589635848999023, acc: 0.4593999981880188, val_loss: 1.209106683731079, val_acc: 0.48420000076293945,  lr: 0.009648972068965518
step: 2020, train_loss: 1.1447523832321167, acc: 0.5212000012397766, val_loss: 1.2154505252838135, val_acc: 0.47859999537467957,  lr: 0.009648627586206897
step: 2021, train_loss: 1.1435015201568604, acc: 0.5202000141143799, val_loss: 1.2156953811645508, val_acc: 0.4796000123023987,  lr: 0.009648283103448277
step: 2022, train_loss: 1.2151755094528198, acc: 0.49320000410079956, val_loss: 1.2161974906921387, val_acc: 0.47999998927116394,  lr: 0.009647938620689656
step: 2023, train_loss: 1.160888910293579, acc: 0.5234000086784363, val_loss: 1.218684196472168, val_acc: 0.4864000082015991,  lr: 0.009647594137931035
step: 2024, train_loss: 1.2037334442138672, acc: 0.49559998512268066, val_loss: 1.2260195016860962, val_acc: 0.4819999933242798,  lr: 0.009647249655172415
step: 2025, train_loss: 1.269379734992981, acc: 0.460999995470047, val_loss: 1.2248541116714478, val_acc: 0.48260000348091125,  lr: 0.009646905172413794
step: 2026, train_loss: 1.166542649269104, acc: 0.5099999904632568, val_loss: 1.2336703538894653, val_acc: 0.47040000557899475,  lr: 0.009646560689655172
step: 2027, train_loss: 1.2324105501174927, acc: 0.4812000095844269, val_loss: 1.23391592502594, val_acc: 0.4722000062465668,  lr: 0.009646216206896551
step: 2028, train_loss: 1.158333420753479, acc: 0.515999972820282, val_loss: 1.2404111623764038, val_acc: 0.4675999879837036,  lr: 0.00964587172413793
step: 2029, train_loss: 1.1935113668441772, acc: 0.49140000343322754, val_loss: 1.2398120164871216, val_acc: 0.4805999994277954,  lr: 0.00964552724137931
step: 2030, train_loss: 1.207381010055542, acc: 0.49459999799728394, val_loss: 1.2379062175750732, val_acc: 0.4717999994754791,  lr: 0.00964518275862069
step: 2031, train_loss: 1.2982542514801025, acc: 0.45179998874664307, val_loss: 1.2407145500183105, val_acc: 0.4729999899864197,  lr: 0.009644838275862069
step: 2032, train_loss: 1.2264957427978516, acc: 0.49300000071525574, val_loss: 1.240699052810669, val_acc: 0.4763999879360199,  lr: 0.009644493793103448
step: 2033, train_loss: 1.1855205297470093, acc: 0.49219998717308044, val_loss: 1.2311850786209106, val_acc: 0.4832000136375427,  lr: 0.009644149310344827
step: 2034, train_loss: 1.1899025440216064, acc: 0.4973999857902527, val_loss: 1.2355828285217285, val_acc: 0.4796000123023987,  lr: 0.009643804827586207
step: 2035, train_loss: 1.171907901763916, acc: 0.5059999823570251, val_loss: 1.237929344177246, val_acc: 0.4821999967098236,  lr: 0.009643460344827586
step: 2036, train_loss: 1.2734295129776, acc: 0.460999995470047, val_loss: 1.2290483713150024, val_acc: 0.4803999960422516,  lr: 0.009643115862068966
step: 2037, train_loss: 1.1368659734725952, acc: 0.525600016117096, val_loss: 1.2137962579727173, val_acc: 0.4830000102519989,  lr: 0.009642771379310345
step: 2038, train_loss: 1.1785281896591187, acc: 0.5034000277519226, val_loss: 1.2143195867538452, val_acc: 0.4832000136375427,  lr: 0.009642426896551724
step: 2039, train_loss: 1.2405427694320679, acc: 0.47519999742507935, val_loss: 1.21453058719635, val_acc: 0.48339998722076416,  lr: 0.009642082413793104
step: 2040, train_loss: 1.1336612701416016, acc: 0.525600016117096, val_loss: 1.21589994430542, val_acc: 0.49000000953674316,  lr: 0.009641737931034483
step: 2041, train_loss: 1.2554517984390259, acc: 0.4607999920845032, val_loss: 1.2193083763122559, val_acc: 0.4918000102043152,  lr: 0.009641393448275862
step: 2042, train_loss: 1.1840455532073975, acc: 0.5013999938964844, val_loss: 1.2199535369873047, val_acc: 0.4878000020980835,  lr: 0.009641048965517242
step: 2043, train_loss: 1.1736605167388916, acc: 0.5094000101089478, val_loss: 1.2249410152435303, val_acc: 0.4790000021457672,  lr: 0.009640704482758621
step: 2044, train_loss: 1.167465090751648, acc: 0.5027999877929688, val_loss: 1.214743733406067, val_acc: 0.4851999878883362,  lr: 0.00964036
step: 2045, train_loss: 1.248916745185852, acc: 0.47119998931884766, val_loss: 1.2313870191574097, val_acc: 0.4781999886035919,  lr: 0.00964001551724138
step: 2046, train_loss: 1.1808773279190063, acc: 0.5034000277519226, val_loss: 1.2239230871200562, val_acc: 0.4781999886035919,  lr: 0.00963967103448276
step: 2047, train_loss: 1.1465328931808472, acc: 0.5152000188827515, val_loss: 1.2355719804763794, val_acc: 0.475600004196167,  lr: 0.009639326551724139
step: 2048, train_loss: 1.2875577211380005, acc: 0.4498000144958496, val_loss: 1.230541467666626, val_acc: 0.4763999879360199,  lr: 0.009638982068965518
step: 2049, train_loss: 1.1588613986968994, acc: 0.5090000033378601, val_loss: 1.2169026136398315, val_acc: 0.48919999599456787,  lr: 0.009638637586206898
step: 2050, train_loss: 1.2199207544326782, acc: 0.4787999987602234, val_loss: 1.2249889373779297, val_acc: 0.48739999532699585,  lr: 0.009638293103448277
step: 2051, train_loss: 1.2633785009384155, acc: 0.46779999136924744, val_loss: 1.2294126749038696, val_acc: 0.48159998655319214,  lr: 0.009637948620689655
step: 2052, train_loss: 1.1479194164276123, acc: 0.5138000249862671, val_loss: 1.2222551107406616, val_acc: 0.4844000041484833,  lr: 0.009637604137931034
step: 2053, train_loss: 1.18271803855896, acc: 0.49320000410079956, val_loss: 1.2234801054000854, val_acc: 0.4844000041484833,  lr: 0.009637259655172413
step: 2054, train_loss: 1.172807216644287, acc: 0.5052000284194946, val_loss: 1.2187303304672241, val_acc: 0.483599990606308,  lr: 0.009636915172413793
step: 2055, train_loss: 1.1578267812728882, acc: 0.5008000135421753, val_loss: 1.2160981893539429, val_acc: 0.4925999939441681,  lr: 0.009636570689655172
step: 2056, train_loss: 1.1557401418685913, acc: 0.5121999979019165, val_loss: 1.2244757413864136, val_acc: 0.4912000000476837,  lr: 0.009636226206896551
step: 2057, train_loss: 1.2072185277938843, acc: 0.4867999851703644, val_loss: 1.2339394092559814, val_acc: 0.4821999967098236,  lr: 0.00963588172413793
step: 2058, train_loss: 1.2193611860275269, acc: 0.4860000014305115, val_loss: 1.2254209518432617, val_acc: 0.4790000021457672,  lr: 0.00963553724137931
step: 2059, train_loss: 1.1528321504592896, acc: 0.510200023651123, val_loss: 1.2273893356323242, val_acc: 0.4787999987602234,  lr: 0.00963519275862069
step: 2060, train_loss: 1.1919161081314087, acc: 0.5044000148773193, val_loss: 1.2200562953948975, val_acc: 0.4772000014781952,  lr: 0.009634848275862069
step: 2061, train_loss: 1.1693998575210571, acc: 0.5080000162124634, val_loss: 1.2208523750305176, val_acc: 0.4851999878883362,  lr: 0.009634503793103448
step: 2062, train_loss: 1.2321609258651733, acc: 0.47519999742507935, val_loss: 1.2309300899505615, val_acc: 0.4828000068664551,  lr: 0.009634159310344828
step: 2063, train_loss: 1.2375154495239258, acc: 0.475600004196167, val_loss: 1.2287276983261108, val_acc: 0.48399999737739563,  lr: 0.009633814827586207
step: 2064, train_loss: 1.25339937210083, acc: 0.4729999899864197, val_loss: 1.2296112775802612, val_acc: 0.4790000021457672,  lr: 0.009633470344827587
step: 2065, train_loss: 1.2370946407318115, acc: 0.4796000123023987, val_loss: 1.2190932035446167, val_acc: 0.4848000109195709,  lr: 0.009633125862068966
step: 2066, train_loss: 1.2430683374404907, acc: 0.4706000089645386, val_loss: 1.2192145586013794, val_acc: 0.48260000348091125,  lr: 0.009632781379310345
step: 2067, train_loss: 1.217073678970337, acc: 0.4821999967098236, val_loss: 1.224703073501587, val_acc: 0.4837999939918518,  lr: 0.009632436896551725
step: 2068, train_loss: 1.2473584413528442, acc: 0.4650000035762787, val_loss: 1.2177574634552002, val_acc: 0.4867999851703644,  lr: 0.009632092413793104
step: 2069, train_loss: 1.1910959482192993, acc: 0.4966000020503998, val_loss: 1.2166792154312134, val_acc: 0.490200012922287,  lr: 0.009631747931034483
step: 2070, train_loss: 1.1814824342727661, acc: 0.49480000138282776, val_loss: 1.2193800210952759, val_acc: 0.4869999885559082,  lr: 0.009631403448275863
step: 2071, train_loss: 1.205635666847229, acc: 0.4887999892234802, val_loss: 1.2149871587753296, val_acc: 0.492000013589859,  lr: 0.009631058965517242
step: 2072, train_loss: 1.1988470554351807, acc: 0.4968000054359436, val_loss: 1.2097893953323364, val_acc: 0.49079999327659607,  lr: 0.009630714482758622
step: 2073, train_loss: 1.1607595682144165, acc: 0.5210000276565552, val_loss: 1.210258960723877, val_acc: 0.4918000102043152,  lr: 0.009630370000000001
step: 2074, train_loss: 1.2157765626907349, acc: 0.4828000068664551, val_loss: 1.2143305540084839, val_acc: 0.48899999260902405,  lr: 0.00963002551724138
step: 2075, train_loss: 1.2686052322387695, acc: 0.454800009727478, val_loss: 1.2203510999679565, val_acc: 0.48260000348091125,  lr: 0.009629681034482758
step: 2076, train_loss: 1.2238236665725708, acc: 0.4747999906539917, val_loss: 1.2197787761688232, val_acc: 0.48500001430511475,  lr: 0.009629336551724137
step: 2077, train_loss: 1.149387240409851, acc: 0.5170000195503235, val_loss: 1.215186357498169, val_acc: 0.48339998722076416,  lr: 0.009628992068965517
step: 2078, train_loss: 1.1884530782699585, acc: 0.5013999938964844, val_loss: 1.211560845375061, val_acc: 0.4903999865055084,  lr: 0.009628647586206896
step: 2079, train_loss: 1.1685158014297485, acc: 0.5049999952316284, val_loss: 1.2095657587051392, val_acc: 0.48159998655319214,  lr: 0.009628303103448276
step: 2080, train_loss: 1.2139590978622437, acc: 0.4812000095844269, val_loss: 1.2143021821975708, val_acc: 0.48579999804496765,  lr: 0.009627958620689655
step: 2081, train_loss: 1.1859166622161865, acc: 0.4975999891757965, val_loss: 1.221994161605835, val_acc: 0.48399999737739563,  lr: 0.009627614137931034
step: 2082, train_loss: 1.283213496208191, acc: 0.4560000002384186, val_loss: 1.2084192037582397, val_acc: 0.4880000054836273,  lr: 0.009627269655172414
step: 2083, train_loss: 1.200639247894287, acc: 0.4903999865055084, val_loss: 1.2067949771881104, val_acc: 0.4862000048160553,  lr: 0.009626925172413793
step: 2084, train_loss: 1.1559655666351318, acc: 0.5170000195503235, val_loss: 1.2011672258377075, val_acc: 0.4903999865055084,  lr: 0.009626580689655172
step: 2085, train_loss: 1.2044453620910645, acc: 0.49320000410079956, val_loss: 1.1958297491073608, val_acc: 0.4950000047683716,  lr: 0.009626236206896552
step: 2086, train_loss: 1.2157788276672363, acc: 0.4936000108718872, val_loss: 1.1973381042480469, val_acc: 0.49559998512268066,  lr: 0.009625891724137931
step: 2087, train_loss: 1.2050378322601318, acc: 0.49219998717308044, val_loss: 1.1897382736206055, val_acc: 0.4927999973297119,  lr: 0.00962554724137931
step: 2088, train_loss: 1.2190238237380981, acc: 0.487199991941452, val_loss: 1.1898257732391357, val_acc: 0.49900001287460327,  lr: 0.00962520275862069
step: 2089, train_loss: 1.183779239654541, acc: 0.5052000284194946, val_loss: 1.1980855464935303, val_acc: 0.4952000081539154,  lr: 0.00962485827586207
step: 2090, train_loss: 1.1843074560165405, acc: 0.5099999904632568, val_loss: 1.1998273134231567, val_acc: 0.4934000074863434,  lr: 0.009624513793103449
step: 2091, train_loss: 1.2318079471588135, acc: 0.47099998593330383, val_loss: 1.208651065826416, val_acc: 0.48660001158714294,  lr: 0.009624169310344828
step: 2092, train_loss: 1.160287857055664, acc: 0.517799973487854, val_loss: 1.19740629196167, val_acc: 0.49300000071525574,  lr: 0.009623824827586207
step: 2093, train_loss: 1.2104259729385376, acc: 0.4837999939918518, val_loss: 1.203616738319397, val_acc: 0.49799999594688416,  lr: 0.009623480344827587
step: 2094, train_loss: 1.2065788507461548, acc: 0.5094000101089478, val_loss: 1.1988017559051514, val_acc: 0.49779999256134033,  lr: 0.009623135862068966
step: 2095, train_loss: 1.281324028968811, acc: 0.4514000117778778, val_loss: 1.2038999795913696, val_acc: 0.4950000047683716,  lr: 0.009622791379310346
step: 2096, train_loss: 1.1820896863937378, acc: 0.5108000040054321, val_loss: 1.204195261001587, val_acc: 0.49079999327659607,  lr: 0.009622446896551725
step: 2097, train_loss: 1.2116824388504028, acc: 0.4878000020980835, val_loss: 1.214556336402893, val_acc: 0.48840001225471497,  lr: 0.009622102413793104
step: 2098, train_loss: 1.2345540523529053, acc: 0.47519999742507935, val_loss: 1.2072848081588745, val_acc: 0.48980000615119934,  lr: 0.009621757931034484
step: 2099, train_loss: 1.2573022842407227, acc: 0.4699999988079071, val_loss: 1.2150261402130127, val_acc: 0.48579999804496765,  lr: 0.009621413448275863
step: 2100, train_loss: 1.187072515487671, acc: 0.5022000074386597, val_loss: 1.2103692293167114, val_acc: 0.48559999465942383,  lr: 0.00962106896551724
step: 2101, train_loss: 1.212760329246521, acc: 0.4844000041484833, val_loss: 1.2101713418960571, val_acc: 0.4846000075340271,  lr: 0.00962072448275862
step: 2102, train_loss: 1.1919426918029785, acc: 0.49799999594688416, val_loss: 1.2177300453186035, val_acc: 0.48579999804496765,  lr: 0.00962038
step: 2103, train_loss: 1.208447813987732, acc: 0.4975999891757965, val_loss: 1.210894227027893, val_acc: 0.4896000027656555,  lr: 0.009620035517241379
step: 2104, train_loss: 1.159348726272583, acc: 0.5134000182151794, val_loss: 1.2137418985366821, val_acc: 0.49160000681877136,  lr: 0.009619691034482758
step: 2105, train_loss: 1.2275598049163818, acc: 0.47679999470710754, val_loss: 1.2108187675476074, val_acc: 0.4869999885559082,  lr: 0.009619346551724138
step: 2106, train_loss: 1.1821540594100952, acc: 0.4909999966621399, val_loss: 1.2154067754745483, val_acc: 0.487199991941452,  lr: 0.009619002068965517
step: 2107, train_loss: 1.1653852462768555, acc: 0.508400022983551, val_loss: 1.2185207605361938, val_acc: 0.4878000020980835,  lr: 0.009618657586206896
step: 2108, train_loss: 1.232446312904358, acc: 0.4844000041484833, val_loss: 1.2173901796340942, val_acc: 0.4846000075340271,  lr: 0.009618313103448276
step: 2109, train_loss: 1.1650125980377197, acc: 0.5139999985694885, val_loss: 1.2213085889816284, val_acc: 0.4781999886035919,  lr: 0.009617968620689655
step: 2110, train_loss: 1.1753214597702026, acc: 0.503600001335144, val_loss: 1.2243213653564453, val_acc: 0.48100000619888306,  lr: 0.009617624137931035
step: 2111, train_loss: 1.140384316444397, acc: 0.519599974155426, val_loss: 1.230479121208191, val_acc: 0.47040000557899475,  lr: 0.009617279655172414
step: 2112, train_loss: 1.1720941066741943, acc: 0.5031999945640564, val_loss: 1.227403163909912, val_acc: 0.4765999913215637,  lr: 0.009616935172413793
step: 2113, train_loss: 1.2802964448928833, acc: 0.45579999685287476, val_loss: 1.2171710729599, val_acc: 0.4887999892234802,  lr: 0.009616590689655173
step: 2114, train_loss: 1.1677217483520508, acc: 0.5034000277519226, val_loss: 1.2107124328613281, val_acc: 0.48919999599456787,  lr: 0.009616246206896552
step: 2115, train_loss: 1.206132173538208, acc: 0.4943999946117401, val_loss: 1.2079094648361206, val_acc: 0.4986000061035156,  lr: 0.009615901724137932
step: 2116, train_loss: 1.1994937658309937, acc: 0.49399998784065247, val_loss: 1.203373908996582, val_acc: 0.4991999864578247,  lr: 0.009615557241379311
step: 2117, train_loss: 1.1806087493896484, acc: 0.5105999708175659, val_loss: 1.1965460777282715, val_acc: 0.4973999857902527,  lr: 0.00961521275862069
step: 2118, train_loss: 1.1863068342208862, acc: 0.503600001335144, val_loss: 1.2069417238235474, val_acc: 0.48919999599456787,  lr: 0.00961486827586207
step: 2119, train_loss: 1.135249137878418, acc: 0.5206000208854675, val_loss: 1.2011401653289795, val_acc: 0.4943999946117401,  lr: 0.009614523793103449
step: 2120, train_loss: 1.1180745363235474, acc: 0.5264000296592712, val_loss: 1.2045966386795044, val_acc: 0.49300000071525574,  lr: 0.009614179310344828
step: 2121, train_loss: 1.1809473037719727, acc: 0.5019999742507935, val_loss: 1.200379729270935, val_acc: 0.4984000027179718,  lr: 0.009613834827586208
step: 2122, train_loss: 1.1629632711410522, acc: 0.5127999782562256, val_loss: 1.1983559131622314, val_acc: 0.501800000667572,  lr: 0.009613490344827587
step: 2123, train_loss: 1.2222075462341309, acc: 0.4830000102519989, val_loss: 1.214600682258606, val_acc: 0.4925999939441681,  lr: 0.009613145862068965
step: 2124, train_loss: 1.2191044092178345, acc: 0.48840001225471497, val_loss: 1.2166649103164673, val_acc: 0.4893999993801117,  lr: 0.009612801379310344
step: 2125, train_loss: 1.2788790464401245, acc: 0.4580000042915344, val_loss: 1.2194586992263794, val_acc: 0.4796000123023987,  lr: 0.009612456896551724
step: 2126, train_loss: 1.2704306840896606, acc: 0.46160000562667847, val_loss: 1.214587688446045, val_acc: 0.48159998655319214,  lr: 0.009612112413793103
step: 2127, train_loss: 1.1699129343032837, acc: 0.5049999952316284, val_loss: 1.2058448791503906, val_acc: 0.4885999858379364,  lr: 0.009611767931034482
step: 2128, train_loss: 1.157043695449829, acc: 0.5108000040054321, val_loss: 1.2111711502075195, val_acc: 0.4885999858379364,  lr: 0.009611423448275862
step: 2129, train_loss: 1.2174837589263916, acc: 0.48820000886917114, val_loss: 1.2089046239852905, val_acc: 0.4912000000476837,  lr: 0.009611078965517241
step: 2130, train_loss: 1.140726089477539, acc: 0.5202000141143799, val_loss: 1.2081691026687622, val_acc: 0.4925999939441681,  lr: 0.00961073448275862
step: 2131, train_loss: 1.223930835723877, acc: 0.47940000891685486, val_loss: 1.2102181911468506, val_acc: 0.492000013589859,  lr: 0.00961039
step: 2132, train_loss: 1.1714731454849243, acc: 0.5081999897956848, val_loss: 1.2035306692123413, val_acc: 0.48840001225471497,  lr: 0.00961004551724138
step: 2133, train_loss: 1.1206796169281006, acc: 0.5293999910354614, val_loss: 1.2013994455337524, val_acc: 0.490200012922287,  lr: 0.009609701034482759
step: 2134, train_loss: 1.1300268173217773, acc: 0.5249999761581421, val_loss: 1.19996976852417, val_acc: 0.49399998784065247,  lr: 0.009609356551724138
step: 2135, train_loss: 1.203734040260315, acc: 0.49880000948905945, val_loss: 1.206815481185913, val_acc: 0.49639999866485596,  lr: 0.009609012068965517
step: 2136, train_loss: 1.172881841659546, acc: 0.5094000101089478, val_loss: 1.2065848112106323, val_acc: 0.49320000410079956,  lr: 0.009608667586206897
step: 2137, train_loss: 1.168241024017334, acc: 0.5077999830245972, val_loss: 1.2031490802764893, val_acc: 0.4934000074863434,  lr: 0.009608323103448276
step: 2138, train_loss: 1.1765133142471313, acc: 0.5016000270843506, val_loss: 1.207115650177002, val_acc: 0.49300000071525574,  lr: 0.009607978620689656
step: 2139, train_loss: 1.2310683727264404, acc: 0.48019999265670776, val_loss: 1.2034871578216553, val_acc: 0.498199999332428,  lr: 0.009607634137931035
step: 2140, train_loss: 1.2623499631881714, acc: 0.46380001306533813, val_loss: 1.2018606662750244, val_acc: 0.49380001425743103,  lr: 0.009607289655172414
step: 2141, train_loss: 1.200799822807312, acc: 0.4975999891757965, val_loss: 1.1966992616653442, val_acc: 0.4966000020503998,  lr: 0.009606945172413794
step: 2142, train_loss: 1.1699074506759644, acc: 0.5027999877929688, val_loss: 1.1965274810791016, val_acc: 0.49559998512268066,  lr: 0.009606600689655173
step: 2143, train_loss: 1.134928822517395, acc: 0.5260000228881836, val_loss: 1.1952532529830933, val_acc: 0.49939998984336853,  lr: 0.009606256206896552
step: 2144, train_loss: 1.1529710292816162, acc: 0.5139999985694885, val_loss: 1.187531590461731, val_acc: 0.5019999742507935,  lr: 0.009605911724137932
step: 2145, train_loss: 1.1745280027389526, acc: 0.5098000168800354, val_loss: 1.1931055784225464, val_acc: 0.5,  lr: 0.009605567241379311
step: 2146, train_loss: 1.1414538621902466, acc: 0.517799973487854, val_loss: 1.1943681240081787, val_acc: 0.49619999527931213,  lr: 0.00960522275862069
step: 2147, train_loss: 1.1591756343841553, acc: 0.5153999924659729, val_loss: 1.192657709121704, val_acc: 0.4975999891757965,  lr: 0.00960487827586207
step: 2148, train_loss: 1.2122809886932373, acc: 0.4851999878883362, val_loss: 1.189207911491394, val_acc: 0.5044000148773193,  lr: 0.009604533793103448
step: 2149, train_loss: 1.1450600624084473, acc: 0.5203999876976013, val_loss: 1.187459111213684, val_acc: 0.503000020980835,  lr: 0.009604189310344827
step: 2150, train_loss: 1.24139404296875, acc: 0.47699999809265137, val_loss: 1.1824142932891846, val_acc: 0.5049999952316284,  lr: 0.009603844827586206
step: 2151, train_loss: 1.185783863067627, acc: 0.49619999527931213, val_loss: 1.1849911212921143, val_acc: 0.5077999830245972,  lr: 0.009603500344827586
step: 2152, train_loss: 1.2040355205535889, acc: 0.4918000102043152, val_loss: 1.1880874633789062, val_acc: 0.5090000033378601,  lr: 0.009603155862068965
step: 2153, train_loss: 1.2786182165145874, acc: 0.4519999921321869, val_loss: 1.1884009838104248, val_acc: 0.5055999755859375,  lr: 0.009602811379310345
step: 2154, train_loss: 1.1704790592193604, acc: 0.508400022983551, val_loss: 1.1915059089660645, val_acc: 0.4997999966144562,  lr: 0.009602466896551724
step: 2155, train_loss: 1.2113628387451172, acc: 0.4864000082015991, val_loss: 1.1944791078567505, val_acc: 0.49900001287460327,  lr: 0.009602122413793103
step: 2156, train_loss: 1.2355873584747314, acc: 0.4733999967575073, val_loss: 1.2096636295318604, val_acc: 0.4957999885082245,  lr: 0.009601777931034483
step: 2157, train_loss: 1.146776795387268, acc: 0.5171999931335449, val_loss: 1.1924892663955688, val_acc: 0.503000020980835,  lr: 0.009601433448275862
step: 2158, train_loss: 1.2699813842773438, acc: 0.46000000834465027, val_loss: 1.1985225677490234, val_acc: 0.4968000054359436,  lr: 0.009601088965517241
step: 2159, train_loss: 1.159118890762329, acc: 0.5049999952316284, val_loss: 1.1916722059249878, val_acc: 0.4970000088214874,  lr: 0.009600744482758621
step: 2160, train_loss: 1.197587251663208, acc: 0.49619999527931213, val_loss: 1.1982872486114502, val_acc: 0.4936000108718872,  lr: 0.0096004
step: 2161, train_loss: 1.1924971342086792, acc: 0.5055999755859375, val_loss: 1.2057573795318604, val_acc: 0.4934000074863434,  lr: 0.00960005551724138
step: 2162, train_loss: 1.2651642560958862, acc: 0.46459999680519104, val_loss: 1.199431300163269, val_acc: 0.49540001153945923,  lr: 0.009599711034482759
step: 2163, train_loss: 1.2304646968841553, acc: 0.4867999851703644, val_loss: 1.2008171081542969, val_acc: 0.5,  lr: 0.009599366551724138
step: 2164, train_loss: 1.2651424407958984, acc: 0.4620000123977661, val_loss: 1.195405125617981, val_acc: 0.49619999527931213,  lr: 0.009599022068965518
step: 2165, train_loss: 1.1513597965240479, acc: 0.521399974822998, val_loss: 1.198319435119629, val_acc: 0.4952000081539154,  lr: 0.009598677586206897
step: 2166, train_loss: 1.204468011856079, acc: 0.49799999594688416, val_loss: 1.2051022052764893, val_acc: 0.487199991941452,  lr: 0.009598333103448277
step: 2167, train_loss: 1.2590618133544922, acc: 0.4674000144004822, val_loss: 1.1917332410812378, val_acc: 0.49559998512268066,  lr: 0.009597988620689656
step: 2168, train_loss: 1.177621603012085, acc: 0.5080000162124634, val_loss: 1.2010835409164429, val_acc: 0.49939998984336853,  lr: 0.009597644137931035
step: 2169, train_loss: 1.1612493991851807, acc: 0.5095999836921692, val_loss: 1.2049680948257446, val_acc: 0.4966000020503998,  lr: 0.009597299655172415
step: 2170, train_loss: 1.253039836883545, acc: 0.4603999853134155, val_loss: 1.2076224088668823, val_acc: 0.49480000138282776,  lr: 0.009596955172413794
step: 2171, train_loss: 1.1892075538635254, acc: 0.4991999864578247, val_loss: 1.1997686624526978, val_acc: 0.5062000155448914,  lr: 0.009596610689655172
step: 2172, train_loss: 1.1874189376831055, acc: 0.5040000081062317, val_loss: 1.2030447721481323, val_acc: 0.4984000027179718,  lr: 0.009596266206896551
step: 2173, train_loss: 1.1305733919143677, acc: 0.5221999883651733, val_loss: 1.194623351097107, val_acc: 0.49540001153945923,  lr: 0.00959592172413793
step: 2174, train_loss: 1.2304911613464355, acc: 0.48100000619888306, val_loss: 1.191673755645752, val_acc: 0.4970000088214874,  lr: 0.00959557724137931
step: 2175, train_loss: 1.2075095176696777, acc: 0.49300000071525574, val_loss: 1.1901172399520874, val_acc: 0.4975999891757965,  lr: 0.00959523275862069
step: 2176, train_loss: 1.1756078004837036, acc: 0.5049999952316284, val_loss: 1.1964703798294067, val_acc: 0.4925999939441681,  lr: 0.009594888275862069
step: 2177, train_loss: 1.1434130668640137, acc: 0.5257999897003174, val_loss: 1.1935389041900635, val_acc: 0.49619999527931213,  lr: 0.009594543793103448
step: 2178, train_loss: 1.1638236045837402, acc: 0.5054000020027161, val_loss: 1.1939479112625122, val_acc: 0.4934000074863434,  lr: 0.009594199310344827
step: 2179, train_loss: 1.147202491760254, acc: 0.5167999863624573, val_loss: 1.2098785638809204, val_acc: 0.48559999465942383,  lr: 0.009593854827586207
step: 2180, train_loss: 1.2275470495224, acc: 0.475600004196167, val_loss: 1.2013753652572632, val_acc: 0.4862000048160553,  lr: 0.009593510344827586
step: 2181, train_loss: 1.2013145685195923, acc: 0.4997999966144562, val_loss: 1.2038710117340088, val_acc: 0.48899999260902405,  lr: 0.009593165862068966
step: 2182, train_loss: 1.1887798309326172, acc: 0.5012000203132629, val_loss: 1.2014025449752808, val_acc: 0.48840001225471497,  lr: 0.009592821379310345
step: 2183, train_loss: 1.2110480070114136, acc: 0.47380000352859497, val_loss: 1.2080082893371582, val_acc: 0.4887999892234802,  lr: 0.009592476896551724
step: 2184, train_loss: 1.2081986665725708, acc: 0.4941999912261963, val_loss: 1.2056626081466675, val_acc: 0.4943999946117401,  lr: 0.009592132413793104
step: 2185, train_loss: 1.1613199710845947, acc: 0.510200023651123, val_loss: 1.1994178295135498, val_acc: 0.49619999527931213,  lr: 0.009591787931034483
step: 2186, train_loss: 1.1546001434326172, acc: 0.5166000127792358, val_loss: 1.1931453943252563, val_acc: 0.4950000047683716,  lr: 0.009591443448275862
step: 2187, train_loss: 1.2623984813690186, acc: 0.46700000762939453, val_loss: 1.2022178173065186, val_acc: 0.49000000953674316,  lr: 0.009591098965517242
step: 2188, train_loss: 1.2113723754882812, acc: 0.49300000071525574, val_loss: 1.2074130773544312, val_acc: 0.4903999865055084,  lr: 0.009590754482758621
step: 2189, train_loss: 1.210486650466919, acc: 0.4837999939918518, val_loss: 1.214123010635376, val_acc: 0.48579999804496765,  lr: 0.00959041
step: 2190, train_loss: 1.2550315856933594, acc: 0.45980000495910645, val_loss: 1.2241556644439697, val_acc: 0.4862000048160553,  lr: 0.00959006551724138
step: 2191, train_loss: 1.2109524011611938, acc: 0.48660001158714294, val_loss: 1.2137491703033447, val_acc: 0.48840001225471497,  lr: 0.00958972103448276
step: 2192, train_loss: 1.191145420074463, acc: 0.5013999938964844, val_loss: 1.2188997268676758, val_acc: 0.48100000619888306,  lr: 0.009589376551724139
step: 2193, train_loss: 1.1679339408874512, acc: 0.5049999952316284, val_loss: 1.214766263961792, val_acc: 0.48559999465942383,  lr: 0.009589032068965518
step: 2194, train_loss: 1.1453206539154053, acc: 0.5198000073432922, val_loss: 1.2127376794815063, val_acc: 0.4934000074863434,  lr: 0.009588687586206898
step: 2195, train_loss: 1.1409050226211548, acc: 0.5212000012397766, val_loss: 1.2112956047058105, val_acc: 0.4934000074863434,  lr: 0.009588343103448277
step: 2196, train_loss: 1.1477717161178589, acc: 0.5148000121116638, val_loss: 1.2124055624008179, val_acc: 0.4893999993801117,  lr: 0.009587998620689655
step: 2197, train_loss: 1.2101283073425293, acc: 0.48559999465942383, val_loss: 1.2111716270446777, val_acc: 0.4887999892234802,  lr: 0.009587654137931034
step: 2198, train_loss: 1.2038415670394897, acc: 0.4909999966621399, val_loss: 1.2196629047393799, val_acc: 0.48500001430511475,  lr: 0.009587309655172413
step: 2199, train_loss: 1.1253830194473267, acc: 0.520799994468689, val_loss: 1.2195820808410645, val_acc: 0.4846000075340271,  lr: 0.009586965172413793
step: 2200, train_loss: 1.1245273351669312, acc: 0.5257999897003174, val_loss: 1.2132642269134521, val_acc: 0.48500001430511475,  lr: 0.009586620689655172
step: 2201, train_loss: 1.2081056833267212, acc: 0.4860000014305115, val_loss: 1.2113028764724731, val_acc: 0.48579999804496765,  lr: 0.009586276206896551
step: 2202, train_loss: 1.178757667541504, acc: 0.5073999762535095, val_loss: 1.2110191583633423, val_acc: 0.4860000014305115,  lr: 0.00958593172413793
step: 2203, train_loss: 1.131831169128418, acc: 0.5170000195503235, val_loss: 1.2183171510696411, val_acc: 0.4722000062465668,  lr: 0.00958558724137931
step: 2204, train_loss: 1.2016644477844238, acc: 0.48739999532699585, val_loss: 1.2027417421340942, val_acc: 0.4848000109195709,  lr: 0.00958524275862069
step: 2205, train_loss: 1.1994564533233643, acc: 0.49720001220703125, val_loss: 1.219254732131958, val_acc: 0.4830000102519989,  lr: 0.009584898275862069
step: 2206, train_loss: 1.1488839387893677, acc: 0.5166000127792358, val_loss: 1.2090437412261963, val_acc: 0.487199991941452,  lr: 0.009584553793103448
step: 2207, train_loss: 1.1493045091629028, acc: 0.5166000127792358, val_loss: 1.2122162580490112, val_acc: 0.48739999532699585,  lr: 0.009584209310344828
step: 2208, train_loss: 1.1487096548080444, acc: 0.5212000012397766, val_loss: 1.2102155685424805, val_acc: 0.4903999865055084,  lr: 0.009583864827586207
step: 2209, train_loss: 1.134508728981018, acc: 0.5293999910354614, val_loss: 1.2084532976150513, val_acc: 0.4848000109195709,  lr: 0.009583520344827587
step: 2210, train_loss: 1.2382677793502808, acc: 0.47099998593330383, val_loss: 1.208143949508667, val_acc: 0.4903999865055084,  lr: 0.009583175862068966
step: 2211, train_loss: 1.1185675859451294, acc: 0.525600016117096, val_loss: 1.2118229866027832, val_acc: 0.48500001430511475,  lr: 0.009582831379310345
step: 2212, train_loss: 1.229935884475708, acc: 0.48399999737739563, val_loss: 1.2276915311813354, val_acc: 0.4745999872684479,  lr: 0.009582486896551725
step: 2213, train_loss: 1.1908539533615112, acc: 0.5085999965667725, val_loss: 1.2211333513259888, val_acc: 0.4779999852180481,  lr: 0.009582142413793104
step: 2214, train_loss: 1.1887603998184204, acc: 0.49239999055862427, val_loss: 1.2293546199798584, val_acc: 0.4733999967575073,  lr: 0.009581797931034483
step: 2215, train_loss: 1.16009521484375, acc: 0.5123999714851379, val_loss: 1.2277480363845825, val_acc: 0.4814000129699707,  lr: 0.009581453448275863
step: 2216, train_loss: 1.2728383541107178, acc: 0.45159998536109924, val_loss: 1.2321829795837402, val_acc: 0.4797999858856201,  lr: 0.009581108965517242
step: 2217, train_loss: 1.2831013202667236, acc: 0.45579999685287476, val_loss: 1.2176204919815063, val_acc: 0.4837999939918518,  lr: 0.009580764482758622
step: 2218, train_loss: 1.2004872560501099, acc: 0.49239999055862427, val_loss: 1.2204772233963013, val_acc: 0.4851999878883362,  lr: 0.009580420000000001
step: 2219, train_loss: 1.195786476135254, acc: 0.5001999735832214, val_loss: 1.2144534587860107, val_acc: 0.48539999127388,  lr: 0.00958007551724138
step: 2220, train_loss: 1.145769715309143, acc: 0.5144000053405762, val_loss: 1.2244317531585693, val_acc: 0.48159998655319214,  lr: 0.009579731034482758
step: 2221, train_loss: 1.1398887634277344, acc: 0.5264000296592712, val_loss: 1.214881181716919, val_acc: 0.4832000136375427,  lr: 0.009579386551724137
step: 2222, train_loss: 1.130366563796997, acc: 0.5249999761581421, val_loss: 1.2149503231048584, val_acc: 0.48559999465942383,  lr: 0.009579042068965517
step: 2223, train_loss: 1.1561708450317383, acc: 0.5139999985694885, val_loss: 1.21293306350708, val_acc: 0.48919999599456787,  lr: 0.009578697586206896
step: 2224, train_loss: 1.2128996849060059, acc: 0.49380001425743103, val_loss: 1.220935583114624, val_acc: 0.4814000129699707,  lr: 0.009578353103448276
step: 2225, train_loss: 1.2521023750305176, acc: 0.47200000286102295, val_loss: 1.2039439678192139, val_acc: 0.4986000061035156,  lr: 0.009578008620689655
step: 2226, train_loss: 1.2056697607040405, acc: 0.5026000142097473, val_loss: 1.223812460899353, val_acc: 0.4903999865055084,  lr: 0.009577664137931034
step: 2227, train_loss: 1.2076244354248047, acc: 0.490200012922287, val_loss: 1.2117187976837158, val_acc: 0.4970000088214874,  lr: 0.009577319655172414
step: 2228, train_loss: 1.14414381980896, acc: 0.5238000154495239, val_loss: 1.209013819694519, val_acc: 0.4862000048160553,  lr: 0.009576975172413793
step: 2229, train_loss: 1.170531988143921, acc: 0.5058000087738037, val_loss: 1.213930606842041, val_acc: 0.4832000136375427,  lr: 0.009576630689655172
step: 2230, train_loss: 1.273470163345337, acc: 0.4641999900341034, val_loss: 1.2008323669433594, val_acc: 0.49140000343322754,  lr: 0.009576286206896552
step: 2231, train_loss: 1.1791192293167114, acc: 0.5081999897956848, val_loss: 1.1879626512527466, val_acc: 0.49939998984336853,  lr: 0.009575941724137931
step: 2232, train_loss: 1.165297508239746, acc: 0.5126000046730042, val_loss: 1.1955018043518066, val_acc: 0.49779999256134033,  lr: 0.00957559724137931
step: 2233, train_loss: 1.211410403251648, acc: 0.48260000348091125, val_loss: 1.196329116821289, val_acc: 0.4927999973297119,  lr: 0.00957525275862069
step: 2234, train_loss: 1.2552975416183472, acc: 0.4690000116825104, val_loss: 1.207754373550415, val_acc: 0.487199991941452,  lr: 0.00957490827586207
step: 2235, train_loss: 1.1713125705718994, acc: 0.5105999708175659, val_loss: 1.1964130401611328, val_acc: 0.4943999946117401,  lr: 0.009574563793103449
step: 2236, train_loss: 1.2435322999954224, acc: 0.4740000069141388, val_loss: 1.1889710426330566, val_acc: 0.49880000948905945,  lr: 0.009574219310344828
step: 2237, train_loss: 1.2131191492080688, acc: 0.4885999858379364, val_loss: 1.1858395338058472, val_acc: 0.49939998984336853,  lr: 0.009573874827586207
step: 2238, train_loss: 1.148682713508606, acc: 0.5108000040054321, val_loss: 1.1880450248718262, val_acc: 0.4973999857902527,  lr: 0.009573530344827587
step: 2239, train_loss: 1.208112120628357, acc: 0.49779999256134033, val_loss: 1.1942718029022217, val_acc: 0.4903999865055084,  lr: 0.009573185862068966
step: 2240, train_loss: 1.1793155670166016, acc: 0.5117999911308289, val_loss: 1.1965140104293823, val_acc: 0.4927999973297119,  lr: 0.009572841379310346
step: 2241, train_loss: 1.126518726348877, acc: 0.5267999768257141, val_loss: 1.1922907829284668, val_acc: 0.4952000081539154,  lr: 0.009572496896551725
step: 2242, train_loss: 1.1385765075683594, acc: 0.5216000080108643, val_loss: 1.1944751739501953, val_acc: 0.4918000102043152,  lr: 0.009572152413793104
step: 2243, train_loss: 1.1692615747451782, acc: 0.5085999965667725, val_loss: 1.19559907913208, val_acc: 0.4943999946117401,  lr: 0.009571807931034484
step: 2244, train_loss: 1.1333540678024292, acc: 0.5230000019073486, val_loss: 1.1908220052719116, val_acc: 0.4943999946117401,  lr: 0.009571463448275861
step: 2245, train_loss: 1.154554843902588, acc: 0.5126000046730042, val_loss: 1.201024055480957, val_acc: 0.48820000886917114,  lr: 0.00957111896551724
step: 2246, train_loss: 1.1685172319412231, acc: 0.5041999816894531, val_loss: 1.1987603902816772, val_acc: 0.49219998717308044,  lr: 0.00957077448275862
step: 2247, train_loss: 1.1448253393173218, acc: 0.5249999761581421, val_loss: 1.2064684629440308, val_acc: 0.48899999260902405,  lr: 0.00957043
step: 2248, train_loss: 1.1471710205078125, acc: 0.5180000066757202, val_loss: 1.2093324661254883, val_acc: 0.487199991941452,  lr: 0.009570085517241379
step: 2249, train_loss: 1.1413612365722656, acc: 0.5210000276565552, val_loss: 1.2062143087387085, val_acc: 0.487199991941452,  lr: 0.009569741034482758
step: 2250, train_loss: 1.112085223197937, acc: 0.5321999788284302, val_loss: 1.2078778743743896, val_acc: 0.48399999737739563,  lr: 0.009569396551724138
step: 2251, train_loss: 1.1824283599853516, acc: 0.5062000155448914, val_loss: 1.2109930515289307, val_acc: 0.48919999599456787,  lr: 0.009569052068965517
step: 2252, train_loss: 1.1663280725479126, acc: 0.5126000046730042, val_loss: 1.213613748550415, val_acc: 0.4909999966621399,  lr: 0.009568707586206896
step: 2253, train_loss: 1.1993911266326904, acc: 0.4918000102043152, val_loss: 1.213574767112732, val_acc: 0.48559999465942383,  lr: 0.009568363103448276
step: 2254, train_loss: 1.2567927837371826, acc: 0.4564000070095062, val_loss: 1.211963415145874, val_acc: 0.4878000020980835,  lr: 0.009568018620689655
step: 2255, train_loss: 1.2324903011322021, acc: 0.46720001101493835, val_loss: 1.2071201801300049, val_acc: 0.4903999865055084,  lr: 0.009567674137931035
step: 2256, train_loss: 1.2426382303237915, acc: 0.46959999203681946, val_loss: 1.2133303880691528, val_acc: 0.4860000014305115,  lr: 0.009567329655172414
step: 2257, train_loss: 1.131930947303772, acc: 0.5314000248908997, val_loss: 1.2043508291244507, val_acc: 0.49140000343322754,  lr: 0.009566985172413793
step: 2258, train_loss: 1.1528180837631226, acc: 0.5181999802589417, val_loss: 1.1986536979675293, val_acc: 0.49480000138282776,  lr: 0.009566640689655173
step: 2259, train_loss: 1.2327669858932495, acc: 0.46639999747276306, val_loss: 1.2018376588821411, val_acc: 0.49300000071525574,  lr: 0.009566296206896552
step: 2260, train_loss: 1.2034941911697388, acc: 0.49639999866485596, val_loss: 1.2050286531448364, val_acc: 0.49160000681877136,  lr: 0.009565951724137932
step: 2261, train_loss: 1.2758644819259644, acc: 0.4625999927520752, val_loss: 1.2036129236221313, val_acc: 0.4927999973297119,  lr: 0.009565607241379311
step: 2262, train_loss: 1.2025878429412842, acc: 0.48820000886917114, val_loss: 1.2027926445007324, val_acc: 0.4959999918937683,  lr: 0.00956526275862069
step: 2263, train_loss: 1.2083125114440918, acc: 0.48579999804496765, val_loss: 1.2018624544143677, val_acc: 0.4968000054359436,  lr: 0.00956491827586207
step: 2264, train_loss: 1.155745506286621, acc: 0.5130000114440918, val_loss: 1.211902379989624, val_acc: 0.4927999973297119,  lr: 0.009564573793103449
step: 2265, train_loss: 1.1730973720550537, acc: 0.5130000114440918, val_loss: 1.1979502439498901, val_acc: 0.49900001287460327,  lr: 0.009564229310344828
step: 2266, train_loss: 1.2126116752624512, acc: 0.48019999265670776, val_loss: 1.2021125555038452, val_acc: 0.4984000027179718,  lr: 0.009563884827586208
step: 2267, train_loss: 1.2482801675796509, acc: 0.46459999680519104, val_loss: 1.2018872499465942, val_acc: 0.49300000071525574,  lr: 0.009563540344827587
step: 2268, train_loss: 1.1798912286758423, acc: 0.501800000667572, val_loss: 1.202845573425293, val_acc: 0.49459999799728394,  lr: 0.009563195862068965
step: 2269, train_loss: 1.198733925819397, acc: 0.4952000081539154, val_loss: 1.2001633644104004, val_acc: 0.49559998512268066,  lr: 0.009562851379310344
step: 2270, train_loss: 1.1363252401351929, acc: 0.524399995803833, val_loss: 1.201399564743042, val_acc: 0.4936000108718872,  lr: 0.009562506896551724
step: 2271, train_loss: 1.126391887664795, acc: 0.5351999998092651, val_loss: 1.2071092128753662, val_acc: 0.49300000071525574,  lr: 0.009562162413793103
step: 2272, train_loss: 1.1351090669631958, acc: 0.5212000012397766, val_loss: 1.2022318840026855, val_acc: 0.4936000108718872,  lr: 0.009561817931034482
step: 2273, train_loss: 1.2483031749725342, acc: 0.47380000352859497, val_loss: 1.2051160335540771, val_acc: 0.49300000071525574,  lr: 0.009561473448275862
step: 2274, train_loss: 1.1064467430114746, acc: 0.5293999910354614, val_loss: 1.212720513343811, val_acc: 0.49059998989105225,  lr: 0.009561128965517241
step: 2275, train_loss: 1.2051889896392822, acc: 0.48899999260902405, val_loss: 1.212003469467163, val_acc: 0.4896000027656555,  lr: 0.00956078448275862
step: 2276, train_loss: 1.226792335510254, acc: 0.47859999537467957, val_loss: 1.2168439626693726, val_acc: 0.49059998989105225,  lr: 0.00956044
step: 2277, train_loss: 1.200142741203308, acc: 0.492000013589859, val_loss: 1.2129513025283813, val_acc: 0.4943999946117401,  lr: 0.00956009551724138
step: 2278, train_loss: 1.2080291509628296, acc: 0.49779999256134033, val_loss: 1.216627597808838, val_acc: 0.49140000343322754,  lr: 0.009559751034482759
step: 2279, train_loss: 1.1788221597671509, acc: 0.4975999891757965, val_loss: 1.2137564420700073, val_acc: 0.48840001225471497,  lr: 0.009559406551724138
step: 2280, train_loss: 1.231256365776062, acc: 0.4787999987602234, val_loss: 1.218797206878662, val_acc: 0.49219998717308044,  lr: 0.009559062068965517
step: 2281, train_loss: 1.2173893451690674, acc: 0.48660001158714294, val_loss: 1.2252928018569946, val_acc: 0.4848000109195709,  lr: 0.009558717586206897
step: 2282, train_loss: 1.182713508605957, acc: 0.501800000667572, val_loss: 1.2354538440704346, val_acc: 0.4706000089645386,  lr: 0.009558373103448276
step: 2283, train_loss: 1.1976910829544067, acc: 0.4912000000476837, val_loss: 1.2271910905838013, val_acc: 0.4697999954223633,  lr: 0.009558028620689656
step: 2284, train_loss: 1.201224446296692, acc: 0.4973999857902527, val_loss: 1.222343921661377, val_acc: 0.47940000891685486,  lr: 0.009557684137931035
step: 2285, train_loss: 1.1982980966567993, acc: 0.4950000047683716, val_loss: 1.2212308645248413, val_acc: 0.4772000014781952,  lr: 0.009557339655172414
step: 2286, train_loss: 1.1811448335647583, acc: 0.5009999871253967, val_loss: 1.2210216522216797, val_acc: 0.4772000014781952,  lr: 0.009556995172413794
step: 2287, train_loss: 1.2818212509155273, acc: 0.454800009727478, val_loss: 1.2166860103607178, val_acc: 0.4832000136375427,  lr: 0.009556650689655173
step: 2288, train_loss: 1.1990928649902344, acc: 0.490200012922287, val_loss: 1.2150465250015259, val_acc: 0.4812000095844269,  lr: 0.009556306206896552
step: 2289, train_loss: 1.153568148612976, acc: 0.517799973487854, val_loss: 1.2179157733917236, val_acc: 0.4796000123023987,  lr: 0.009555961724137932
step: 2290, train_loss: 1.2806774377822876, acc: 0.44839999079704285, val_loss: 1.2046356201171875, val_acc: 0.4885999858379364,  lr: 0.009555617241379311
step: 2291, train_loss: 1.1450912952423096, acc: 0.5166000127792358, val_loss: 1.20182466506958, val_acc: 0.49239999055862427,  lr: 0.00955527275862069
step: 2292, train_loss: 1.1592198610305786, acc: 0.522599995136261, val_loss: 1.199034333229065, val_acc: 0.48980000615119934,  lr: 0.00955492827586207
step: 2293, train_loss: 1.1939916610717773, acc: 0.4973999857902527, val_loss: 1.2053231000900269, val_acc: 0.4943999946117401,  lr: 0.009554583793103448
step: 2294, train_loss: 1.160416603088379, acc: 0.5052000284194946, val_loss: 1.2045414447784424, val_acc: 0.4909999966621399,  lr: 0.009554239310344827
step: 2295, train_loss: 1.1738684177398682, acc: 0.504800021648407, val_loss: 1.2123464345932007, val_acc: 0.4844000041484833,  lr: 0.009553894827586206
step: 2296, train_loss: 1.2656689882278442, acc: 0.46459999680519104, val_loss: 1.2137701511383057, val_acc: 0.48980000615119934,  lr: 0.009553550344827586
step: 2297, train_loss: 1.1184312105178833, acc: 0.5248000025749207, val_loss: 1.218259334564209, val_acc: 0.4893999993801117,  lr: 0.009553205862068965
step: 2298, train_loss: 1.1778204441070557, acc: 0.49619999527931213, val_loss: 1.2074213027954102, val_acc: 0.492000013589859,  lr: 0.009552861379310345
step: 2299, train_loss: 1.230835199356079, acc: 0.4860000014305115, val_loss: 1.2103819847106934, val_acc: 0.4936000108718872,  lr: 0.009552516896551724
step: 2300, train_loss: 1.1892887353897095, acc: 0.49399998784065247, val_loss: 1.2083981037139893, val_acc: 0.49799999594688416,  lr: 0.009552172413793103
step: 2301, train_loss: 1.2041386365890503, acc: 0.4936000108718872, val_loss: 1.2146339416503906, val_acc: 0.4950000047683716,  lr: 0.009551827931034483
step: 2302, train_loss: 1.231385588645935, acc: 0.4869999885559082, val_loss: 1.219066858291626, val_acc: 0.48260000348091125,  lr: 0.009551483448275862
step: 2303, train_loss: 1.2423514127731323, acc: 0.47099998593330383, val_loss: 1.2137832641601562, val_acc: 0.4848000109195709,  lr: 0.009551138965517241
step: 2304, train_loss: 1.2392311096191406, acc: 0.4733999967575073, val_loss: 1.2171400785446167, val_acc: 0.49059998989105225,  lr: 0.009550794482758621
step: 2305, train_loss: 1.251936912536621, acc: 0.46959999203681946, val_loss: 1.2129123210906982, val_acc: 0.49300000071525574,  lr: 0.00955045
step: 2306, train_loss: 1.209258794784546, acc: 0.4912000000476837, val_loss: 1.2132760286331177, val_acc: 0.4959999918937683,  lr: 0.00955010551724138
step: 2307, train_loss: 1.1669420003890991, acc: 0.5171999931335449, val_loss: 1.2089436054229736, val_acc: 0.49160000681877136,  lr: 0.009549761034482759
step: 2308, train_loss: 1.1705518960952759, acc: 0.5116000175476074, val_loss: 1.207121729850769, val_acc: 0.48820000886917114,  lr: 0.009549416551724138
step: 2309, train_loss: 1.165216326713562, acc: 0.515999972820282, val_loss: 1.2114529609680176, val_acc: 0.48159998655319214,  lr: 0.009549072068965518
step: 2310, train_loss: 1.1734373569488525, acc: 0.5004000067710876, val_loss: 1.2251737117767334, val_acc: 0.4887999892234802,  lr: 0.009548727586206897
step: 2311, train_loss: 1.1431362628936768, acc: 0.5189999938011169, val_loss: 1.226810336112976, val_acc: 0.48739999532699585,  lr: 0.009548383103448277
step: 2312, train_loss: 1.1663196086883545, acc: 0.5054000020027161, val_loss: 1.2283549308776855, val_acc: 0.47780001163482666,  lr: 0.009548038620689656
step: 2313, train_loss: 1.2130972146987915, acc: 0.487199991941452, val_loss: 1.2144564390182495, val_acc: 0.4875999987125397,  lr: 0.009547694137931035
step: 2314, train_loss: 1.1983839273452759, acc: 0.49959999322891235, val_loss: 1.2219921350479126, val_acc: 0.4844000041484833,  lr: 0.009547349655172415
step: 2315, train_loss: 1.2173776626586914, acc: 0.48420000076293945, val_loss: 1.2108395099639893, val_acc: 0.4851999878883362,  lr: 0.009547005172413794
step: 2316, train_loss: 1.1823461055755615, acc: 0.5080000162124634, val_loss: 1.2160099744796753, val_acc: 0.47999998927116394,  lr: 0.009546660689655172
step: 2317, train_loss: 1.1301969289779663, acc: 0.5267999768257141, val_loss: 1.2204080820083618, val_acc: 0.48100000619888306,  lr: 0.009546316206896551
step: 2318, train_loss: 1.1151682138442993, acc: 0.5320000052452087, val_loss: 1.2192420959472656, val_acc: 0.4860000014305115,  lr: 0.00954597172413793
step: 2319, train_loss: 1.1554077863693237, acc: 0.5202000141143799, val_loss: 1.2229164838790894, val_acc: 0.48820000886917114,  lr: 0.00954562724137931
step: 2320, train_loss: 1.1478787660598755, acc: 0.5170000195503235, val_loss: 1.2267217636108398, val_acc: 0.4830000102519989,  lr: 0.00954528275862069
step: 2321, train_loss: 1.1240479946136475, acc: 0.5260000228881836, val_loss: 1.226096272468567, val_acc: 0.4832000136375427,  lr: 0.009544938275862069
step: 2322, train_loss: 1.2035627365112305, acc: 0.49059998989105225, val_loss: 1.2208024263381958, val_acc: 0.47839999198913574,  lr: 0.009544593793103448
step: 2323, train_loss: 1.1829253435134888, acc: 0.4997999966144562, val_loss: 1.2326295375823975, val_acc: 0.4740000069141388,  lr: 0.009544249310344827
step: 2324, train_loss: 1.234897255897522, acc: 0.47600001096725464, val_loss: 1.2233364582061768, val_acc: 0.47780001163482666,  lr: 0.009543904827586207
step: 2325, train_loss: 1.1492588520050049, acc: 0.5175999999046326, val_loss: 1.2230768203735352, val_acc: 0.47440001368522644,  lr: 0.009543560344827586
step: 2326, train_loss: 1.2109884023666382, acc: 0.4862000048160553, val_loss: 1.2207146883010864, val_acc: 0.4805999994277954,  lr: 0.009543215862068966
step: 2327, train_loss: 1.1299431324005127, acc: 0.5130000114440918, val_loss: 1.227149486541748, val_acc: 0.47699999809265137,  lr: 0.009542871379310345
step: 2328, train_loss: 1.115013837814331, acc: 0.5293999910354614, val_loss: 1.2240798473358154, val_acc: 0.4724000096321106,  lr: 0.009542526896551724
step: 2329, train_loss: 1.2025818824768066, acc: 0.4893999993801117, val_loss: 1.223068118095398, val_acc: 0.48159998655319214,  lr: 0.009542182413793104
step: 2330, train_loss: 1.116667628288269, acc: 0.5275999903678894, val_loss: 1.2345538139343262, val_acc: 0.4749999940395355,  lr: 0.009541837931034483
step: 2331, train_loss: 1.165103554725647, acc: 0.5130000114440918, val_loss: 1.2354285717010498, val_acc: 0.4796000123023987,  lr: 0.009541493448275862
step: 2332, train_loss: 1.2596676349639893, acc: 0.46380001306533813, val_loss: 1.2420917749404907, val_acc: 0.48080000281333923,  lr: 0.009541148965517242
step: 2333, train_loss: 1.1991446018218994, acc: 0.48840001225471497, val_loss: 1.242902398109436, val_acc: 0.47600001096725464,  lr: 0.009540804482758621
step: 2334, train_loss: 1.2069660425186157, acc: 0.49320000410079956, val_loss: 1.2401679754257202, val_acc: 0.47679999470710754,  lr: 0.00954046
step: 2335, train_loss: 1.208785891532898, acc: 0.48339998722076416, val_loss: 1.2368541955947876, val_acc: 0.4749999940395355,  lr: 0.00954011551724138
step: 2336, train_loss: 1.2054942846298218, acc: 0.4927999973297119, val_loss: 1.243835210800171, val_acc: 0.46560001373291016,  lr: 0.00953977103448276
step: 2337, train_loss: 1.1202493906021118, acc: 0.5266000032424927, val_loss: 1.2418782711029053, val_acc: 0.4690000116825104,  lr: 0.009539426551724139
step: 2338, train_loss: 1.1780436038970947, acc: 0.5027999877929688, val_loss: 1.235261082649231, val_acc: 0.4749999940395355,  lr: 0.009539082068965518
step: 2339, train_loss: 1.1683847904205322, acc: 0.5116000175476074, val_loss: 1.2382575273513794, val_acc: 0.47620001435279846,  lr: 0.009538737586206898
step: 2340, train_loss: 1.2033573389053345, acc: 0.4970000088214874, val_loss: 1.239467740058899, val_acc: 0.4749999940395355,  lr: 0.009538393103448277
step: 2341, train_loss: 1.1607722043991089, acc: 0.5099999904632568, val_loss: 1.245280146598816, val_acc: 0.47540000081062317,  lr: 0.009538048620689655
step: 2342, train_loss: 1.1616559028625488, acc: 0.5080000162124634, val_loss: 1.253984808921814, val_acc: 0.47699999809265137,  lr: 0.009537704137931034
step: 2343, train_loss: 1.178684115409851, acc: 0.4966000020503998, val_loss: 1.243506908416748, val_acc: 0.47940000891685486,  lr: 0.009537359655172413
step: 2344, train_loss: 1.2514971494674683, acc: 0.4742000102996826, val_loss: 1.245949387550354, val_acc: 0.4740000069141388,  lr: 0.009537015172413793
step: 2345, train_loss: 1.1812033653259277, acc: 0.4952000081539154, val_loss: 1.2427127361297607, val_acc: 0.4659999907016754,  lr: 0.009536670689655172
step: 2346, train_loss: 1.3059139251708984, acc: 0.44440001249313354, val_loss: 1.2359486818313599, val_acc: 0.47940000891685486,  lr: 0.009536326206896551
step: 2347, train_loss: 1.1614928245544434, acc: 0.5166000127792358, val_loss: 1.2440497875213623, val_acc: 0.4763999879360199,  lr: 0.00953598172413793
step: 2348, train_loss: 1.2202085256576538, acc: 0.4860000014305115, val_loss: 1.236919641494751, val_acc: 0.4781999886035919,  lr: 0.00953563724137931
step: 2349, train_loss: 1.2238458395004272, acc: 0.4717999994754791, val_loss: 1.2315983772277832, val_acc: 0.475600004196167,  lr: 0.00953529275862069
step: 2350, train_loss: 1.1781784296035767, acc: 0.5076000094413757, val_loss: 1.2332367897033691, val_acc: 0.47440001368522644,  lr: 0.009534948275862069
step: 2351, train_loss: 1.2218308448791504, acc: 0.48500001430511475, val_loss: 1.2369017601013184, val_acc: 0.4758000075817108,  lr: 0.009534603793103448
step: 2352, train_loss: 1.1650761365890503, acc: 0.510200023651123, val_loss: 1.2244901657104492, val_acc: 0.48260000348091125,  lr: 0.009534259310344828
step: 2353, train_loss: 1.116676926612854, acc: 0.5216000080108643, val_loss: 1.2226417064666748, val_acc: 0.4862000048160553,  lr: 0.009533914827586207
step: 2354, train_loss: 1.1153148412704468, acc: 0.5315999984741211, val_loss: 1.2297883033752441, val_acc: 0.4812000095844269,  lr: 0.009533570344827587
step: 2355, train_loss: 1.1903798580169678, acc: 0.4927999973297119, val_loss: 1.2309397459030151, val_acc: 0.4844000041484833,  lr: 0.009533225862068966
step: 2356, train_loss: 1.260210394859314, acc: 0.46459999680519104, val_loss: 1.226590633392334, val_acc: 0.4814000129699707,  lr: 0.009532881379310345
step: 2357, train_loss: 1.16533625125885, acc: 0.5037999749183655, val_loss: 1.2198926210403442, val_acc: 0.49000000953674316,  lr: 0.009532536896551725
step: 2358, train_loss: 1.142068862915039, acc: 0.5149999856948853, val_loss: 1.2193169593811035, val_acc: 0.49380001425743103,  lr: 0.009532192413793104
step: 2359, train_loss: 1.2387365102767944, acc: 0.47620001435279846, val_loss: 1.2197835445404053, val_acc: 0.4936000108718872,  lr: 0.009531847931034483
step: 2360, train_loss: 1.134634256362915, acc: 0.5299999713897705, val_loss: 1.2199922800064087, val_acc: 0.49300000071525574,  lr: 0.009531503448275863
step: 2361, train_loss: 1.199278473854065, acc: 0.48899999260902405, val_loss: 1.222427487373352, val_acc: 0.487199991941452,  lr: 0.009531158965517242
step: 2362, train_loss: 1.1513259410858154, acc: 0.5180000066757202, val_loss: 1.2228693962097168, val_acc: 0.48919999599456787,  lr: 0.009530814482758622
step: 2363, train_loss: 1.2071611881256104, acc: 0.4878000020980835, val_loss: 1.2185304164886475, val_acc: 0.4844000041484833,  lr: 0.009530470000000001
step: 2364, train_loss: 1.1767696142196655, acc: 0.5034000277519226, val_loss: 1.216455340385437, val_acc: 0.4869999885559082,  lr: 0.009530125517241379
step: 2365, train_loss: 1.2520912885665894, acc: 0.4742000102996826, val_loss: 1.2222707271575928, val_acc: 0.48399999737739563,  lr: 0.009529781034482758
step: 2366, train_loss: 1.2039012908935547, acc: 0.4925999939441681, val_loss: 1.2233564853668213, val_acc: 0.4772000014781952,  lr: 0.009529436551724137
step: 2367, train_loss: 1.1500169038772583, acc: 0.5181999802589417, val_loss: 1.2207098007202148, val_acc: 0.4812000095844269,  lr: 0.009529092068965517
step: 2368, train_loss: 1.2303134202957153, acc: 0.47920000553131104, val_loss: 1.215156078338623, val_acc: 0.49559998512268066,  lr: 0.009528747586206896
step: 2369, train_loss: 1.1847686767578125, acc: 0.5031999945640564, val_loss: 1.222965121269226, val_acc: 0.48559999465942383,  lr: 0.009528403103448276
step: 2370, train_loss: 1.1506801843643188, acc: 0.5224000215530396, val_loss: 1.2112693786621094, val_acc: 0.4950000047683716,  lr: 0.009528058620689655
step: 2371, train_loss: 1.2250601053237915, acc: 0.4747999906539917, val_loss: 1.2133249044418335, val_acc: 0.4864000082015991,  lr: 0.009527714137931034
step: 2372, train_loss: 1.173946499824524, acc: 0.508400022983551, val_loss: 1.2189420461654663, val_acc: 0.4837999939918518,  lr: 0.009527369655172414
step: 2373, train_loss: 1.1734719276428223, acc: 0.5072000026702881, val_loss: 1.2215425968170166, val_acc: 0.49000000953674316,  lr: 0.009527025172413793
step: 2374, train_loss: 1.1352812051773071, acc: 0.5221999883651733, val_loss: 1.2191277742385864, val_acc: 0.4864000082015991,  lr: 0.009526680689655172
step: 2375, train_loss: 1.1514712572097778, acc: 0.5145999789237976, val_loss: 1.2130956649780273, val_acc: 0.490200012922287,  lr: 0.009526336206896552
step: 2376, train_loss: 1.1577389240264893, acc: 0.5145999789237976, val_loss: 1.2227966785430908, val_acc: 0.48100000619888306,  lr: 0.009525991724137931
step: 2377, train_loss: 1.2071573734283447, acc: 0.4896000027656555, val_loss: 1.2114309072494507, val_acc: 0.48579999804496765,  lr: 0.00952564724137931
step: 2378, train_loss: 1.1747350692749023, acc: 0.5016000270843506, val_loss: 1.2157949209213257, val_acc: 0.48820000886917114,  lr: 0.00952530275862069
step: 2379, train_loss: 1.124473214149475, acc: 0.5275999903678894, val_loss: 1.2132012844085693, val_acc: 0.48559999465942383,  lr: 0.00952495827586207
step: 2380, train_loss: 1.1467703580856323, acc: 0.522599995136261, val_loss: 1.2132121324539185, val_acc: 0.4878000020980835,  lr: 0.009524613793103449
step: 2381, train_loss: 1.2664551734924316, acc: 0.45820000767707825, val_loss: 1.2178857326507568, val_acc: 0.4875999987125397,  lr: 0.009524269310344828
step: 2382, train_loss: 1.2775102853775024, acc: 0.46140000224113464, val_loss: 1.2125749588012695, val_acc: 0.4903999865055084,  lr: 0.009523924827586207
step: 2383, train_loss: 1.1946961879730225, acc: 0.49619999527931213, val_loss: 1.1947062015533447, val_acc: 0.5045999884605408,  lr: 0.009523580344827587
step: 2384, train_loss: 1.1599737405776978, acc: 0.5072000026702881, val_loss: 1.2037220001220703, val_acc: 0.49219998717308044,  lr: 0.009523235862068966
step: 2385, train_loss: 1.1636607646942139, acc: 0.5153999924659729, val_loss: 1.1967679262161255, val_acc: 0.49140000343322754,  lr: 0.009522891379310346
step: 2386, train_loss: 1.2184661626815796, acc: 0.4887999892234802, val_loss: 1.2015254497528076, val_acc: 0.5063999891281128,  lr: 0.009522546896551725
step: 2387, train_loss: 1.252791166305542, acc: 0.47099998593330383, val_loss: 1.2017210721969604, val_acc: 0.5026000142097473,  lr: 0.009522202413793104
step: 2388, train_loss: 1.1862415075302124, acc: 0.5022000074386597, val_loss: 1.2063339948654175, val_acc: 0.4959999918937683,  lr: 0.009521857931034484
step: 2389, train_loss: 1.1518292427062988, acc: 0.5094000101089478, val_loss: 1.2023906707763672, val_acc: 0.49959999322891235,  lr: 0.009521513448275861
step: 2390, train_loss: 1.1725822687149048, acc: 0.5072000026702881, val_loss: 1.2077534198760986, val_acc: 0.49639999866485596,  lr: 0.00952116896551724
step: 2391, train_loss: 1.1960183382034302, acc: 0.4986000061035156, val_loss: 1.2077643871307373, val_acc: 0.4943999946117401,  lr: 0.00952082448275862
step: 2392, train_loss: 1.1427037715911865, acc: 0.5275999903678894, val_loss: 1.204993724822998, val_acc: 0.4950000047683716,  lr: 0.00952048
step: 2393, train_loss: 1.1480231285095215, acc: 0.5088000297546387, val_loss: 1.2094162702560425, val_acc: 0.4959999918937683,  lr: 0.009520135517241379
step: 2394, train_loss: 1.1226236820220947, acc: 0.5288000106811523, val_loss: 1.2069164514541626, val_acc: 0.49380001425743103,  lr: 0.009519791034482758
step: 2395, train_loss: 1.160130500793457, acc: 0.5192000269889832, val_loss: 1.2006844282150269, val_acc: 0.49619999527931213,  lr: 0.009519446551724138
step: 2396, train_loss: 1.2393088340759277, acc: 0.48899999260902405, val_loss: 1.1963354349136353, val_acc: 0.5008000135421753,  lr: 0.009519102068965517
step: 2397, train_loss: 1.2795895338058472, acc: 0.4596000015735626, val_loss: 1.2009265422821045, val_acc: 0.5012000203132629,  lr: 0.009518757586206896
step: 2398, train_loss: 1.1332956552505493, acc: 0.5284000039100647, val_loss: 1.197478175163269, val_acc: 0.49779999256134033,  lr: 0.009518413103448276
step: 2399, train_loss: 1.1912028789520264, acc: 0.5023999810218811, val_loss: 1.1985844373703003, val_acc: 0.4966000020503998,  lr: 0.009518068620689655
step: 2400, train_loss: 1.1282597780227661, acc: 0.5235999822616577, val_loss: 1.1921552419662476, val_acc: 0.4997999966144562,  lr: 0.009517724137931035
step: 2401, train_loss: 1.196699857711792, acc: 0.4943999946117401, val_loss: 1.197556495666504, val_acc: 0.49939998984336853,  lr: 0.009517379655172414
step: 2402, train_loss: 1.2142795324325562, acc: 0.4878000020980835, val_loss: 1.190858006477356, val_acc: 0.4997999966144562,  lr: 0.009517035172413793
step: 2403, train_loss: 1.2364352941513062, acc: 0.46720001101493835, val_loss: 1.1903096437454224, val_acc: 0.5012000203132629,  lr: 0.009516690689655173
step: 2404, train_loss: 1.1961747407913208, acc: 0.4984000027179718, val_loss: 1.18435800075531, val_acc: 0.5080000162124634,  lr: 0.009516346206896552
step: 2405, train_loss: 1.15629243850708, acc: 0.5220000147819519, val_loss: 1.1877517700195312, val_acc: 0.501800000667572,  lr: 0.009516001724137932
step: 2406, train_loss: 1.2063316106796265, acc: 0.49239999055862427, val_loss: 1.1895095109939575, val_acc: 0.503000020980835,  lr: 0.009515657241379311
step: 2407, train_loss: 1.2214642763137817, acc: 0.47839999198913574, val_loss: 1.1861165761947632, val_acc: 0.5095999836921692,  lr: 0.00951531275862069
step: 2408, train_loss: 1.1526947021484375, acc: 0.517799973487854, val_loss: 1.1890418529510498, val_acc: 0.5090000033378601,  lr: 0.00951496827586207
step: 2409, train_loss: 1.1279929876327515, acc: 0.521399974822998, val_loss: 1.1911120414733887, val_acc: 0.5094000101089478,  lr: 0.009514623793103449
step: 2410, train_loss: 1.1732277870178223, acc: 0.5123999714851379, val_loss: 1.1883553266525269, val_acc: 0.5067999958992004,  lr: 0.009514279310344828
step: 2411, train_loss: 1.1482341289520264, acc: 0.515999972820282, val_loss: 1.1888631582260132, val_acc: 0.5080000162124634,  lr: 0.009513934827586208
step: 2412, train_loss: 1.1517152786254883, acc: 0.5145999789237976, val_loss: 1.1908259391784668, val_acc: 0.5094000101089478,  lr: 0.009513590344827587
step: 2413, train_loss: 1.1308293342590332, acc: 0.5297999978065491, val_loss: 1.1970751285552979, val_acc: 0.4957999885082245,  lr: 0.009513245862068965
step: 2414, train_loss: 1.190335988998413, acc: 0.4862000048160553, val_loss: 1.1908475160598755, val_acc: 0.49799999594688416,  lr: 0.009512901379310344
step: 2415, train_loss: 1.1160151958465576, acc: 0.5338000059127808, val_loss: 1.192946434020996, val_acc: 0.49380001425743103,  lr: 0.009512556896551724
step: 2416, train_loss: 1.2542166709899902, acc: 0.4602000117301941, val_loss: 1.1920528411865234, val_acc: 0.4970000088214874,  lr: 0.009512212413793103
step: 2417, train_loss: 1.138385534286499, acc: 0.5153999924659729, val_loss: 1.1903533935546875, val_acc: 0.5022000074386597,  lr: 0.009511867931034482
step: 2418, train_loss: 1.2206367254257202, acc: 0.48260000348091125, val_loss: 1.1970984935760498, val_acc: 0.49900001287460327,  lr: 0.009511523448275862
step: 2419, train_loss: 1.0928305387496948, acc: 0.5406000018119812, val_loss: 1.1945801973342896, val_acc: 0.5012000203132629,  lr: 0.009511178965517241
step: 2420, train_loss: 1.1823116540908813, acc: 0.5009999871253967, val_loss: 1.2053557634353638, val_acc: 0.49959999322891235,  lr: 0.00951083448275862
step: 2421, train_loss: 1.1807680130004883, acc: 0.5041999816894531, val_loss: 1.2044936418533325, val_acc: 0.4918000102043152,  lr: 0.00951049
step: 2422, train_loss: 1.2024646997451782, acc: 0.49880000948905945, val_loss: 1.2111198902130127, val_acc: 0.4927999973297119,  lr: 0.00951014551724138
step: 2423, train_loss: 1.1915398836135864, acc: 0.49799999594688416, val_loss: 1.2161145210266113, val_acc: 0.48919999599456787,  lr: 0.009509801034482759
step: 2424, train_loss: 1.2297909259796143, acc: 0.48579999804496765, val_loss: 1.2122271060943604, val_acc: 0.4918000102043152,  lr: 0.009509456551724138
step: 2425, train_loss: 1.1233779191970825, acc: 0.5248000025749207, val_loss: 1.2060236930847168, val_acc: 0.49160000681877136,  lr: 0.009509112068965517
step: 2426, train_loss: 1.1341665983200073, acc: 0.5264000296592712, val_loss: 1.2029767036437988, val_acc: 0.49320000410079956,  lr: 0.009508767586206897
step: 2427, train_loss: 1.1408146619796753, acc: 0.5163999795913696, val_loss: 1.2008031606674194, val_acc: 0.5,  lr: 0.009508423103448276
step: 2428, train_loss: 1.2099411487579346, acc: 0.4878000020980835, val_loss: 1.2092115879058838, val_acc: 0.49140000343322754,  lr: 0.009508078620689656
step: 2429, train_loss: 1.1709915399551392, acc: 0.5037999749183655, val_loss: 1.2038390636444092, val_acc: 0.48559999465942383,  lr: 0.009507734137931035
step: 2430, train_loss: 1.1535614728927612, acc: 0.5148000121116638, val_loss: 1.2065342664718628, val_acc: 0.4909999966621399,  lr: 0.009507389655172414
step: 2431, train_loss: 1.1725753545761108, acc: 0.5073999762535095, val_loss: 1.2079449892044067, val_acc: 0.49459999799728394,  lr: 0.009507045172413794
step: 2432, train_loss: 1.116072416305542, acc: 0.5278000235557556, val_loss: 1.2057136297225952, val_acc: 0.49219998717308044,  lr: 0.009506700689655173
step: 2433, train_loss: 1.1389453411102295, acc: 0.5202000141143799, val_loss: 1.20514976978302, val_acc: 0.48260000348091125,  lr: 0.009506356206896553
step: 2434, train_loss: 1.1554956436157227, acc: 0.5167999863624573, val_loss: 1.2093757390975952, val_acc: 0.48559999465942383,  lr: 0.009506011724137932
step: 2435, train_loss: 1.131544589996338, acc: 0.5252000093460083, val_loss: 1.2143505811691284, val_acc: 0.48980000615119934,  lr: 0.009505667241379311
step: 2436, train_loss: 1.1513957977294922, acc: 0.5174000263214111, val_loss: 1.2159309387207031, val_acc: 0.49160000681877136,  lr: 0.00950532275862069
step: 2437, train_loss: 1.1085916757583618, acc: 0.532800018787384, val_loss: 1.213400959968567, val_acc: 0.4903999865055084,  lr: 0.00950497827586207
step: 2438, train_loss: 1.109824538230896, acc: 0.5315999984741211, val_loss: 1.2128725051879883, val_acc: 0.4896000027656555,  lr: 0.009504633793103448
step: 2439, train_loss: 1.18454909324646, acc: 0.5073999762535095, val_loss: 1.2144438028335571, val_acc: 0.4912000000476837,  lr: 0.009504289310344827
step: 2440, train_loss: 1.1174817085266113, acc: 0.54339998960495, val_loss: 1.2119814157485962, val_acc: 0.4912000000476837,  lr: 0.009503944827586206
step: 2441, train_loss: 1.1248905658721924, acc: 0.519599974155426, val_loss: 1.2137776613235474, val_acc: 0.4896000027656555,  lr: 0.009503600344827586
step: 2442, train_loss: 1.083346962928772, acc: 0.5532000064849854, val_loss: 1.2074276208877563, val_acc: 0.4867999851703644,  lr: 0.009503255862068965
step: 2443, train_loss: 1.266830325126648, acc: 0.46540001034736633, val_loss: 1.2024322748184204, val_acc: 0.4934000074863434,  lr: 0.009502911379310345
step: 2444, train_loss: 1.13529372215271, acc: 0.524399995803833, val_loss: 1.2037618160247803, val_acc: 0.4952000081539154,  lr: 0.009502566896551724
step: 2445, train_loss: 1.1656790971755981, acc: 0.5103999972343445, val_loss: 1.20438814163208, val_acc: 0.490200012922287,  lr: 0.009502222413793103
step: 2446, train_loss: 1.0790308713912964, acc: 0.5479999780654907, val_loss: 1.2104884386062622, val_acc: 0.483599990606308,  lr: 0.009501877931034483
step: 2447, train_loss: 1.169390082359314, acc: 0.5045999884605408, val_loss: 1.2075406312942505, val_acc: 0.48559999465942383,  lr: 0.009501533448275862
step: 2448, train_loss: 1.176465392112732, acc: 0.5044000148773193, val_loss: 1.2103887796401978, val_acc: 0.4862000048160553,  lr: 0.009501188965517241
step: 2449, train_loss: 1.1654222011566162, acc: 0.5070000290870667, val_loss: 1.2010469436645508, val_acc: 0.4918000102043152,  lr: 0.009500844482758621
step: 2450, train_loss: 1.1374845504760742, acc: 0.5134000182151794, val_loss: 1.2082620859146118, val_acc: 0.4909999966621399,  lr: 0.0095005
step: 2451, train_loss: 1.1519858837127686, acc: 0.5145999789237976, val_loss: 1.1989315748214722, val_acc: 0.49239999055862427,  lr: 0.00950015551724138
step: 2452, train_loss: 1.2075262069702148, acc: 0.49300000071525574, val_loss: 1.2021838426589966, val_acc: 0.49000000953674316,  lr: 0.009499811034482759
step: 2453, train_loss: 1.1363312005996704, acc: 0.5253999829292297, val_loss: 1.2086554765701294, val_acc: 0.49160000681877136,  lr: 0.009499466551724138
step: 2454, train_loss: 1.1749173402786255, acc: 0.5090000033378601, val_loss: 1.2031654119491577, val_acc: 0.49000000953674316,  lr: 0.009499122068965518
step: 2455, train_loss: 1.224042534828186, acc: 0.47679999470710754, val_loss: 1.2104281187057495, val_acc: 0.48660001158714294,  lr: 0.009498777586206897
step: 2456, train_loss: 1.2411950826644897, acc: 0.4812000095844269, val_loss: 1.21629798412323, val_acc: 0.4821999967098236,  lr: 0.009498433103448277
step: 2457, train_loss: 1.2781031131744385, acc: 0.4575999975204468, val_loss: 1.2034562826156616, val_acc: 0.49079999327659607,  lr: 0.009498088620689656
step: 2458, train_loss: 1.1849281787872314, acc: 0.5072000026702881, val_loss: 1.2061028480529785, val_acc: 0.4918000102043152,  lr: 0.009497744137931035
step: 2459, train_loss: 1.1768375635147095, acc: 0.5055999755859375, val_loss: 1.199552059173584, val_acc: 0.49540001153945923,  lr: 0.009497399655172415
step: 2460, train_loss: 1.1659451723098755, acc: 0.5073999762535095, val_loss: 1.2067900896072388, val_acc: 0.4909999966621399,  lr: 0.009497055172413794
step: 2461, train_loss: 1.1867239475250244, acc: 0.503000020980835, val_loss: 1.2092382907867432, val_acc: 0.49079999327659607,  lr: 0.009496710689655172
step: 2462, train_loss: 1.272586464881897, acc: 0.459199994802475, val_loss: 1.2081806659698486, val_acc: 0.4927999973297119,  lr: 0.009496366206896551
step: 2463, train_loss: 1.2226048707962036, acc: 0.48100000619888306, val_loss: 1.2090873718261719, val_acc: 0.49140000343322754,  lr: 0.00949602172413793
step: 2464, train_loss: 1.1508558988571167, acc: 0.5126000046730042, val_loss: 1.2135852575302124, val_acc: 0.4869999885559082,  lr: 0.00949567724137931
step: 2465, train_loss: 1.1569664478302002, acc: 0.5098000168800354, val_loss: 1.1993402242660522, val_acc: 0.48500001430511475,  lr: 0.00949533275862069
step: 2466, train_loss: 1.1378450393676758, acc: 0.526199996471405, val_loss: 1.1989095211029053, val_acc: 0.49480000138282776,  lr: 0.009494988275862069
step: 2467, train_loss: 1.2507480382919312, acc: 0.4749999940395355, val_loss: 1.1896653175354004, val_acc: 0.49799999594688416,  lr: 0.009494643793103448
step: 2468, train_loss: 1.1497893333435059, acc: 0.5171999931335449, val_loss: 1.1885836124420166, val_acc: 0.5023999810218811,  lr: 0.009494299310344827
step: 2469, train_loss: 1.2124494314193726, acc: 0.48399999737739563, val_loss: 1.1856601238250732, val_acc: 0.5008000135421753,  lr: 0.009493954827586207
step: 2470, train_loss: 1.1467933654785156, acc: 0.517799973487854, val_loss: 1.1857304573059082, val_acc: 0.504800021648407,  lr: 0.009493610344827586
step: 2471, train_loss: 1.2496014833450317, acc: 0.46399998664855957, val_loss: 1.2064552307128906, val_acc: 0.4893999993801117,  lr: 0.009493265862068966
step: 2472, train_loss: 1.2367327213287354, acc: 0.46860000491142273, val_loss: 1.1890777349472046, val_acc: 0.5026000142097473,  lr: 0.009492921379310345
step: 2473, train_loss: 1.2083255052566528, acc: 0.48339998722076416, val_loss: 1.1982805728912354, val_acc: 0.4952000081539154,  lr: 0.009492576896551724
step: 2474, train_loss: 1.188399076461792, acc: 0.5031999945640564, val_loss: 1.1905995607376099, val_acc: 0.4997999966144562,  lr: 0.009492232413793104
step: 2475, train_loss: 1.1519865989685059, acc: 0.5112000107765198, val_loss: 1.192627191543579, val_acc: 0.5001999735832214,  lr: 0.009491887931034483
step: 2476, train_loss: 1.1412397623062134, acc: 0.5188000202178955, val_loss: 1.1895225048065186, val_acc: 0.5090000033378601,  lr: 0.009491543448275862
step: 2477, train_loss: 1.131435751914978, acc: 0.5257999897003174, val_loss: 1.1884015798568726, val_acc: 0.5009999871253967,  lr: 0.009491198965517242
step: 2478, train_loss: 1.2083455324172974, acc: 0.5, val_loss: 1.1951336860656738, val_acc: 0.49619999527931213,  lr: 0.009490854482758621
step: 2479, train_loss: 1.135058045387268, acc: 0.519599974155426, val_loss: 1.1929898262023926, val_acc: 0.49779999256134033,  lr: 0.00949051
step: 2480, train_loss: 1.1873506307601929, acc: 0.4943999946117401, val_loss: 1.1939678192138672, val_acc: 0.4984000027179718,  lr: 0.00949016551724138
step: 2481, train_loss: 1.1699907779693604, acc: 0.5112000107765198, val_loss: 1.184923529624939, val_acc: 0.5049999952316284,  lr: 0.00948982103448276
step: 2482, train_loss: 1.178322672843933, acc: 0.5019999742507935, val_loss: 1.1912956237792969, val_acc: 0.5004000067710876,  lr: 0.009489476551724139
step: 2483, train_loss: 1.1175156831741333, acc: 0.5338000059127808, val_loss: 1.1939467191696167, val_acc: 0.5016000270843506,  lr: 0.009489132068965518
step: 2484, train_loss: 1.2799711227416992, acc: 0.4544000029563904, val_loss: 1.1926112174987793, val_acc: 0.4991999864578247,  lr: 0.009488787586206898
step: 2485, train_loss: 1.1145713329315186, acc: 0.5370000004768372, val_loss: 1.196813941001892, val_acc: 0.4952000081539154,  lr: 0.009488443103448277
step: 2486, train_loss: 1.1620348691940308, acc: 0.5081999897956848, val_loss: 1.1932353973388672, val_acc: 0.49320000410079956,  lr: 0.009488098620689655
step: 2487, train_loss: 1.1966756582260132, acc: 0.4973999857902527, val_loss: 1.1977282762527466, val_acc: 0.49880000948905945,  lr: 0.009487754137931034
step: 2488, train_loss: 1.2517017126083374, acc: 0.47279998660087585, val_loss: 1.1941617727279663, val_acc: 0.5012000203132629,  lr: 0.009487409655172413
step: 2489, train_loss: 1.2708477973937988, acc: 0.46480000019073486, val_loss: 1.1931170225143433, val_acc: 0.5037999749183655,  lr: 0.009487065172413793
step: 2490, train_loss: 1.2639824151992798, acc: 0.45739999413490295, val_loss: 1.1947801113128662, val_acc: 0.5034000277519226,  lr: 0.009486720689655172
step: 2491, train_loss: 1.157057523727417, acc: 0.5077999830245972, val_loss: 1.199721336364746, val_acc: 0.5016000270843506,  lr: 0.009486376206896551
step: 2492, train_loss: 1.2780524492263794, acc: 0.4596000015735626, val_loss: 1.2101351022720337, val_acc: 0.5005999803543091,  lr: 0.00948603172413793
step: 2493, train_loss: 1.1733722686767578, acc: 0.5094000101089478, val_loss: 1.20408034324646, val_acc: 0.49459999799728394,  lr: 0.00948568724137931
step: 2494, train_loss: 1.170771837234497, acc: 0.5040000081062317, val_loss: 1.20082426071167, val_acc: 0.48899999260902405,  lr: 0.00948534275862069
step: 2495, train_loss: 1.2138563394546509, acc: 0.4864000082015991, val_loss: 1.2068562507629395, val_acc: 0.4860000014305115,  lr: 0.009484998275862069
step: 2496, train_loss: 1.2375377416610718, acc: 0.47999998927116394, val_loss: 1.2041746377944946, val_acc: 0.490200012922287,  lr: 0.009484653793103448
step: 2497, train_loss: 1.2075648307800293, acc: 0.49300000071525574, val_loss: 1.1973598003387451, val_acc: 0.5041999816894531,  lr: 0.009484309310344828
step: 2498, train_loss: 1.1648606061935425, acc: 0.5073999762535095, val_loss: 1.200061321258545, val_acc: 0.4997999966144562,  lr: 0.009483964827586207
step: 2499, train_loss: 1.1457165479660034, acc: 0.5194000005722046, val_loss: 1.1959844827651978, val_acc: 0.4997999966144562,  lr: 0.009483620344827587
step: 2500, train_loss: 1.1927400827407837, acc: 0.4896000027656555, val_loss: 1.1960394382476807, val_acc: 0.49720001220703125,  lr: 0.009483275862068966
step: 2501, train_loss: 1.1394675970077515, acc: 0.5234000086784363, val_loss: 1.1935920715332031, val_acc: 0.49880000948905945,  lr: 0.009482931379310345
step: 2502, train_loss: 1.2312359809875488, acc: 0.4765999913215637, val_loss: 1.2024279832839966, val_acc: 0.4867999851703644,  lr: 0.009482586896551725
step: 2503, train_loss: 1.1754261255264282, acc: 0.5059999823570251, val_loss: 1.2156808376312256, val_acc: 0.4869999885559082,  lr: 0.009482242413793104
step: 2504, train_loss: 1.2547757625579834, acc: 0.47200000286102295, val_loss: 1.2024588584899902, val_acc: 0.4943999946117401,  lr: 0.009481897931034483
step: 2505, train_loss: 1.1547845602035522, acc: 0.5149999856948853, val_loss: 1.1953727006912231, val_acc: 0.498199999332428,  lr: 0.009481553448275863
step: 2506, train_loss: 1.1363900899887085, acc: 0.5289999842643738, val_loss: 1.1936794519424438, val_acc: 0.49559998512268066,  lr: 0.009481208965517242
step: 2507, train_loss: 1.1539634466171265, acc: 0.5163999795913696, val_loss: 1.198554515838623, val_acc: 0.4975999891757965,  lr: 0.009480864482758622
step: 2508, train_loss: 1.1806299686431885, acc: 0.49779999256134033, val_loss: 1.1895819902420044, val_acc: 0.4997999966144562,  lr: 0.009480520000000001
step: 2509, train_loss: 1.1999889612197876, acc: 0.4927999973297119, val_loss: 1.199808955192566, val_acc: 0.48980000615119934,  lr: 0.009480175517241379
step: 2510, train_loss: 1.1755322217941284, acc: 0.504800021648407, val_loss: 1.183688998222351, val_acc: 0.49720001220703125,  lr: 0.009479831034482758
step: 2511, train_loss: 1.1677435636520386, acc: 0.5103999972343445, val_loss: 1.2062796354293823, val_acc: 0.48579999804496765,  lr: 0.009479486551724137
step: 2512, train_loss: 1.2050243616104126, acc: 0.49000000953674316, val_loss: 1.1858011484146118, val_acc: 0.5016000270843506,  lr: 0.009479142068965517
step: 2513, train_loss: 1.1219274997711182, acc: 0.5339999794960022, val_loss: 1.192322015762329, val_acc: 0.4986000061035156,  lr: 0.009478797586206896
step: 2514, train_loss: 1.1259769201278687, acc: 0.5235999822616577, val_loss: 1.1936790943145752, val_acc: 0.49939998984336853,  lr: 0.009478453103448276
step: 2515, train_loss: 1.2369384765625, acc: 0.4837999939918518, val_loss: 1.1873348951339722, val_acc: 0.5031999945640564,  lr: 0.009478108620689655
step: 2516, train_loss: 1.183987021446228, acc: 0.5044000148773193, val_loss: 1.1893097162246704, val_acc: 0.4970000088214874,  lr: 0.009477764137931034
step: 2517, train_loss: 1.1177669763565063, acc: 0.5252000093460083, val_loss: 1.1824370622634888, val_acc: 0.503600001335144,  lr: 0.009477419655172414
step: 2518, train_loss: 1.1490994691848755, acc: 0.5194000005722046, val_loss: 1.1841903924942017, val_acc: 0.5044000148773193,  lr: 0.009477075172413793
step: 2519, train_loss: 1.1438040733337402, acc: 0.5267999768257141, val_loss: 1.1798652410507202, val_acc: 0.5062000155448914,  lr: 0.009476730689655172
step: 2520, train_loss: 1.1402206420898438, acc: 0.5202000141143799, val_loss: 1.1929939985275269, val_acc: 0.4970000088214874,  lr: 0.009476386206896552
step: 2521, train_loss: 1.221248984336853, acc: 0.47839999198913574, val_loss: 1.1872093677520752, val_acc: 0.4986000061035156,  lr: 0.009476041724137931
step: 2522, train_loss: 1.0939258337020874, acc: 0.5422000288963318, val_loss: 1.1946808099746704, val_acc: 0.4975999891757965,  lr: 0.00947569724137931
step: 2523, train_loss: 1.1812256574630737, acc: 0.5063999891281128, val_loss: 1.189732313156128, val_acc: 0.5037999749183655,  lr: 0.00947535275862069
step: 2524, train_loss: 1.177780032157898, acc: 0.5067999958992004, val_loss: 1.2082291841506958, val_acc: 0.49320000410079956,  lr: 0.00947500827586207
step: 2525, train_loss: 1.1338744163513184, acc: 0.5238000154495239, val_loss: 1.1919883489608765, val_acc: 0.5059999823570251,  lr: 0.009474663793103449
step: 2526, train_loss: 1.2057865858078003, acc: 0.49059998989105225, val_loss: 1.1899077892303467, val_acc: 0.5040000081062317,  lr: 0.009474319310344828
step: 2527, train_loss: 1.1491832733154297, acc: 0.5108000040054321, val_loss: 1.1851516962051392, val_acc: 0.5063999891281128,  lr: 0.009473974827586207
step: 2528, train_loss: 1.2729952335357666, acc: 0.4575999975204468, val_loss: 1.1940442323684692, val_acc: 0.49900001287460327,  lr: 0.009473630344827587
step: 2529, train_loss: 1.1249991655349731, acc: 0.527400016784668, val_loss: 1.1917840242385864, val_acc: 0.4991999864578247,  lr: 0.009473285862068966
step: 2530, train_loss: 1.1229368448257446, acc: 0.5293999910354614, val_loss: 1.1887977123260498, val_acc: 0.501800000667572,  lr: 0.009472941379310346
step: 2531, train_loss: 1.1339448690414429, acc: 0.5311999917030334, val_loss: 1.1922314167022705, val_acc: 0.4991999864578247,  lr: 0.009472596896551725
step: 2532, train_loss: 1.1406551599502563, acc: 0.5175999999046326, val_loss: 1.193758249282837, val_acc: 0.49900001287460327,  lr: 0.009472252413793104
step: 2533, train_loss: 1.0777183771133423, acc: 0.553600013256073, val_loss: 1.2004128694534302, val_acc: 0.4973999857902527,  lr: 0.009471907931034484
step: 2534, train_loss: 1.163804292678833, acc: 0.5094000101089478, val_loss: 1.196271300315857, val_acc: 0.49639999866485596,  lr: 0.009471563448275863
step: 2535, train_loss: 1.2162004709243774, acc: 0.4851999878883362, val_loss: 1.1895341873168945, val_acc: 0.5044000148773193,  lr: 0.00947121896551724
step: 2536, train_loss: 1.1660606861114502, acc: 0.5095999836921692, val_loss: 1.1833642721176147, val_acc: 0.5094000101089478,  lr: 0.00947087448275862
step: 2537, train_loss: 1.1769449710845947, acc: 0.5049999952316284, val_loss: 1.184620976448059, val_acc: 0.49959999322891235,  lr: 0.00947053
step: 2538, train_loss: 1.2535022497177124, acc: 0.4674000144004822, val_loss: 1.1793501377105713, val_acc: 0.5052000284194946,  lr: 0.009470185517241379
step: 2539, train_loss: 1.1607401371002197, acc: 0.5156000256538391, val_loss: 1.1788170337677002, val_acc: 0.5031999945640564,  lr: 0.009469841034482758
step: 2540, train_loss: 1.1734386682510376, acc: 0.5085999965667725, val_loss: 1.1824805736541748, val_acc: 0.5081999897956848,  lr: 0.009469496551724138
step: 2541, train_loss: 1.2539364099502563, acc: 0.46860000491142273, val_loss: 1.1774038076400757, val_acc: 0.5063999891281128,  lr: 0.009469152068965517
step: 2542, train_loss: 1.0758812427520752, acc: 0.551800012588501, val_loss: 1.176444411277771, val_acc: 0.5077999830245972,  lr: 0.009468807586206896
step: 2543, train_loss: 1.1881624460220337, acc: 0.5, val_loss: 1.1765351295471191, val_acc: 0.5156000256538391,  lr: 0.009468463103448276
step: 2544, train_loss: 1.206851840019226, acc: 0.4864000082015991, val_loss: 1.172989845275879, val_acc: 0.5139999985694885,  lr: 0.009468118620689655
step: 2545, train_loss: 1.151699423789978, acc: 0.5163999795913696, val_loss: 1.1746845245361328, val_acc: 0.5121999979019165,  lr: 0.009467774137931035
step: 2546, train_loss: 1.2392666339874268, acc: 0.4765999913215637, val_loss: 1.1806365251541138, val_acc: 0.5077999830245972,  lr: 0.009467429655172414
step: 2547, train_loss: 1.1601226329803467, acc: 0.5108000040054321, val_loss: 1.1848516464233398, val_acc: 0.5063999891281128,  lr: 0.009467085172413793
step: 2548, train_loss: 1.1727272272109985, acc: 0.5085999965667725, val_loss: 1.1761794090270996, val_acc: 0.5077999830245972,  lr: 0.009466740689655173
step: 2549, train_loss: 1.1254322528839111, acc: 0.5285999774932861, val_loss: 1.1736515760421753, val_acc: 0.5120000243186951,  lr: 0.009466396206896552
step: 2550, train_loss: 1.239715337753296, acc: 0.47040000557899475, val_loss: 1.1778011322021484, val_acc: 0.5055999755859375,  lr: 0.009466051724137932
step: 2551, train_loss: 1.1927212476730347, acc: 0.501800000667572, val_loss: 1.1792696714401245, val_acc: 0.5085999965667725,  lr: 0.009465707241379311
step: 2552, train_loss: 1.2076047658920288, acc: 0.48840001225471497, val_loss: 1.1916474103927612, val_acc: 0.5001999735832214,  lr: 0.00946536275862069
step: 2553, train_loss: 1.213545799255371, acc: 0.4837999939918518, val_loss: 1.1862680912017822, val_acc: 0.5041999816894531,  lr: 0.00946501827586207
step: 2554, train_loss: 1.1465197801589966, acc: 0.517799973487854, val_loss: 1.1895684003829956, val_acc: 0.501800000667572,  lr: 0.009464673793103449
step: 2555, train_loss: 1.2028276920318604, acc: 0.4975999891757965, val_loss: 1.1892755031585693, val_acc: 0.504800021648407,  lr: 0.009464329310344828
step: 2556, train_loss: 1.1782937049865723, acc: 0.4984000027179718, val_loss: 1.1879669427871704, val_acc: 0.5012000203132629,  lr: 0.009463984827586208
step: 2557, train_loss: 1.243751883506775, acc: 0.47040000557899475, val_loss: 1.177754521369934, val_acc: 0.51419997215271,  lr: 0.009463640344827585
step: 2558, train_loss: 1.0949792861938477, acc: 0.5429999828338623, val_loss: 1.1769765615463257, val_acc: 0.5180000066757202,  lr: 0.009463295862068965
step: 2559, train_loss: 1.1666173934936523, acc: 0.5091999769210815, val_loss: 1.1782156229019165, val_acc: 0.5081999897956848,  lr: 0.009462951379310344
step: 2560, train_loss: 1.1360359191894531, acc: 0.5210000276565552, val_loss: 1.1712520122528076, val_acc: 0.508400022983551,  lr: 0.009462606896551724
step: 2561, train_loss: 1.2550276517868042, acc: 0.46380001306533813, val_loss: 1.1712778806686401, val_acc: 0.5052000284194946,  lr: 0.009462262413793103
step: 2562, train_loss: 1.1658387184143066, acc: 0.5005999803543091, val_loss: 1.1708709001541138, val_acc: 0.5121999979019165,  lr: 0.009461917931034482
step: 2563, train_loss: 1.1178492307662964, acc: 0.5228000283241272, val_loss: 1.1731020212173462, val_acc: 0.51419997215271,  lr: 0.009461573448275862
step: 2564, train_loss: 1.131170392036438, acc: 0.5230000019073486, val_loss: 1.1814188957214355, val_acc: 0.5034000277519226,  lr: 0.009461228965517241
step: 2565, train_loss: 1.2244421243667603, acc: 0.48660001158714294, val_loss: 1.1771655082702637, val_acc: 0.5126000046730042,  lr: 0.00946088448275862
step: 2566, train_loss: 1.2389804124832153, acc: 0.48019999265670776, val_loss: 1.183840274810791, val_acc: 0.5040000081062317,  lr: 0.00946054
step: 2567, train_loss: 1.2109469175338745, acc: 0.4878000020980835, val_loss: 1.1747239828109741, val_acc: 0.5109999775886536,  lr: 0.00946019551724138
step: 2568, train_loss: 1.1620150804519653, acc: 0.5139999985694885, val_loss: 1.172334909439087, val_acc: 0.5058000087738037,  lr: 0.009459851034482759
step: 2569, train_loss: 1.1763519048690796, acc: 0.5049999952316284, val_loss: 1.182178020477295, val_acc: 0.49939998984336853,  lr: 0.009459506551724138
step: 2570, train_loss: 1.1245880126953125, acc: 0.5230000019073486, val_loss: 1.1758830547332764, val_acc: 0.5041999816894531,  lr: 0.009459162068965517
step: 2571, train_loss: 1.1579465866088867, acc: 0.5076000094413757, val_loss: 1.1752320528030396, val_acc: 0.5037999749183655,  lr: 0.009458817586206897
step: 2572, train_loss: 1.1463514566421509, acc: 0.5157999992370605, val_loss: 1.1787996292114258, val_acc: 0.5034000277519226,  lr: 0.009458473103448276
step: 2573, train_loss: 1.152313232421875, acc: 0.5162000060081482, val_loss: 1.171546459197998, val_acc: 0.5126000046730042,  lr: 0.009458128620689656
step: 2574, train_loss: 1.1787751913070679, acc: 0.5099999904632568, val_loss: 1.1724005937576294, val_acc: 0.5095999836921692,  lr: 0.009457784137931035
step: 2575, train_loss: 1.2523255348205566, acc: 0.47679999470710754, val_loss: 1.185787558555603, val_acc: 0.501800000667572,  lr: 0.009457439655172414
step: 2576, train_loss: 1.131213903427124, acc: 0.527999997138977, val_loss: 1.1891529560089111, val_acc: 0.5008000135421753,  lr: 0.009457095172413794
step: 2577, train_loss: 1.1017729043960571, acc: 0.5461999773979187, val_loss: 1.1922920942306519, val_acc: 0.4968000054359436,  lr: 0.009456750689655173
step: 2578, train_loss: 1.158286690711975, acc: 0.5041999816894531, val_loss: 1.1979602575302124, val_acc: 0.4927999973297119,  lr: 0.009456406206896553
step: 2579, train_loss: 1.186439037322998, acc: 0.5013999938964844, val_loss: 1.1884022951126099, val_acc: 0.49959999322891235,  lr: 0.009456061724137932
step: 2580, train_loss: 1.1832313537597656, acc: 0.5034000277519226, val_loss: 1.189836859703064, val_acc: 0.5004000067710876,  lr: 0.009455717241379311
step: 2581, train_loss: 1.2205710411071777, acc: 0.4918000102043152, val_loss: 1.187378168106079, val_acc: 0.5008000135421753,  lr: 0.00945537275862069
step: 2582, train_loss: 1.1968010663986206, acc: 0.49939998984336853, val_loss: 1.1958168745040894, val_acc: 0.49720001220703125,  lr: 0.00945502827586207
step: 2583, train_loss: 1.1409642696380615, acc: 0.5216000080108643, val_loss: 1.1933876276016235, val_acc: 0.5012000203132629,  lr: 0.009454683793103448
step: 2584, train_loss: 1.1232905387878418, acc: 0.5315999984741211, val_loss: 1.195817470550537, val_acc: 0.4975999891757965,  lr: 0.009454339310344827
step: 2585, train_loss: 1.213781476020813, acc: 0.48420000076293945, val_loss: 1.200622320175171, val_acc: 0.4925999939441681,  lr: 0.009453994827586206
step: 2586, train_loss: 1.1543067693710327, acc: 0.5145999789237976, val_loss: 1.197677731513977, val_acc: 0.49219998717308044,  lr: 0.009453650344827586
step: 2587, train_loss: 1.1392736434936523, acc: 0.5220000147819519, val_loss: 1.2029820680618286, val_acc: 0.4893999993801117,  lr: 0.009453305862068965
step: 2588, train_loss: 1.1329964399337769, acc: 0.527400016784668, val_loss: 1.19687819480896, val_acc: 0.49399998784065247,  lr: 0.009452961379310345
step: 2589, train_loss: 1.1780433654785156, acc: 0.5049999952316284, val_loss: 1.1981314420700073, val_acc: 0.498199999332428,  lr: 0.009452616896551724
step: 2590, train_loss: 1.1313122510910034, acc: 0.5224000215530396, val_loss: 1.2014987468719482, val_acc: 0.4986000061035156,  lr: 0.009452272413793103
step: 2591, train_loss: 1.1315728425979614, acc: 0.5275999903678894, val_loss: 1.2100696563720703, val_acc: 0.48899999260902405,  lr: 0.009451927931034483
step: 2592, train_loss: 1.1030079126358032, acc: 0.5364000201225281, val_loss: 1.2010350227355957, val_acc: 0.4867999851703644,  lr: 0.009451583448275862
step: 2593, train_loss: 1.2265400886535645, acc: 0.47440001368522644, val_loss: 1.2051650285720825, val_acc: 0.48660001158714294,  lr: 0.009451238965517242
step: 2594, train_loss: 1.2335355281829834, acc: 0.47780001163482666, val_loss: 1.2140257358551025, val_acc: 0.487199991941452,  lr: 0.009450894482758621
step: 2595, train_loss: 1.2184919118881226, acc: 0.48660001158714294, val_loss: 1.2184817790985107, val_acc: 0.4864000082015991,  lr: 0.00945055
step: 2596, train_loss: 1.097845435142517, acc: 0.5406000018119812, val_loss: 1.2086254358291626, val_acc: 0.4925999939441681,  lr: 0.00945020551724138
step: 2597, train_loss: 1.1801493167877197, acc: 0.504800021648407, val_loss: 1.207078456878662, val_acc: 0.49160000681877136,  lr: 0.009449861034482759
step: 2598, train_loss: 1.1898080110549927, acc: 0.5013999938964844, val_loss: 1.2096565961837769, val_acc: 0.4903999865055084,  lr: 0.009449516551724138
step: 2599, train_loss: 1.1571927070617676, acc: 0.508400022983551, val_loss: 1.2004731893539429, val_acc: 0.4968000054359436,  lr: 0.009449172068965518
step: 2600, train_loss: 1.1854721307754517, acc: 0.5072000026702881, val_loss: 1.2091892957687378, val_acc: 0.4887999892234802,  lr: 0.009448827586206897
step: 2601, train_loss: 1.1776858568191528, acc: 0.5103999972343445, val_loss: 1.2070099115371704, val_acc: 0.49219998717308044,  lr: 0.009448483103448277
step: 2602, train_loss: 1.1351711750030518, acc: 0.5162000060081482, val_loss: 1.2050060033798218, val_acc: 0.48919999599456787,  lr: 0.009448138620689656
step: 2603, train_loss: 1.0961143970489502, acc: 0.5382000207901001, val_loss: 1.2071682214736938, val_acc: 0.4867999851703644,  lr: 0.009447794137931035
step: 2604, train_loss: 1.1895776987075806, acc: 0.4986000061035156, val_loss: 1.203906774520874, val_acc: 0.48660001158714294,  lr: 0.009447449655172415
step: 2605, train_loss: 1.1148537397384644, acc: 0.5338000059127808, val_loss: 1.2023658752441406, val_acc: 0.4885999858379364,  lr: 0.009447105172413794
step: 2606, train_loss: 1.1582984924316406, acc: 0.5144000053405762, val_loss: 1.2013391256332397, val_acc: 0.4885999858379364,  lr: 0.009446760689655172
step: 2607, train_loss: 1.1458667516708374, acc: 0.5252000093460083, val_loss: 1.2088134288787842, val_acc: 0.487199991941452,  lr: 0.009446416206896551
step: 2608, train_loss: 1.1685220003128052, acc: 0.5126000046730042, val_loss: 1.2099297046661377, val_acc: 0.48660001158714294,  lr: 0.00944607172413793
step: 2609, train_loss: 1.2068239450454712, acc: 0.4970000088214874, val_loss: 1.213333249092102, val_acc: 0.48559999465942383,  lr: 0.00944572724137931
step: 2610, train_loss: 1.185980200767517, acc: 0.5009999871253967, val_loss: 1.2170968055725098, val_acc: 0.49000000953674316,  lr: 0.00944538275862069
step: 2611, train_loss: 1.140939474105835, acc: 0.5230000019073486, val_loss: 1.22373366355896, val_acc: 0.48260000348091125,  lr: 0.009445038275862069
step: 2612, train_loss: 1.1431602239608765, acc: 0.5210000276565552, val_loss: 1.2121210098266602, val_acc: 0.4880000054836273,  lr: 0.009444693793103448
step: 2613, train_loss: 1.1658295392990112, acc: 0.5067999958992004, val_loss: 1.2089868783950806, val_acc: 0.49079999327659607,  lr: 0.009444349310344827
step: 2614, train_loss: 1.1323431730270386, acc: 0.5221999883651733, val_loss: 1.2121226787567139, val_acc: 0.48579999804496765,  lr: 0.009444004827586207
step: 2615, train_loss: 1.1233973503112793, acc: 0.527400016784668, val_loss: 1.2104939222335815, val_acc: 0.4862000048160553,  lr: 0.009443660344827586
step: 2616, train_loss: 1.2541552782058716, acc: 0.47859999537467957, val_loss: 1.220140814781189, val_acc: 0.48500001430511475,  lr: 0.009443315862068966
step: 2617, train_loss: 1.1964503526687622, acc: 0.504800021648407, val_loss: 1.2187459468841553, val_acc: 0.48260000348091125,  lr: 0.009442971379310345
step: 2618, train_loss: 1.1293132305145264, acc: 0.5270000100135803, val_loss: 1.21347975730896, val_acc: 0.47999998927116394,  lr: 0.009442626896551724
step: 2619, train_loss: 1.1723546981811523, acc: 0.508400022983551, val_loss: 1.2200186252593994, val_acc: 0.47999998927116394,  lr: 0.009442282413793104
step: 2620, train_loss: 1.173952341079712, acc: 0.5044000148773193, val_loss: 1.2267076969146729, val_acc: 0.4790000021457672,  lr: 0.009441937931034483
step: 2621, train_loss: 1.1366735696792603, acc: 0.5217999815940857, val_loss: 1.2274439334869385, val_acc: 0.4819999933242798,  lr: 0.009441593448275862
step: 2622, train_loss: 1.1337958574295044, acc: 0.5203999876976013, val_loss: 1.222474455833435, val_acc: 0.4821999967098236,  lr: 0.009441248965517242
step: 2623, train_loss: 1.1285196542739868, acc: 0.524399995803833, val_loss: 1.2187955379486084, val_acc: 0.4830000102519989,  lr: 0.009440904482758621
step: 2624, train_loss: 1.1292390823364258, acc: 0.521399974822998, val_loss: 1.2100012302398682, val_acc: 0.48980000615119934,  lr: 0.00944056
step: 2625, train_loss: 1.0871120691299438, acc: 0.5562000274658203, val_loss: 1.2130895853042603, val_acc: 0.4848000109195709,  lr: 0.00944021551724138
step: 2626, train_loss: 1.1655088663101196, acc: 0.5130000114440918, val_loss: 1.211191177368164, val_acc: 0.48559999465942383,  lr: 0.00943987103448276
step: 2627, train_loss: 1.1448801755905151, acc: 0.5152000188827515, val_loss: 1.2105131149291992, val_acc: 0.48739999532699585,  lr: 0.009439526551724139
step: 2628, train_loss: 1.0846375226974487, acc: 0.5396000146865845, val_loss: 1.2112194299697876, val_acc: 0.4878000020980835,  lr: 0.009439182068965518
step: 2629, train_loss: 1.1252186298370361, acc: 0.5297999978065491, val_loss: 1.2174850702285767, val_acc: 0.49239999055862427,  lr: 0.009438837586206898
step: 2630, train_loss: 1.1460490226745605, acc: 0.5163999795913696, val_loss: 1.2153496742248535, val_acc: 0.48579999804496765,  lr: 0.009438493103448277
step: 2631, train_loss: 1.1937222480773926, acc: 0.492000013589859, val_loss: 1.2204508781433105, val_acc: 0.487199991941452,  lr: 0.009438148620689655
step: 2632, train_loss: 1.1024104356765747, acc: 0.5370000004768372, val_loss: 1.2217810153961182, val_acc: 0.4862000048160553,  lr: 0.009437804137931034
step: 2633, train_loss: 1.143217921257019, acc: 0.5134000182151794, val_loss: 1.2141683101654053, val_acc: 0.48840001225471497,  lr: 0.009437459655172413
step: 2634, train_loss: 1.2687969207763672, acc: 0.462799996137619, val_loss: 1.2093085050582886, val_acc: 0.48840001225471497,  lr: 0.009437115172413793
step: 2635, train_loss: 1.1472952365875244, acc: 0.5153999924659729, val_loss: 1.2098153829574585, val_acc: 0.48240000009536743,  lr: 0.009436770689655172
step: 2636, train_loss: 1.2129230499267578, acc: 0.4885999858379364, val_loss: 1.2126764059066772, val_acc: 0.4830000102519989,  lr: 0.009436426206896551
step: 2637, train_loss: 1.1584327220916748, acc: 0.5153999924659729, val_loss: 1.2054541110992432, val_acc: 0.4869999885559082,  lr: 0.00943608172413793
step: 2638, train_loss: 1.1231683492660522, acc: 0.5278000235557556, val_loss: 1.2079249620437622, val_acc: 0.4903999865055084,  lr: 0.00943573724137931
step: 2639, train_loss: 1.165649175643921, acc: 0.5203999876976013, val_loss: 1.217307209968567, val_acc: 0.4830000102519989,  lr: 0.00943539275862069
step: 2640, train_loss: 1.2569657564163208, acc: 0.4697999954223633, val_loss: 1.2004116773605347, val_acc: 0.49480000138282776,  lr: 0.009435048275862069
step: 2641, train_loss: 1.1260771751403809, acc: 0.5281999707221985, val_loss: 1.1887598037719727, val_acc: 0.5016000270843506,  lr: 0.009434703793103448
step: 2642, train_loss: 1.1911484003067017, acc: 0.4936000108718872, val_loss: 1.1950088739395142, val_acc: 0.49300000071525574,  lr: 0.009434359310344828
step: 2643, train_loss: 1.1092820167541504, acc: 0.5270000100135803, val_loss: 1.1945512294769287, val_acc: 0.4957999885082245,  lr: 0.009434014827586207
step: 2644, train_loss: 1.2424529790878296, acc: 0.4713999927043915, val_loss: 1.1826040744781494, val_acc: 0.5049999952316284,  lr: 0.009433670344827587
step: 2645, train_loss: 1.1211241483688354, acc: 0.5246000289916992, val_loss: 1.1840486526489258, val_acc: 0.4973999857902527,  lr: 0.009433325862068966
step: 2646, train_loss: 1.1395981311798096, acc: 0.5242000222206116, val_loss: 1.1833159923553467, val_acc: 0.5012000203132629,  lr: 0.009432981379310345
step: 2647, train_loss: 1.1332640647888184, acc: 0.5184000134468079, val_loss: 1.1782277822494507, val_acc: 0.5016000270843506,  lr: 0.009432636896551725
step: 2648, train_loss: 1.1967922449111938, acc: 0.4880000054836273, val_loss: 1.1733942031860352, val_acc: 0.5063999891281128,  lr: 0.009432292413793104
step: 2649, train_loss: 1.2434651851654053, acc: 0.4675999879837036, val_loss: 1.1798136234283447, val_acc: 0.5001999735832214,  lr: 0.009431947931034483
step: 2650, train_loss: 1.1451729536056519, acc: 0.522599995136261, val_loss: 1.1748816967010498, val_acc: 0.5034000277519226,  lr: 0.009431603448275863
step: 2651, train_loss: 1.1110738515853882, acc: 0.531000018119812, val_loss: 1.1736797094345093, val_acc: 0.5085999965667725,  lr: 0.009431258965517242
step: 2652, train_loss: 1.1517152786254883, acc: 0.5224000215530396, val_loss: 1.1775505542755127, val_acc: 0.5077999830245972,  lr: 0.009430914482758622
step: 2653, train_loss: 1.1698707342147827, acc: 0.5072000026702881, val_loss: 1.1789621114730835, val_acc: 0.5073999762535095,  lr: 0.009430570000000001
step: 2654, train_loss: 1.0852441787719727, acc: 0.5455999970436096, val_loss: 1.1837025880813599, val_acc: 0.5009999871253967,  lr: 0.009430225517241379
step: 2655, train_loss: 1.1909873485565186, acc: 0.490200012922287, val_loss: 1.1818543672561646, val_acc: 0.5127999782562256,  lr: 0.009429881034482758
step: 2656, train_loss: 1.248575210571289, acc: 0.46860000491142273, val_loss: 1.1849100589752197, val_acc: 0.5049999952316284,  lr: 0.009429536551724137
step: 2657, train_loss: 1.1058168411254883, acc: 0.5333999991416931, val_loss: 1.181213617324829, val_acc: 0.5090000033378601,  lr: 0.009429192068965517
step: 2658, train_loss: 1.222471833229065, acc: 0.4837999939918518, val_loss: 1.1899936199188232, val_acc: 0.5085999965667725,  lr: 0.009428847586206896
step: 2659, train_loss: 1.1394487619400024, acc: 0.5234000086784363, val_loss: 1.1794203519821167, val_acc: 0.5138000249862671,  lr: 0.009428503103448276
step: 2660, train_loss: 1.2215536832809448, acc: 0.4819999933242798, val_loss: 1.1889970302581787, val_acc: 0.5004000067710876,  lr: 0.009428158620689655
step: 2661, train_loss: 1.117221474647522, acc: 0.5311999917030334, val_loss: 1.1913429498672485, val_acc: 0.5012000203132629,  lr: 0.009427814137931034
step: 2662, train_loss: 1.1073007583618164, acc: 0.5389999747276306, val_loss: 1.1972556114196777, val_acc: 0.4975999891757965,  lr: 0.009427469655172414
step: 2663, train_loss: 1.2123520374298096, acc: 0.48399999737739563, val_loss: 1.1965172290802002, val_acc: 0.503000020980835,  lr: 0.009427125172413793
step: 2664, train_loss: 1.142182469367981, acc: 0.5185999870300293, val_loss: 1.2136108875274658, val_acc: 0.4828000068664551,  lr: 0.009426780689655172
step: 2665, train_loss: 1.1577107906341553, acc: 0.5091999769210815, val_loss: 1.221602201461792, val_acc: 0.47780001163482666,  lr: 0.009426436206896552
step: 2666, train_loss: 1.1769028902053833, acc: 0.5081999897956848, val_loss: 1.2062585353851318, val_acc: 0.4991999864578247,  lr: 0.009426091724137931
step: 2667, train_loss: 1.1272292137145996, acc: 0.527999997138977, val_loss: 1.2046160697937012, val_acc: 0.49160000681877136,  lr: 0.00942574724137931
step: 2668, train_loss: 1.1437498331069946, acc: 0.5242000222206116, val_loss: 1.2129913568496704, val_acc: 0.48559999465942383,  lr: 0.00942540275862069
step: 2669, train_loss: 1.2582355737686157, acc: 0.4684000015258789, val_loss: 1.2092477083206177, val_acc: 0.48539999127388,  lr: 0.00942505827586207
step: 2670, train_loss: 1.235442042350769, acc: 0.4797999858856201, val_loss: 1.2106273174285889, val_acc: 0.49459999799728394,  lr: 0.009424713793103449
step: 2671, train_loss: 1.2119731903076172, acc: 0.487199991941452, val_loss: 1.2168946266174316, val_acc: 0.48980000615119934,  lr: 0.009424369310344828
step: 2672, train_loss: 1.1213380098342896, acc: 0.5271999835968018, val_loss: 1.2051782608032227, val_acc: 0.4896000027656555,  lr: 0.009424024827586207
step: 2673, train_loss: 1.2263143062591553, acc: 0.4860000014305115, val_loss: 1.2049566507339478, val_acc: 0.49239999055862427,  lr: 0.009423680344827587
step: 2674, train_loss: 1.2187440395355225, acc: 0.4837999939918518, val_loss: 1.1991521120071411, val_acc: 0.4973999857902527,  lr: 0.009423335862068966
step: 2675, train_loss: 1.128724217414856, acc: 0.5284000039100647, val_loss: 1.1978535652160645, val_acc: 0.5055999755859375,  lr: 0.009422991379310346
step: 2676, train_loss: 1.2004365921020508, acc: 0.4952000081539154, val_loss: 1.2087632417678833, val_acc: 0.4986000061035156,  lr: 0.009422646896551725
step: 2677, train_loss: 1.2756927013397217, acc: 0.45500001311302185, val_loss: 1.2080750465393066, val_acc: 0.4934000074863434,  lr: 0.009422302413793104
step: 2678, train_loss: 1.1963061094284058, acc: 0.48899999260902405, val_loss: 1.2107765674591064, val_acc: 0.49320000410079956,  lr: 0.009421957931034484
step: 2679, train_loss: 1.180625557899475, acc: 0.49779999256134033, val_loss: 1.2009638547897339, val_acc: 0.49900001287460327,  lr: 0.009421613448275861
step: 2680, train_loss: 1.1498812437057495, acc: 0.519599974155426, val_loss: 1.194693922996521, val_acc: 0.49939998984336853,  lr: 0.00942126896551724
step: 2681, train_loss: 1.125828742980957, acc: 0.531000018119812, val_loss: 1.1927292346954346, val_acc: 0.49540001153945923,  lr: 0.00942092448275862
step: 2682, train_loss: 1.121522307395935, acc: 0.5242000222206116, val_loss: 1.1868290901184082, val_acc: 0.5040000081062317,  lr: 0.00942058
step: 2683, train_loss: 1.0974560976028442, acc: 0.545199990272522, val_loss: 1.1906359195709229, val_acc: 0.504800021648407,  lr: 0.009420235517241379
step: 2684, train_loss: 1.208754062652588, acc: 0.49000000953674316, val_loss: 1.1969983577728271, val_acc: 0.5004000067710876,  lr: 0.009419891034482758
step: 2685, train_loss: 1.1766966581344604, acc: 0.5027999877929688, val_loss: 1.195084810256958, val_acc: 0.4966000020503998,  lr: 0.009419546551724138
step: 2686, train_loss: 1.1669424772262573, acc: 0.5054000020027161, val_loss: 1.1941156387329102, val_acc: 0.49459999799728394,  lr: 0.009419202068965517
step: 2687, train_loss: 1.2101494073867798, acc: 0.49459999799728394, val_loss: 1.1948773860931396, val_acc: 0.5012000203132629,  lr: 0.009418857586206896
step: 2688, train_loss: 1.1973923444747925, acc: 0.4885999858379364, val_loss: 1.1936548948287964, val_acc: 0.5012000203132629,  lr: 0.009418513103448276
step: 2689, train_loss: 1.1399441957473755, acc: 0.5175999999046326, val_loss: 1.193266749382019, val_acc: 0.49779999256134033,  lr: 0.009418168620689655
step: 2690, train_loss: 1.162155032157898, acc: 0.5130000114440918, val_loss: 1.187736988067627, val_acc: 0.4984000027179718,  lr: 0.009417824137931035
step: 2691, train_loss: 1.2173123359680176, acc: 0.4893999993801117, val_loss: 1.1935722827911377, val_acc: 0.4997999966144562,  lr: 0.009417479655172414
step: 2692, train_loss: 1.1674144268035889, acc: 0.5037999749183655, val_loss: 1.195586085319519, val_acc: 0.49540001153945923,  lr: 0.009417135172413793
step: 2693, train_loss: 1.1912264823913574, acc: 0.49619999527931213, val_loss: 1.194566249847412, val_acc: 0.5041999816894531,  lr: 0.009416790689655173
step: 2694, train_loss: 1.1838440895080566, acc: 0.4984000027179718, val_loss: 1.1975035667419434, val_acc: 0.503600001335144,  lr: 0.009416446206896552
step: 2695, train_loss: 1.1230283975601196, acc: 0.5303999781608582, val_loss: 1.1969223022460938, val_acc: 0.5012000203132629,  lr: 0.009416101724137932
step: 2696, train_loss: 1.2125223875045776, acc: 0.4927999973297119, val_loss: 1.1916636228561401, val_acc: 0.49939998984336853,  lr: 0.009415757241379311
step: 2697, train_loss: 1.1618362665176392, acc: 0.5088000297546387, val_loss: 1.2032179832458496, val_acc: 0.4970000088214874,  lr: 0.00941541275862069
step: 2698, train_loss: 1.2099956274032593, acc: 0.49399998784065247, val_loss: 1.198310136795044, val_acc: 0.5009999871253967,  lr: 0.00941506827586207
step: 2699, train_loss: 1.1371052265167236, acc: 0.5238000154495239, val_loss: 1.2008721828460693, val_acc: 0.5037999749183655,  lr: 0.009414723793103449
step: 2700, train_loss: 1.2614473104476929, acc: 0.47380000352859497, val_loss: 1.1895209550857544, val_acc: 0.5045999884605408,  lr: 0.009414379310344828
step: 2701, train_loss: 1.121639609336853, acc: 0.5270000100135803, val_loss: 1.181451678276062, val_acc: 0.5059999823570251,  lr: 0.009414034827586208
step: 2702, train_loss: 1.149598479270935, acc: 0.508400022983551, val_loss: 1.1819871664047241, val_acc: 0.5022000074386597,  lr: 0.009413690344827585
step: 2703, train_loss: 1.1244574785232544, acc: 0.5285999774932861, val_loss: 1.1823992729187012, val_acc: 0.5027999877929688,  lr: 0.009413345862068965
step: 2704, train_loss: 1.189979910850525, acc: 0.49380001425743103, val_loss: 1.1776280403137207, val_acc: 0.5045999884605408,  lr: 0.009413001379310344
step: 2705, train_loss: 1.1294164657592773, acc: 0.5266000032424927, val_loss: 1.1745392084121704, val_acc: 0.5072000026702881,  lr: 0.009412656896551724
step: 2706, train_loss: 1.1841785907745361, acc: 0.49779999256134033, val_loss: 1.1752376556396484, val_acc: 0.5031999945640564,  lr: 0.009412312413793103
step: 2707, train_loss: 1.1533457040786743, acc: 0.5108000040054321, val_loss: 1.1822041273117065, val_acc: 0.4986000061035156,  lr: 0.009411967931034482
step: 2708, train_loss: 1.1213709115982056, acc: 0.5297999978065491, val_loss: 1.177645206451416, val_acc: 0.49779999256134033,  lr: 0.009411623448275862
step: 2709, train_loss: 1.1837960481643677, acc: 0.5026000142097473, val_loss: 1.1769449710845947, val_acc: 0.5022000074386597,  lr: 0.009411278965517241
step: 2710, train_loss: 1.1643704175949097, acc: 0.5170000195503235, val_loss: 1.1810473203659058, val_acc: 0.5012000203132629,  lr: 0.00941093448275862
step: 2711, train_loss: 1.2087006568908691, acc: 0.49000000953674316, val_loss: 1.1861920356750488, val_acc: 0.5001999735832214,  lr: 0.00941059
step: 2712, train_loss: 1.1401057243347168, acc: 0.5189999938011169, val_loss: 1.1914198398590088, val_acc: 0.49959999322891235,  lr: 0.00941024551724138
step: 2713, train_loss: 1.158575177192688, acc: 0.5144000053405762, val_loss: 1.1901408433914185, val_acc: 0.5031999945640564,  lr: 0.009409901034482759
step: 2714, train_loss: 1.1277014017105103, acc: 0.5248000025749207, val_loss: 1.197144865989685, val_acc: 0.4984000027179718,  lr: 0.009409556551724138
step: 2715, train_loss: 1.139948844909668, acc: 0.5144000053405762, val_loss: 1.1925731897354126, val_acc: 0.4950000047683716,  lr: 0.009409212068965517
step: 2716, train_loss: 1.1597976684570312, acc: 0.5081999897956848, val_loss: 1.1937296390533447, val_acc: 0.49880000948905945,  lr: 0.009408867586206897
step: 2717, train_loss: 1.087103009223938, acc: 0.5465999841690063, val_loss: 1.191954255104065, val_acc: 0.49619999527931213,  lr: 0.009408523103448276
step: 2718, train_loss: 1.1141928434371948, acc: 0.524399995803833, val_loss: 1.194916009902954, val_acc: 0.4941999912261963,  lr: 0.009408178620689656
step: 2719, train_loss: 1.1612553596496582, acc: 0.515999972820282, val_loss: 1.1936153173446655, val_acc: 0.49880000948905945,  lr: 0.009407834137931035
step: 2720, train_loss: 1.1319737434387207, acc: 0.5293999910354614, val_loss: 1.2088642120361328, val_acc: 0.49079999327659607,  lr: 0.009407489655172414
step: 2721, train_loss: 1.078399658203125, acc: 0.5460000038146973, val_loss: 1.198594331741333, val_acc: 0.49559998512268066,  lr: 0.009407145172413794
step: 2722, train_loss: 1.2333406209945679, acc: 0.475600004196167, val_loss: 1.2129852771759033, val_acc: 0.4830000102519989,  lr: 0.009406800689655173
step: 2723, train_loss: 1.1510239839553833, acc: 0.5206000208854675, val_loss: 1.2085260152816772, val_acc: 0.49140000343322754,  lr: 0.009406456206896553
step: 2724, train_loss: 1.1242364645004272, acc: 0.52920001745224, val_loss: 1.2120072841644287, val_acc: 0.49000000953674316,  lr: 0.009406111724137932
step: 2725, train_loss: 1.1157797574996948, acc: 0.5293999910354614, val_loss: 1.2092021703720093, val_acc: 0.4885999858379364,  lr: 0.009405767241379311
step: 2726, train_loss: 1.148314356803894, acc: 0.5185999870300293, val_loss: 1.2241759300231934, val_acc: 0.483599990606308,  lr: 0.00940542275862069
step: 2727, train_loss: 1.24776029586792, acc: 0.4726000130176544, val_loss: 1.2236629724502563, val_acc: 0.4880000054836273,  lr: 0.00940507827586207
step: 2728, train_loss: 1.2828760147094727, acc: 0.45579999685287476, val_loss: 1.2058643102645874, val_acc: 0.4912000000476837,  lr: 0.009404733793103448
step: 2729, train_loss: 1.068904995918274, acc: 0.5541999936103821, val_loss: 1.2098418474197388, val_acc: 0.4941999912261963,  lr: 0.009404389310344827
step: 2730, train_loss: 1.1609654426574707, acc: 0.5113999843597412, val_loss: 1.202448844909668, val_acc: 0.48919999599456787,  lr: 0.009404044827586206
step: 2731, train_loss: 1.1361656188964844, acc: 0.5235999822616577, val_loss: 1.1950291395187378, val_acc: 0.49079999327659607,  lr: 0.009403700344827586
step: 2732, train_loss: 1.1937906742095947, acc: 0.49959999322891235, val_loss: 1.1968284845352173, val_acc: 0.4950000047683716,  lr: 0.009403355862068965
step: 2733, train_loss: 1.2290911674499512, acc: 0.4697999954223633, val_loss: 1.2014198303222656, val_acc: 0.49720001220703125,  lr: 0.009403011379310345
step: 2734, train_loss: 1.1719067096710205, acc: 0.5072000026702881, val_loss: 1.2038037776947021, val_acc: 0.490200012922287,  lr: 0.009402666896551724
step: 2735, train_loss: 1.162674903869629, acc: 0.4966000020503998, val_loss: 1.1937360763549805, val_acc: 0.4943999946117401,  lr: 0.009402322413793103
step: 2736, train_loss: 1.1088095903396606, acc: 0.5406000018119812, val_loss: 1.1957166194915771, val_acc: 0.48559999465942383,  lr: 0.009401977931034483
step: 2737, train_loss: 1.1995744705200195, acc: 0.4909999966621399, val_loss: 1.192976713180542, val_acc: 0.48919999599456787,  lr: 0.009401633448275862
step: 2738, train_loss: 1.17759108543396, acc: 0.4968000054359436, val_loss: 1.19539213180542, val_acc: 0.49219998717308044,  lr: 0.009401288965517242
step: 2739, train_loss: 1.1391264200210571, acc: 0.5210000276565552, val_loss: 1.197395920753479, val_acc: 0.4934000074863434,  lr: 0.009400944482758621
step: 2740, train_loss: 1.120682954788208, acc: 0.527999997138977, val_loss: 1.1870838403701782, val_acc: 0.4952000081539154,  lr: 0.0094006
step: 2741, train_loss: 1.1482576131820679, acc: 0.5320000052452087, val_loss: 1.187472939491272, val_acc: 0.5,  lr: 0.00940025551724138
step: 2742, train_loss: 1.1954143047332764, acc: 0.4912000000476837, val_loss: 1.1860407590866089, val_acc: 0.49799999594688416,  lr: 0.009399911034482759
step: 2743, train_loss: 1.2005945444107056, acc: 0.48820000886917114, val_loss: 1.1867886781692505, val_acc: 0.49939998984336853,  lr: 0.009399566551724138
step: 2744, train_loss: 1.169037938117981, acc: 0.5090000033378601, val_loss: 1.185165524482727, val_acc: 0.5040000081062317,  lr: 0.009399222068965518
step: 2745, train_loss: 1.160334587097168, acc: 0.5040000081062317, val_loss: 1.177061676979065, val_acc: 0.5062000155448914,  lr: 0.009398877586206897
step: 2746, train_loss: 1.0911208391189575, acc: 0.5428000092506409, val_loss: 1.1761184930801392, val_acc: 0.5103999972343445,  lr: 0.009398533103448277
step: 2747, train_loss: 1.1642179489135742, acc: 0.5058000087738037, val_loss: 1.17838716506958, val_acc: 0.510200023651123,  lr: 0.009398188620689656
step: 2748, train_loss: 1.2047617435455322, acc: 0.4959999918937683, val_loss: 1.177626132965088, val_acc: 0.5031999945640564,  lr: 0.009397844137931035
step: 2749, train_loss: 1.176637887954712, acc: 0.5040000081062317, val_loss: 1.189269781112671, val_acc: 0.4970000088214874,  lr: 0.009397499655172415
step: 2750, train_loss: 1.1026197671890259, acc: 0.5374000072479248, val_loss: 1.18234384059906, val_acc: 0.5022000074386597,  lr: 0.009397155172413792
step: 2751, train_loss: 1.1391849517822266, acc: 0.5199999809265137, val_loss: 1.1728135347366333, val_acc: 0.5013999938964844,  lr: 0.009396810689655172
step: 2752, train_loss: 1.254802942276001, acc: 0.4772000014781952, val_loss: 1.1800225973129272, val_acc: 0.503600001335144,  lr: 0.009396466206896551
step: 2753, train_loss: 1.142646074295044, acc: 0.5212000012397766, val_loss: 1.1858893632888794, val_acc: 0.5063999891281128,  lr: 0.00939612172413793
step: 2754, train_loss: 1.2617379426956177, acc: 0.45719999074935913, val_loss: 1.1830787658691406, val_acc: 0.5045999884605408,  lr: 0.00939577724137931
step: 2755, train_loss: 1.2030513286590576, acc: 0.4864000082015991, val_loss: 1.1800732612609863, val_acc: 0.5063999891281128,  lr: 0.00939543275862069
step: 2756, train_loss: 1.1280320882797241, acc: 0.5278000235557556, val_loss: 1.191642165184021, val_acc: 0.4934000074863434,  lr: 0.009395088275862069
step: 2757, train_loss: 1.2000718116760254, acc: 0.498199999332428, val_loss: 1.1776093244552612, val_acc: 0.4991999864578247,  lr: 0.009394743793103448
step: 2758, train_loss: 1.1101384162902832, acc: 0.5350000262260437, val_loss: 1.1832044124603271, val_acc: 0.4959999918937683,  lr: 0.009394399310344827
step: 2759, train_loss: 1.2097479104995728, acc: 0.48539999127388, val_loss: 1.1808927059173584, val_acc: 0.5004000067710876,  lr: 0.009394054827586207
step: 2760, train_loss: 1.126899242401123, acc: 0.5199999809265137, val_loss: 1.1841800212860107, val_acc: 0.5023999810218811,  lr: 0.009393710344827586
step: 2761, train_loss: 1.1850428581237793, acc: 0.49559998512268066, val_loss: 1.1898486614227295, val_acc: 0.4966000020503998,  lr: 0.009393365862068966
step: 2762, train_loss: 1.1190532445907593, acc: 0.5320000052452087, val_loss: 1.1862623691558838, val_acc: 0.5012000203132629,  lr: 0.009393021379310345
step: 2763, train_loss: 1.1602588891983032, acc: 0.5085999965667725, val_loss: 1.1810818910598755, val_acc: 0.5022000074386597,  lr: 0.009392676896551724
step: 2764, train_loss: 1.1083133220672607, acc: 0.5303999781608582, val_loss: 1.1902905702590942, val_acc: 0.5054000020027161,  lr: 0.009392332413793104
step: 2765, train_loss: 1.2150017023086548, acc: 0.48100000619888306, val_loss: 1.186698317527771, val_acc: 0.503600001335144,  lr: 0.009391987931034483
step: 2766, train_loss: 1.1453417539596558, acc: 0.5189999938011169, val_loss: 1.1815049648284912, val_acc: 0.5072000026702881,  lr: 0.009391643448275862
step: 2767, train_loss: 1.1261776685714722, acc: 0.5299999713897705, val_loss: 1.1915496587753296, val_acc: 0.5037999749183655,  lr: 0.009391298965517242
step: 2768, train_loss: 1.2501542568206787, acc: 0.46399998664855957, val_loss: 1.1857858896255493, val_acc: 0.506600022315979,  lr: 0.009390954482758621
step: 2769, train_loss: 1.1567658185958862, acc: 0.5055999755859375, val_loss: 1.1853379011154175, val_acc: 0.5031999945640564,  lr: 0.00939061
step: 2770, train_loss: 1.0956190824508667, acc: 0.5361999869346619, val_loss: 1.188321590423584, val_acc: 0.5054000020027161,  lr: 0.00939026551724138
step: 2771, train_loss: 1.1311968564987183, acc: 0.5231999754905701, val_loss: 1.189279317855835, val_acc: 0.49619999527931213,  lr: 0.00938992103448276
step: 2772, train_loss: 1.1234824657440186, acc: 0.5320000052452087, val_loss: 1.1916121244430542, val_acc: 0.4997999966144562,  lr: 0.009389576551724139
step: 2773, train_loss: 1.1043552160263062, acc: 0.5406000018119812, val_loss: 1.1928893327713013, val_acc: 0.4986000061035156,  lr: 0.009389232068965518
step: 2774, train_loss: 1.1953880786895752, acc: 0.5062000155448914, val_loss: 1.1891365051269531, val_acc: 0.5045999884605408,  lr: 0.009388887586206898
step: 2775, train_loss: 1.1742366552352905, acc: 0.5041999816894531, val_loss: 1.1938565969467163, val_acc: 0.503000020980835,  lr: 0.009388543103448277
step: 2776, train_loss: 1.138684630393982, acc: 0.5249999761581421, val_loss: 1.1967105865478516, val_acc: 0.501800000667572,  lr: 0.009388198620689655
step: 2777, train_loss: 1.1647059917449951, acc: 0.5138000249862671, val_loss: 1.1947565078735352, val_acc: 0.4950000047683716,  lr: 0.009387854137931034
step: 2778, train_loss: 1.126739740371704, acc: 0.525600016117096, val_loss: 1.1923134326934814, val_acc: 0.49459999799728394,  lr: 0.009387509655172413
step: 2779, train_loss: 1.2213842868804932, acc: 0.48179998993873596, val_loss: 1.196096420288086, val_acc: 0.4997999966144562,  lr: 0.009387165172413793
step: 2780, train_loss: 1.2590831518173218, acc: 0.4674000144004822, val_loss: 1.180984377861023, val_acc: 0.5008000135421753,  lr: 0.009386820689655172
step: 2781, train_loss: 1.1086913347244263, acc: 0.5360000133514404, val_loss: 1.1754229068756104, val_acc: 0.5026000142097473,  lr: 0.009386476206896551
step: 2782, train_loss: 1.1176326274871826, acc: 0.5400000214576721, val_loss: 1.1663874387741089, val_acc: 0.5026000142097473,  lr: 0.00938613172413793
step: 2783, train_loss: 1.1164098978042603, acc: 0.5234000086784363, val_loss: 1.16097891330719, val_acc: 0.5109999775886536,  lr: 0.00938578724137931
step: 2784, train_loss: 1.1884855031967163, acc: 0.4973999857902527, val_loss: 1.1695359945297241, val_acc: 0.5094000101089478,  lr: 0.00938544275862069
step: 2785, train_loss: 1.157774567604065, acc: 0.519599974155426, val_loss: 1.1674269437789917, val_acc: 0.5108000040054321,  lr: 0.009385098275862069
step: 2786, train_loss: 1.1131346225738525, acc: 0.5307999849319458, val_loss: 1.1723377704620361, val_acc: 0.5059999823570251,  lr: 0.009384753793103448
step: 2787, train_loss: 1.2329398393630981, acc: 0.4812000095844269, val_loss: 1.1735204458236694, val_acc: 0.5073999762535095,  lr: 0.009384409310344828
step: 2788, train_loss: 1.0851317644119263, acc: 0.5401999950408936, val_loss: 1.1733366250991821, val_acc: 0.5099999904632568,  lr: 0.009384064827586207
step: 2789, train_loss: 1.2321326732635498, acc: 0.4797999858856201, val_loss: 1.166207194328308, val_acc: 0.5126000046730042,  lr: 0.009383720344827587
step: 2790, train_loss: 1.1888457536697388, acc: 0.504800021648407, val_loss: 1.1711441278457642, val_acc: 0.5073999762535095,  lr: 0.009383375862068966
step: 2791, train_loss: 1.0850539207458496, acc: 0.5533999800682068, val_loss: 1.1621021032333374, val_acc: 0.5116000175476074,  lr: 0.009383031379310345
step: 2792, train_loss: 1.1725431680679321, acc: 0.49480000138282776, val_loss: 1.1574510335922241, val_acc: 0.510200023651123,  lr: 0.009382686896551725
step: 2793, train_loss: 1.1098806858062744, acc: 0.5379999876022339, val_loss: 1.1615065336227417, val_acc: 0.5113999843597412,  lr: 0.009382342413793104
step: 2794, train_loss: 1.1318166255950928, acc: 0.5284000039100647, val_loss: 1.1637201309204102, val_acc: 0.5058000087738037,  lr: 0.009381997931034483
step: 2795, train_loss: 1.1728955507278442, acc: 0.5044000148773193, val_loss: 1.160508394241333, val_acc: 0.5113999843597412,  lr: 0.009381653448275863
step: 2796, train_loss: 1.1668282747268677, acc: 0.5062000155448914, val_loss: 1.1619232892990112, val_acc: 0.5109999775886536,  lr: 0.009381308965517242
step: 2797, train_loss: 1.180591106414795, acc: 0.5054000020027161, val_loss: 1.1626241207122803, val_acc: 0.5109999775886536,  lr: 0.009380964482758622
step: 2798, train_loss: 1.1110039949417114, acc: 0.5410000085830688, val_loss: 1.158290147781372, val_acc: 0.5145999789237976,  lr: 0.009380620000000001
step: 2799, train_loss: 1.1576381921768188, acc: 0.5162000060081482, val_loss: 1.165785551071167, val_acc: 0.5077999830245972,  lr: 0.009380275517241379
step: 2800, train_loss: 1.10370934009552, acc: 0.5320000052452087, val_loss: 1.1721882820129395, val_acc: 0.5055999755859375,  lr: 0.009379931034482758
step: 2801, train_loss: 1.1531858444213867, acc: 0.5090000033378601, val_loss: 1.169822096824646, val_acc: 0.5126000046730042,  lr: 0.009379586551724137
step: 2802, train_loss: 1.0943912267684937, acc: 0.5401999950408936, val_loss: 1.172620415687561, val_acc: 0.506600022315979,  lr: 0.009379242068965517
step: 2803, train_loss: 1.0886039733886719, acc: 0.5404000282287598, val_loss: 1.1656594276428223, val_acc: 0.5037999749183655,  lr: 0.009378897586206896
step: 2804, train_loss: 1.1056805849075317, acc: 0.5415999889373779, val_loss: 1.1673344373703003, val_acc: 0.5026000142097473,  lr: 0.009378553103448276
step: 2805, train_loss: 1.0788313150405884, acc: 0.5475999712944031, val_loss: 1.1718279123306274, val_acc: 0.49779999256134033,  lr: 0.009378208620689655
step: 2806, train_loss: 1.2327251434326172, acc: 0.483599990606308, val_loss: 1.174412488937378, val_acc: 0.5062000155448914,  lr: 0.009377864137931034
step: 2807, train_loss: 1.2371116876602173, acc: 0.48399999737739563, val_loss: 1.1880130767822266, val_acc: 0.49880000948905945,  lr: 0.009377519655172414
step: 2808, train_loss: 1.1160590648651123, acc: 0.5303999781608582, val_loss: 1.1914920806884766, val_acc: 0.4912000000476837,  lr: 0.009377175172413793
step: 2809, train_loss: 1.1608980894088745, acc: 0.5112000107765198, val_loss: 1.1842327117919922, val_acc: 0.5026000142097473,  lr: 0.009376830689655172
step: 2810, train_loss: 1.1064363718032837, acc: 0.5382000207901001, val_loss: 1.184435248374939, val_acc: 0.5022000074386597,  lr: 0.009376486206896552
step: 2811, train_loss: 1.182327389717102, acc: 0.5040000081062317, val_loss: 1.1898682117462158, val_acc: 0.49959999322891235,  lr: 0.009376141724137931
step: 2812, train_loss: 1.182100772857666, acc: 0.5062000155448914, val_loss: 1.1895830631256104, val_acc: 0.49959999322891235,  lr: 0.00937579724137931
step: 2813, train_loss: 1.1740964651107788, acc: 0.5013999938964844, val_loss: 1.1921353340148926, val_acc: 0.5005999803543091,  lr: 0.00937545275862069
step: 2814, train_loss: 1.151305913925171, acc: 0.5212000012397766, val_loss: 1.1934804916381836, val_acc: 0.4943999946117401,  lr: 0.00937510827586207
step: 2815, train_loss: 1.103236198425293, acc: 0.5371999740600586, val_loss: 1.1967129707336426, val_acc: 0.49959999322891235,  lr: 0.009374763793103449
step: 2816, train_loss: 1.112516164779663, acc: 0.5303999781608582, val_loss: 1.184602975845337, val_acc: 0.504800021648407,  lr: 0.009374419310344828
step: 2817, train_loss: 1.1511967182159424, acc: 0.5253999829292297, val_loss: 1.1836415529251099, val_acc: 0.5095999836921692,  lr: 0.009374074827586207
step: 2818, train_loss: 1.1462222337722778, acc: 0.5149999856948853, val_loss: 1.1872735023498535, val_acc: 0.5085999965667725,  lr: 0.009373730344827587
step: 2819, train_loss: 1.2650035619735718, acc: 0.4731999933719635, val_loss: 1.1874489784240723, val_acc: 0.501800000667572,  lr: 0.009373385862068966
step: 2820, train_loss: 1.2380837202072144, acc: 0.48179998993873596, val_loss: 1.1813127994537354, val_acc: 0.4970000088214874,  lr: 0.009373041379310346
step: 2821, train_loss: 1.1445163488388062, acc: 0.5163999795913696, val_loss: 1.1799191236495972, val_acc: 0.4997999966144562,  lr: 0.009372696896551725
step: 2822, train_loss: 1.1535028219223022, acc: 0.5067999958992004, val_loss: 1.1804453134536743, val_acc: 0.5052000284194946,  lr: 0.009372352413793104
step: 2823, train_loss: 1.1830166578292847, acc: 0.5012000203132629, val_loss: 1.1810970306396484, val_acc: 0.5070000290870667,  lr: 0.009372007931034484
step: 2824, train_loss: 1.161545991897583, acc: 0.5127999782562256, val_loss: 1.188248634338379, val_acc: 0.5012000203132629,  lr: 0.009371663448275861
step: 2825, train_loss: 1.163192629814148, acc: 0.5131999850273132, val_loss: 1.1892105340957642, val_acc: 0.49799999594688416,  lr: 0.00937131896551724
step: 2826, train_loss: 1.2242060899734497, acc: 0.47780001163482666, val_loss: 1.1869882345199585, val_acc: 0.5005999803543091,  lr: 0.00937097448275862
step: 2827, train_loss: 1.180981159210205, acc: 0.5, val_loss: 1.1894311904907227, val_acc: 0.49799999594688416,  lr: 0.00937063
step: 2828, train_loss: 1.195875644683838, acc: 0.49720001220703125, val_loss: 1.1855599880218506, val_acc: 0.5052000284194946,  lr: 0.009370285517241379
step: 2829, train_loss: 1.1583149433135986, acc: 0.5163999795913696, val_loss: 1.187701940536499, val_acc: 0.503000020980835,  lr: 0.009369941034482758
step: 2830, train_loss: 1.1527783870697021, acc: 0.5221999883651733, val_loss: 1.187761664390564, val_acc: 0.5059999823570251,  lr: 0.009369596551724138
step: 2831, train_loss: 1.1169852018356323, acc: 0.532800018787384, val_loss: 1.1826876401901245, val_acc: 0.5081999897956848,  lr: 0.009369252068965517
step: 2832, train_loss: 1.1175956726074219, acc: 0.5285999774932861, val_loss: 1.18790602684021, val_acc: 0.5009999871253967,  lr: 0.009368907586206896
step: 2833, train_loss: 1.199068546295166, acc: 0.4997999966144562, val_loss: 1.1826876401901245, val_acc: 0.504800021648407,  lr: 0.009368563103448276
step: 2834, train_loss: 1.1377952098846436, acc: 0.5249999761581421, val_loss: 1.1819077730178833, val_acc: 0.5062000155448914,  lr: 0.009368218620689655
step: 2835, train_loss: 1.13410222530365, acc: 0.5264000296592712, val_loss: 1.196387529373169, val_acc: 0.4975999891757965,  lr: 0.009367874137931035
step: 2836, train_loss: 1.258007287979126, acc: 0.4657999873161316, val_loss: 1.178640604019165, val_acc: 0.5054000020027161,  lr: 0.009367529655172414
step: 2837, train_loss: 1.1355571746826172, acc: 0.5144000053405762, val_loss: 1.1793599128723145, val_acc: 0.5055999755859375,  lr: 0.009367185172413793
step: 2838, train_loss: 1.1910653114318848, acc: 0.49880000948905945, val_loss: 1.1884024143218994, val_acc: 0.492000013589859,  lr: 0.009366840689655173
step: 2839, train_loss: 1.218034029006958, acc: 0.48739999532699585, val_loss: 1.1932926177978516, val_acc: 0.4934000074863434,  lr: 0.009366496206896552
step: 2840, train_loss: 1.2048171758651733, acc: 0.49900001287460327, val_loss: 1.1943449974060059, val_acc: 0.49619999527931213,  lr: 0.009366151724137932
step: 2841, train_loss: 1.1240875720977783, acc: 0.5321999788284302, val_loss: 1.1861740350723267, val_acc: 0.49900001287460327,  lr: 0.009365807241379311
step: 2842, train_loss: 1.1100010871887207, acc: 0.5368000268936157, val_loss: 1.1887043714523315, val_acc: 0.503000020980835,  lr: 0.00936546275862069
step: 2843, train_loss: 1.223872184753418, acc: 0.487199991941452, val_loss: 1.1819941997528076, val_acc: 0.5022000074386597,  lr: 0.00936511827586207
step: 2844, train_loss: 1.1416352987289429, acc: 0.5228000283241272, val_loss: 1.1749883890151978, val_acc: 0.5,  lr: 0.009364773793103449
step: 2845, train_loss: 1.1320680379867554, acc: 0.5210000276565552, val_loss: 1.172143816947937, val_acc: 0.5098000168800354,  lr: 0.009364429310344828
step: 2846, train_loss: 1.1014997959136963, acc: 0.5410000085830688, val_loss: 1.179352045059204, val_acc: 0.5027999877929688,  lr: 0.009364084827586208
step: 2847, train_loss: 1.1998653411865234, acc: 0.49559998512268066, val_loss: 1.1756486892700195, val_acc: 0.5019999742507935,  lr: 0.009363740344827585
step: 2848, train_loss: 1.191758394241333, acc: 0.49140000343322754, val_loss: 1.1739232540130615, val_acc: 0.5034000277519226,  lr: 0.009363395862068965
step: 2849, train_loss: 1.1196129322052002, acc: 0.5275999903678894, val_loss: 1.1767230033874512, val_acc: 0.5023999810218811,  lr: 0.009363051379310344
step: 2850, train_loss: 1.1601393222808838, acc: 0.5131999850273132, val_loss: 1.1701395511627197, val_acc: 0.5088000297546387,  lr: 0.009362706896551724
step: 2851, train_loss: 1.166835069656372, acc: 0.506600022315979, val_loss: 1.1711382865905762, val_acc: 0.5152000188827515,  lr: 0.009362362413793103
step: 2852, train_loss: 1.0994008779525757, acc: 0.5356000065803528, val_loss: 1.1679831743240356, val_acc: 0.5113999843597412,  lr: 0.009362017931034482
step: 2853, train_loss: 1.1920067071914673, acc: 0.49639999866485596, val_loss: 1.1698269844055176, val_acc: 0.5135999917984009,  lr: 0.009361673448275862
step: 2854, train_loss: 1.2030272483825684, acc: 0.4943999946117401, val_loss: 1.1719287633895874, val_acc: 0.5052000284194946,  lr: 0.009361328965517241
step: 2855, train_loss: 1.1037567853927612, acc: 0.5370000004768372, val_loss: 1.174067735671997, val_acc: 0.5049999952316284,  lr: 0.00936098448275862
step: 2856, train_loss: 1.128860592842102, acc: 0.5239999890327454, val_loss: 1.1767178773880005, val_acc: 0.5059999823570251,  lr: 0.00936064
step: 2857, train_loss: 1.1446468830108643, acc: 0.5185999870300293, val_loss: 1.1747177839279175, val_acc: 0.5049999952316284,  lr: 0.00936029551724138
step: 2858, train_loss: 1.1171486377716064, acc: 0.525600016117096, val_loss: 1.1795817613601685, val_acc: 0.5076000094413757,  lr: 0.009359951034482759
step: 2859, train_loss: 1.1193352937698364, acc: 0.527999997138977, val_loss: 1.176375389099121, val_acc: 0.5090000033378601,  lr: 0.009359606551724138
step: 2860, train_loss: 1.1868518590927124, acc: 0.5044000148773193, val_loss: 1.1731598377227783, val_acc: 0.5167999863624573,  lr: 0.009359262068965517
step: 2861, train_loss: 1.1576976776123047, acc: 0.515999972820282, val_loss: 1.187407374382019, val_acc: 0.5,  lr: 0.009358917586206897
step: 2862, train_loss: 1.1368621587753296, acc: 0.5289999842643738, val_loss: 1.1898771524429321, val_acc: 0.5044000148773193,  lr: 0.009358573103448276
step: 2863, train_loss: 1.1139452457427979, acc: 0.5392000079154968, val_loss: 1.1923550367355347, val_acc: 0.4959999918937683,  lr: 0.009358228620689656
step: 2864, train_loss: 1.1875262260437012, acc: 0.4896000027656555, val_loss: 1.1965078115463257, val_acc: 0.49160000681877136,  lr: 0.009357884137931035
step: 2865, train_loss: 1.1393307447433472, acc: 0.517799973487854, val_loss: 1.1870923042297363, val_acc: 0.4973999857902527,  lr: 0.009357539655172414
step: 2866, train_loss: 1.1430585384368896, acc: 0.524399995803833, val_loss: 1.1886135339736938, val_acc: 0.49380001425743103,  lr: 0.009357195172413794
step: 2867, train_loss: 1.1028207540512085, acc: 0.5379999876022339, val_loss: 1.1964796781539917, val_acc: 0.4903999865055084,  lr: 0.009356850689655173
step: 2868, train_loss: 1.1258981227874756, acc: 0.5289999842643738, val_loss: 1.1990330219268799, val_acc: 0.4936000108718872,  lr: 0.009356506206896553
step: 2869, train_loss: 1.1319619417190552, acc: 0.525600016117096, val_loss: 1.1897236108779907, val_acc: 0.5,  lr: 0.009356161724137932
step: 2870, train_loss: 1.1187703609466553, acc: 0.5357999801635742, val_loss: 1.183175802230835, val_acc: 0.5013999938964844,  lr: 0.009355817241379311
step: 2871, train_loss: 1.1572140455245972, acc: 0.515999972820282, val_loss: 1.1900211572647095, val_acc: 0.4959999918937683,  lr: 0.00935547275862069
step: 2872, train_loss: 1.106492042541504, acc: 0.5383999943733215, val_loss: 1.1892657279968262, val_acc: 0.4975999891757965,  lr: 0.00935512827586207
step: 2873, train_loss: 1.0886503458023071, acc: 0.5400000214576721, val_loss: 1.1905792951583862, val_acc: 0.4973999857902527,  lr: 0.009354783793103448
step: 2874, train_loss: 1.2548296451568604, acc: 0.4726000130176544, val_loss: 1.190018653869629, val_acc: 0.5005999803543091,  lr: 0.009354439310344827
step: 2875, train_loss: 1.0791350603103638, acc: 0.5432000160217285, val_loss: 1.1958388090133667, val_acc: 0.498199999332428,  lr: 0.009354094827586206
step: 2876, train_loss: 1.1033859252929688, acc: 0.531000018119812, val_loss: 1.1915466785430908, val_acc: 0.4968000054359436,  lr: 0.009353750344827586
step: 2877, train_loss: 1.213520884513855, acc: 0.47999998927116394, val_loss: 1.1956729888916016, val_acc: 0.4957999885082245,  lr: 0.009353405862068965
step: 2878, train_loss: 1.1067101955413818, acc: 0.5383999943733215, val_loss: 1.2056269645690918, val_acc: 0.49559998512268066,  lr: 0.009353061379310345
step: 2879, train_loss: 1.09088134765625, acc: 0.5374000072479248, val_loss: 1.2078883647918701, val_acc: 0.49779999256134033,  lr: 0.009352716896551724
step: 2880, train_loss: 1.1184498071670532, acc: 0.5335999727249146, val_loss: 1.2040780782699585, val_acc: 0.5005999803543091,  lr: 0.009352372413793103
step: 2881, train_loss: 1.105418086051941, acc: 0.5440000295639038, val_loss: 1.2009433507919312, val_acc: 0.5027999877929688,  lr: 0.009352027931034483
step: 2882, train_loss: 1.1821140050888062, acc: 0.5001999735832214, val_loss: 1.1974749565124512, val_acc: 0.5,  lr: 0.009351683448275862
step: 2883, train_loss: 1.1203757524490356, acc: 0.5267999768257141, val_loss: 1.195125937461853, val_acc: 0.49140000343322754,  lr: 0.009351338965517242
step: 2884, train_loss: 1.09083092212677, acc: 0.5404000282287598, val_loss: 1.1947910785675049, val_acc: 0.4991999864578247,  lr: 0.009350994482758621
step: 2885, train_loss: 1.2445546388626099, acc: 0.4722000062465668, val_loss: 1.1944081783294678, val_acc: 0.5031999945640564,  lr: 0.00935065
step: 2886, train_loss: 1.1106927394866943, acc: 0.5388000011444092, val_loss: 1.1847095489501953, val_acc: 0.510200023651123,  lr: 0.00935030551724138
step: 2887, train_loss: 1.2290611267089844, acc: 0.4867999851703644, val_loss: 1.1793307065963745, val_acc: 0.5073999762535095,  lr: 0.009349961034482759
step: 2888, train_loss: 1.1636024713516235, acc: 0.51419997215271, val_loss: 1.1774905920028687, val_acc: 0.5008000135421753,  lr: 0.009349616551724138
step: 2889, train_loss: 1.2343467473983765, acc: 0.4715999960899353, val_loss: 1.1826272010803223, val_acc: 0.49939998984336853,  lr: 0.009349272068965518
step: 2890, train_loss: 1.1675790548324585, acc: 0.5019999742507935, val_loss: 1.1754909753799438, val_acc: 0.5009999871253967,  lr: 0.009348927586206897
step: 2891, train_loss: 1.0948834419250488, acc: 0.5374000072479248, val_loss: 1.1772480010986328, val_acc: 0.5052000284194946,  lr: 0.009348583103448277
step: 2892, train_loss: 1.2198635339736938, acc: 0.48019999265670776, val_loss: 1.178652048110962, val_acc: 0.5041999816894531,  lr: 0.009348238620689656
step: 2893, train_loss: 1.1152909994125366, acc: 0.526199996471405, val_loss: 1.170864224433899, val_acc: 0.5063999891281128,  lr: 0.009347894137931035
step: 2894, train_loss: 1.1813291311264038, acc: 0.48840001225471497, val_loss: 1.1722657680511475, val_acc: 0.5134000182151794,  lr: 0.009347549655172415
step: 2895, train_loss: 1.2122840881347656, acc: 0.4880000054836273, val_loss: 1.1627979278564453, val_acc: 0.5144000053405762,  lr: 0.009347205172413792
step: 2896, train_loss: 1.1777724027633667, acc: 0.5012000203132629, val_loss: 1.1637967824935913, val_acc: 0.5139999985694885,  lr: 0.009346860689655172
step: 2897, train_loss: 1.1027987003326416, acc: 0.5365999937057495, val_loss: 1.1678149700164795, val_acc: 0.5099999904632568,  lr: 0.009346516206896551
step: 2898, train_loss: 1.1459015607833862, acc: 0.5138000249862671, val_loss: 1.1654481887817383, val_acc: 0.5127999782562256,  lr: 0.00934617172413793
step: 2899, train_loss: 1.1386252641677856, acc: 0.5353999733924866, val_loss: 1.1620393991470337, val_acc: 0.51419997215271,  lr: 0.00934582724137931
step: 2900, train_loss: 1.1806389093399048, acc: 0.4968000054359436, val_loss: 1.1573511362075806, val_acc: 0.5203999876976013,  lr: 0.00934548275862069
step: 2901, train_loss: 1.1187806129455566, acc: 0.5198000073432922, val_loss: 1.1567445993423462, val_acc: 0.5194000005722046,  lr: 0.009345138275862069
step: 2902, train_loss: 1.1087692975997925, acc: 0.5382000207901001, val_loss: 1.154634952545166, val_acc: 0.5217999815940857,  lr: 0.009344793793103448
step: 2903, train_loss: 1.1599936485290527, acc: 0.5134000182151794, val_loss: 1.1602013111114502, val_acc: 0.5152000188827515,  lr: 0.009344449310344827
step: 2904, train_loss: 1.2003605365753174, acc: 0.49480000138282776, val_loss: 1.1655497550964355, val_acc: 0.5157999992370605,  lr: 0.009344104827586207
step: 2905, train_loss: 1.116165280342102, acc: 0.5335999727249146, val_loss: 1.1668460369110107, val_acc: 0.5138000249862671,  lr: 0.009343760344827586
step: 2906, train_loss: 1.1510426998138428, acc: 0.5242000222206116, val_loss: 1.172971487045288, val_acc: 0.5123999714851379,  lr: 0.009343415862068966
step: 2907, train_loss: 1.215360164642334, acc: 0.4867999851703644, val_loss: 1.1867609024047852, val_acc: 0.5055999755859375,  lr: 0.009343071379310345
step: 2908, train_loss: 1.1231633424758911, acc: 0.5299999713897705, val_loss: 1.1781511306762695, val_acc: 0.5090000033378601,  lr: 0.009342726896551724
step: 2909, train_loss: 1.1917449235916138, acc: 0.5005999803543091, val_loss: 1.1861116886138916, val_acc: 0.501800000667572,  lr: 0.009342382413793104
step: 2910, train_loss: 1.204028606414795, acc: 0.49059998989105225, val_loss: 1.1898174285888672, val_acc: 0.5076000094413757,  lr: 0.009342037931034483
step: 2911, train_loss: 1.2534763813018799, acc: 0.4758000075817108, val_loss: 1.1970514059066772, val_acc: 0.5005999803543091,  lr: 0.009341693448275862
step: 2912, train_loss: 1.218993902206421, acc: 0.47920000553131104, val_loss: 1.1975332498550415, val_acc: 0.49779999256134033,  lr: 0.009341348965517242
step: 2913, train_loss: 1.1630353927612305, acc: 0.5090000033378601, val_loss: 1.1931970119476318, val_acc: 0.498199999332428,  lr: 0.009341004482758621
step: 2914, train_loss: 1.1225471496582031, acc: 0.5299999713897705, val_loss: 1.181653618812561, val_acc: 0.5059999823570251,  lr: 0.00934066
step: 2915, train_loss: 1.1303000450134277, acc: 0.5297999978065491, val_loss: 1.1801512241363525, val_acc: 0.5045999884605408,  lr: 0.00934031551724138
step: 2916, train_loss: 1.1187940835952759, acc: 0.525600016117096, val_loss: 1.1769119501113892, val_acc: 0.5067999958992004,  lr: 0.00933997103448276
step: 2917, train_loss: 1.1136412620544434, acc: 0.5302000045776367, val_loss: 1.1793057918548584, val_acc: 0.5099999904632568,  lr: 0.009339626551724139
step: 2918, train_loss: 1.1961408853530884, acc: 0.4887999892234802, val_loss: 1.1909137964248657, val_acc: 0.5041999816894531,  lr: 0.009339282068965518
step: 2919, train_loss: 1.112717628479004, acc: 0.5415999889373779, val_loss: 1.1940323114395142, val_acc: 0.5052000284194946,  lr: 0.009338937586206898
step: 2920, train_loss: 1.1668508052825928, acc: 0.503600001335144, val_loss: 1.1913561820983887, val_acc: 0.4991999864578247,  lr: 0.009338593103448277
step: 2921, train_loss: 1.1748106479644775, acc: 0.5094000101089478, val_loss: 1.1992087364196777, val_acc: 0.49140000343322754,  lr: 0.009338248620689655
step: 2922, train_loss: 1.107291340827942, acc: 0.5407999753952026, val_loss: 1.1910219192504883, val_acc: 0.503000020980835,  lr: 0.009337904137931034
step: 2923, train_loss: 1.1801081895828247, acc: 0.503600001335144, val_loss: 1.1852550506591797, val_acc: 0.5112000107765198,  lr: 0.009337559655172413
step: 2924, train_loss: 1.1337424516677856, acc: 0.526199996471405, val_loss: 1.1797423362731934, val_acc: 0.5098000168800354,  lr: 0.009337215172413793
step: 2925, train_loss: 1.2034273147583008, acc: 0.48980000615119934, val_loss: 1.1837427616119385, val_acc: 0.5009999871253967,  lr: 0.009336870689655172
step: 2926, train_loss: 1.1911002397537231, acc: 0.5023999810218811, val_loss: 1.186023235321045, val_acc: 0.5004000067710876,  lr: 0.009336526206896551
step: 2927, train_loss: 1.1268303394317627, acc: 0.5329999923706055, val_loss: 1.1901639699935913, val_acc: 0.5044000148773193,  lr: 0.00933618172413793
step: 2928, train_loss: 1.2145037651062012, acc: 0.49480000138282776, val_loss: 1.1824924945831299, val_acc: 0.5072000026702881,  lr: 0.00933583724137931
step: 2929, train_loss: 1.0813039541244507, acc: 0.5415999889373779, val_loss: 1.1861870288848877, val_acc: 0.5034000277519226,  lr: 0.00933549275862069
step: 2930, train_loss: 1.121382474899292, acc: 0.5321999788284302, val_loss: 1.1822339296340942, val_acc: 0.5077999830245972,  lr: 0.009335148275862069
step: 2931, train_loss: 1.116771936416626, acc: 0.5383999943733215, val_loss: 1.183302402496338, val_acc: 0.5062000155448914,  lr: 0.009334803793103448
step: 2932, train_loss: 1.1837369203567505, acc: 0.4912000000476837, val_loss: 1.1850721836090088, val_acc: 0.5045999884605408,  lr: 0.009334459310344828
step: 2933, train_loss: 1.1783885955810547, acc: 0.5031999945640564, val_loss: 1.182525634765625, val_acc: 0.5077999830245972,  lr: 0.009334114827586207
step: 2934, train_loss: 1.2445151805877686, acc: 0.4706000089645386, val_loss: 1.1881239414215088, val_acc: 0.4991999864578247,  lr: 0.009333770344827587
step: 2935, train_loss: 1.0992937088012695, acc: 0.5415999889373779, val_loss: 1.187913179397583, val_acc: 0.4986000061035156,  lr: 0.009333425862068966
step: 2936, train_loss: 1.1131001710891724, acc: 0.5375999808311462, val_loss: 1.1955385208129883, val_acc: 0.5005999803543091,  lr: 0.009333081379310345
step: 2937, train_loss: 1.2227973937988281, acc: 0.47999998927116394, val_loss: 1.1861542463302612, val_acc: 0.5109999775886536,  lr: 0.009332736896551725
step: 2938, train_loss: 1.1716413497924805, acc: 0.5157999992370605, val_loss: 1.1950552463531494, val_acc: 0.5062000155448914,  lr: 0.009332392413793104
step: 2939, train_loss: 1.1834542751312256, acc: 0.49880000948905945, val_loss: 1.193589210510254, val_acc: 0.5022000074386597,  lr: 0.009332047931034483
step: 2940, train_loss: 1.2191046476364136, acc: 0.4848000109195709, val_loss: 1.2000560760498047, val_acc: 0.49939998984336853,  lr: 0.009331703448275863
step: 2941, train_loss: 1.151780605316162, acc: 0.5249999761581421, val_loss: 1.1967082023620605, val_acc: 0.49459999799728394,  lr: 0.009331358965517242
step: 2942, train_loss: 1.1300861835479736, acc: 0.5239999890327454, val_loss: 1.201661229133606, val_acc: 0.4952000081539154,  lr: 0.009331014482758622
step: 2943, train_loss: 1.1034653186798096, acc: 0.5350000262260437, val_loss: 1.2008945941925049, val_acc: 0.49480000138282776,  lr: 0.009330670000000001
step: 2944, train_loss: 1.1870571374893188, acc: 0.5037999749183655, val_loss: 1.1974867582321167, val_acc: 0.5019999742507935,  lr: 0.009330325517241379
step: 2945, train_loss: 1.262244701385498, acc: 0.45899999141693115, val_loss: 1.2000494003295898, val_acc: 0.5041999816894531,  lr: 0.009329981034482758
step: 2946, train_loss: 1.1069363355636597, acc: 0.5288000106811523, val_loss: 1.1949717998504639, val_acc: 0.5013999938964844,  lr: 0.009329636551724137
step: 2947, train_loss: 1.1465198993682861, acc: 0.517799973487854, val_loss: 1.2011475563049316, val_acc: 0.49239999055862427,  lr: 0.009329292068965517
step: 2948, train_loss: 1.131316065788269, acc: 0.5216000080108643, val_loss: 1.1993101835250854, val_acc: 0.49779999256134033,  lr: 0.009328947586206896
step: 2949, train_loss: 1.1746454238891602, acc: 0.5013999938964844, val_loss: 1.2011817693710327, val_acc: 0.49459999799728394,  lr: 0.009328603103448276
step: 2950, train_loss: 1.1898349523544312, acc: 0.5009999871253967, val_loss: 1.2179673910140991, val_acc: 0.48660001158714294,  lr: 0.009328258620689655
step: 2951, train_loss: 1.1495912075042725, acc: 0.5210000276565552, val_loss: 1.2094311714172363, val_acc: 0.4896000027656555,  lr: 0.009327914137931034
step: 2952, train_loss: 1.129335880279541, acc: 0.5275999903678894, val_loss: 1.2056013345718384, val_acc: 0.4896000027656555,  lr: 0.009327569655172414
step: 2953, train_loss: 1.1327975988388062, acc: 0.5297999978065491, val_loss: 1.198185682296753, val_acc: 0.4973999857902527,  lr: 0.009327225172413793
step: 2954, train_loss: 1.1347852945327759, acc: 0.5188000202178955, val_loss: 1.2093431949615479, val_acc: 0.4887999892234802,  lr: 0.009326880689655172
step: 2955, train_loss: 1.2033641338348389, acc: 0.4837999939918518, val_loss: 1.2045506238937378, val_acc: 0.48980000615119934,  lr: 0.009326536206896552
step: 2956, train_loss: 1.1227734088897705, acc: 0.531000018119812, val_loss: 1.2043756246566772, val_acc: 0.49300000071525574,  lr: 0.009326191724137931
step: 2957, train_loss: 1.182288408279419, acc: 0.490200012922287, val_loss: 1.20574152469635, val_acc: 0.4909999966621399,  lr: 0.00932584724137931
step: 2958, train_loss: 1.150683045387268, acc: 0.5174000263214111, val_loss: 1.2085886001586914, val_acc: 0.49619999527931213,  lr: 0.00932550275862069
step: 2959, train_loss: 1.1573282480239868, acc: 0.5123999714851379, val_loss: 1.2060500383377075, val_acc: 0.4966000020503998,  lr: 0.00932515827586207
step: 2960, train_loss: 1.1341248750686646, acc: 0.5216000080108643, val_loss: 1.21280038356781, val_acc: 0.4957999885082245,  lr: 0.009324813793103449
step: 2961, train_loss: 1.1089304685592651, acc: 0.532800018787384, val_loss: 1.2071962356567383, val_acc: 0.49559998512268066,  lr: 0.009324469310344828
step: 2962, train_loss: 1.101616382598877, acc: 0.5365999937057495, val_loss: 1.2120853662490845, val_acc: 0.4887999892234802,  lr: 0.009324124827586208
step: 2963, train_loss: 1.1089470386505127, acc: 0.5311999917030334, val_loss: 1.2095435857772827, val_acc: 0.49300000071525574,  lr: 0.009323780344827587
step: 2964, train_loss: 1.0921753644943237, acc: 0.5496000051498413, val_loss: 1.2055022716522217, val_acc: 0.49459999799728394,  lr: 0.009323435862068966
step: 2965, train_loss: 1.089792013168335, acc: 0.5432000160217285, val_loss: 1.2078231573104858, val_acc: 0.487199991941452,  lr: 0.009323091379310346
step: 2966, train_loss: 1.1663594245910645, acc: 0.5090000033378601, val_loss: 1.2137231826782227, val_acc: 0.49000000953674316,  lr: 0.009322746896551725
step: 2967, train_loss: 1.1812528371810913, acc: 0.5077999830245972, val_loss: 1.2046676874160767, val_acc: 0.49300000071525574,  lr: 0.009322402413793104
step: 2968, train_loss: 1.2518572807312012, acc: 0.4643999934196472, val_loss: 1.2047924995422363, val_acc: 0.4966000020503998,  lr: 0.009322057931034484
step: 2969, train_loss: 1.1071536540985107, acc: 0.5356000065803528, val_loss: 1.1994198560714722, val_acc: 0.4975999891757965,  lr: 0.009321713448275861
step: 2970, train_loss: 1.1473292112350464, acc: 0.5134000182151794, val_loss: 1.1937214136123657, val_acc: 0.4968000054359436,  lr: 0.00932136896551724
step: 2971, train_loss: 1.1574358940124512, acc: 0.5185999870300293, val_loss: 1.1957597732543945, val_acc: 0.4991999864578247,  lr: 0.00932102448275862
step: 2972, train_loss: 1.1377182006835938, acc: 0.5239999890327454, val_loss: 1.1903274059295654, val_acc: 0.5009999871253967,  lr: 0.00932068
step: 2973, train_loss: 1.0986076593399048, acc: 0.5389999747276306, val_loss: 1.1876120567321777, val_acc: 0.5022000074386597,  lr: 0.009320335517241379
step: 2974, train_loss: 1.0783202648162842, acc: 0.5533999800682068, val_loss: 1.186156988143921, val_acc: 0.5044000148773193,  lr: 0.009319991034482758
step: 2975, train_loss: 1.1449025869369507, acc: 0.5217999815940857, val_loss: 1.1847525835037231, val_acc: 0.5040000081062317,  lr: 0.009319646551724138
step: 2976, train_loss: 1.097595453262329, acc: 0.5464000105857849, val_loss: 1.1846753358840942, val_acc: 0.5013999938964844,  lr: 0.009319302068965517
step: 2977, train_loss: 1.1117210388183594, acc: 0.5383999943733215, val_loss: 1.1821436882019043, val_acc: 0.49939998984336853,  lr: 0.009318957586206897
step: 2978, train_loss: 1.1185691356658936, acc: 0.5299999713897705, val_loss: 1.1800642013549805, val_acc: 0.5031999945640564,  lr: 0.009318613103448276
step: 2979, train_loss: 1.127790093421936, acc: 0.5264000296592712, val_loss: 1.1806896924972534, val_acc: 0.5027999877929688,  lr: 0.009318268620689655
step: 2980, train_loss: 1.0987427234649658, acc: 0.5321999788284302, val_loss: 1.1816462278366089, val_acc: 0.5034000277519226,  lr: 0.009317924137931035
step: 2981, train_loss: 1.086525321006775, acc: 0.5465999841690063, val_loss: 1.193928837776184, val_acc: 0.4934000074863434,  lr: 0.009317579655172414
step: 2982, train_loss: 1.0865662097930908, acc: 0.5515999794006348, val_loss: 1.191347360610962, val_acc: 0.4966000020503998,  lr: 0.009317235172413793
step: 2983, train_loss: 1.2068864107131958, acc: 0.4896000027656555, val_loss: 1.1855649948120117, val_acc: 0.49959999322891235,  lr: 0.009316890689655173
step: 2984, train_loss: 1.1390024423599243, acc: 0.5253999829292297, val_loss: 1.1774884462356567, val_acc: 0.5013999938964844,  lr: 0.009316546206896552
step: 2985, train_loss: 1.2180497646331787, acc: 0.4909999966621399, val_loss: 1.192382574081421, val_acc: 0.4984000027179718,  lr: 0.009316201724137932
step: 2986, train_loss: 1.24605393409729, acc: 0.4747999906539917, val_loss: 1.1856253147125244, val_acc: 0.49799999594688416,  lr: 0.009315857241379311
step: 2987, train_loss: 1.1942681074142456, acc: 0.4927999973297119, val_loss: 1.1934524774551392, val_acc: 0.5008000135421753,  lr: 0.00931551275862069
step: 2988, train_loss: 1.1527259349822998, acc: 0.5098000168800354, val_loss: 1.1766486167907715, val_acc: 0.5037999749183655,  lr: 0.00931516827586207
step: 2989, train_loss: 1.162950038909912, acc: 0.5135999917984009, val_loss: 1.179017424583435, val_acc: 0.506600022315979,  lr: 0.009314823793103449
step: 2990, train_loss: 1.154626727104187, acc: 0.5127999782562256, val_loss: 1.1870568990707397, val_acc: 0.5067999958992004,  lr: 0.009314479310344828
step: 2991, train_loss: 1.2574715614318848, acc: 0.4722000062465668, val_loss: 1.1793193817138672, val_acc: 0.5063999891281128,  lr: 0.009314134827586208
step: 2992, train_loss: 1.1774598360061646, acc: 0.5080000162124634, val_loss: 1.1743477582931519, val_acc: 0.5072000026702881,  lr: 0.009313790344827585
step: 2993, train_loss: 1.2237805128097534, acc: 0.48539999127388, val_loss: 1.1634551286697388, val_acc: 0.5073999762535095,  lr: 0.009313445862068965
step: 2994, train_loss: 1.1238824129104614, acc: 0.5347999930381775, val_loss: 1.1509881019592285, val_acc: 0.5188000202178955,  lr: 0.009313101379310344
step: 2995, train_loss: 1.1903108358383179, acc: 0.5040000081062317, val_loss: 1.141116738319397, val_acc: 0.5235999822616577,  lr: 0.009312756896551724
step: 2996, train_loss: 1.1615327596664429, acc: 0.5121999979019165, val_loss: 1.1448122262954712, val_acc: 0.5181999802589417,  lr: 0.009312412413793103
step: 2997, train_loss: 1.1264617443084717, acc: 0.517799973487854, val_loss: 1.1442391872406006, val_acc: 0.5194000005722046,  lr: 0.009312067931034482
step: 2998, train_loss: 1.1407009363174438, acc: 0.5260000228881836, val_loss: 1.1478561162948608, val_acc: 0.5166000127792358,  lr: 0.009311723448275862
step: 2999, train_loss: 1.1268008947372437, acc: 0.5257999897003174, val_loss: 1.1477409601211548, val_acc: 0.517799973487854,  lr: 0.009311378965517241
step: 3000, train_loss: 1.1207847595214844, acc: 0.52920001745224, val_loss: 1.1427377462387085, val_acc: 0.524399995803833,  lr: 0.00931103448275862
step: 3001, train_loss: 1.1088231801986694, acc: 0.5386000275611877, val_loss: 1.1408311128616333, val_acc: 0.5230000019073486,  lr: 0.00931069
step: 3002, train_loss: 1.0800800323486328, acc: 0.5388000011444092, val_loss: 1.1445112228393555, val_acc: 0.5194000005722046,  lr: 0.00931034551724138
step: 3003, train_loss: 1.1253845691680908, acc: 0.532800018787384, val_loss: 1.1473182439804077, val_acc: 0.5212000012397766,  lr: 0.009310001034482759
step: 3004, train_loss: 1.2515461444854736, acc: 0.47540000081062317, val_loss: 1.160920262336731, val_acc: 0.51419997215271,  lr: 0.009309656551724138
step: 3005, train_loss: 1.1758989095687866, acc: 0.5004000067710876, val_loss: 1.1602295637130737, val_acc: 0.5121999979019165,  lr: 0.009309312068965517
step: 3006, train_loss: 1.1808220148086548, acc: 0.5054000020027161, val_loss: 1.1617276668548584, val_acc: 0.5127999782562256,  lr: 0.009308967586206897
step: 3007, train_loss: 1.1256797313690186, acc: 0.526199996471405, val_loss: 1.1583360433578491, val_acc: 0.5162000060081482,  lr: 0.009308623103448276
step: 3008, train_loss: 1.1109708547592163, acc: 0.5386000275611877, val_loss: 1.1579536199569702, val_acc: 0.5175999999046326,  lr: 0.009308278620689656
step: 3009, train_loss: 1.185694694519043, acc: 0.4968000054359436, val_loss: 1.1585990190505981, val_acc: 0.5157999992370605,  lr: 0.009307934137931035
step: 3010, train_loss: 1.2063056230545044, acc: 0.4869999885559082, val_loss: 1.1624953746795654, val_acc: 0.5094000101089478,  lr: 0.009307589655172414
step: 3011, train_loss: 1.1017568111419678, acc: 0.5386000275611877, val_loss: 1.1596060991287231, val_acc: 0.5108000040054321,  lr: 0.009307245172413794
step: 3012, train_loss: 1.2303801774978638, acc: 0.4828000068664551, val_loss: 1.1597861051559448, val_acc: 0.5109999775886536,  lr: 0.009306900689655173
step: 3013, train_loss: 1.2080883979797363, acc: 0.47920000553131104, val_loss: 1.166629433631897, val_acc: 0.5054000020027161,  lr: 0.009306556206896553
step: 3014, train_loss: 1.1453255414962769, acc: 0.5188000202178955, val_loss: 1.1674469709396362, val_acc: 0.5013999938964844,  lr: 0.009306211724137932
step: 3015, train_loss: 1.1036871671676636, acc: 0.5397999882698059, val_loss: 1.1701682806015015, val_acc: 0.5059999823570251,  lr: 0.009305867241379311
step: 3016, train_loss: 1.15241539478302, acc: 0.520799994468689, val_loss: 1.1653528213500977, val_acc: 0.5145999789237976,  lr: 0.00930552275862069
step: 3017, train_loss: 1.1373186111450195, acc: 0.5126000046730042, val_loss: 1.169223427772522, val_acc: 0.5091999769210815,  lr: 0.00930517827586207
step: 3018, train_loss: 1.1199427843093872, acc: 0.5350000262260437, val_loss: 1.1811176538467407, val_acc: 0.5012000203132629,  lr: 0.009304833793103448
step: 3019, train_loss: 1.198718547821045, acc: 0.5009999871253967, val_loss: 1.1716959476470947, val_acc: 0.508400022983551,  lr: 0.009304489310344827
step: 3020, train_loss: 1.1433355808258057, acc: 0.5171999931335449, val_loss: 1.178148627281189, val_acc: 0.5045999884605408,  lr: 0.009304144827586206
step: 3021, train_loss: 1.222199559211731, acc: 0.48159998655319214, val_loss: 1.1714829206466675, val_acc: 0.5144000053405762,  lr: 0.009303800344827586
step: 3022, train_loss: 1.1799832582473755, acc: 0.4943999946117401, val_loss: 1.1831341981887817, val_acc: 0.5019999742507935,  lr: 0.009303455862068965
step: 3023, train_loss: 1.2084976434707642, acc: 0.49380001425743103, val_loss: 1.1719112396240234, val_acc: 0.5113999843597412,  lr: 0.009303111379310345
step: 3024, train_loss: 1.111877679824829, acc: 0.5296000242233276, val_loss: 1.1791951656341553, val_acc: 0.5063999891281128,  lr: 0.009302766896551724
step: 3025, train_loss: 1.1373687982559204, acc: 0.5180000066757202, val_loss: 1.1754138469696045, val_acc: 0.5199999809265137,  lr: 0.009302422413793103
step: 3026, train_loss: 1.1242185831069946, acc: 0.5239999890327454, val_loss: 1.1819543838500977, val_acc: 0.5058000087738037,  lr: 0.009302077931034483
step: 3027, train_loss: 1.0997282266616821, acc: 0.5414000153541565, val_loss: 1.1814769506454468, val_acc: 0.503600001335144,  lr: 0.009301733448275862
step: 3028, train_loss: 1.240146279335022, acc: 0.4837999939918518, val_loss: 1.1912189722061157, val_acc: 0.5009999871253967,  lr: 0.009301388965517242
step: 3029, train_loss: 1.1904023885726929, acc: 0.4903999865055084, val_loss: 1.1950892210006714, val_acc: 0.4966000020503998,  lr: 0.009301044482758621
step: 3030, train_loss: 1.1494083404541016, acc: 0.520799994468689, val_loss: 1.1814781427383423, val_acc: 0.4997999966144562,  lr: 0.0093007
step: 3031, train_loss: 1.1113440990447998, acc: 0.5400000214576721, val_loss: 1.1782984733581543, val_acc: 0.5008000135421753,  lr: 0.00930035551724138
step: 3032, train_loss: 1.1553858518600464, acc: 0.5174000263214111, val_loss: 1.1773161888122559, val_acc: 0.5027999877929688,  lr: 0.009300011034482759
step: 3033, train_loss: 1.0987820625305176, acc: 0.5353999733924866, val_loss: 1.1828454732894897, val_acc: 0.5005999803543091,  lr: 0.009299666551724138
step: 3034, train_loss: 1.1880477666854858, acc: 0.49219998717308044, val_loss: 1.1805857419967651, val_acc: 0.5052000284194946,  lr: 0.009299322068965518
step: 3035, train_loss: 1.1337193250656128, acc: 0.5228000283241272, val_loss: 1.1777106523513794, val_acc: 0.5026000142097473,  lr: 0.009298977586206897
step: 3036, train_loss: 1.1573017835617065, acc: 0.5113999843597412, val_loss: 1.179077386856079, val_acc: 0.5041999816894531,  lr: 0.009298633103448277
step: 3037, train_loss: 1.1067147254943848, acc: 0.5374000072479248, val_loss: 1.180009126663208, val_acc: 0.5019999742507935,  lr: 0.009298288620689656
step: 3038, train_loss: 1.152885913848877, acc: 0.5228000283241272, val_loss: 1.1785227060317993, val_acc: 0.5019999742507935,  lr: 0.009297944137931035
step: 3039, train_loss: 1.144749402999878, acc: 0.5180000066757202, val_loss: 1.1785329580307007, val_acc: 0.503000020980835,  lr: 0.009297599655172415
step: 3040, train_loss: 1.1210280656814575, acc: 0.5299999713897705, val_loss: 1.1814182996749878, val_acc: 0.49900001287460327,  lr: 0.009297255172413792
step: 3041, train_loss: 1.1407041549682617, acc: 0.5231999754905701, val_loss: 1.186018705368042, val_acc: 0.498199999332428,  lr: 0.009296910689655172
step: 3042, train_loss: 1.1881917715072632, acc: 0.49619999527931213, val_loss: 1.1825475692749023, val_acc: 0.49559998512268066,  lr: 0.009296566206896551
step: 3043, train_loss: 1.1209443807601929, acc: 0.526199996471405, val_loss: 1.1776782274246216, val_acc: 0.5027999877929688,  lr: 0.00929622172413793
step: 3044, train_loss: 1.1837794780731201, acc: 0.503000020980835, val_loss: 1.1799681186676025, val_acc: 0.5016000270843506,  lr: 0.00929587724137931
step: 3045, train_loss: 1.104160189628601, acc: 0.5410000085830688, val_loss: 1.176209568977356, val_acc: 0.5013999938964844,  lr: 0.00929553275862069
step: 3046, train_loss: 1.112892985343933, acc: 0.5378000140190125, val_loss: 1.1885930299758911, val_acc: 0.5037999749183655,  lr: 0.009295188275862069
step: 3047, train_loss: 1.1084911823272705, acc: 0.5370000004768372, val_loss: 1.1823269128799438, val_acc: 0.5022000074386597,  lr: 0.009294843793103448
step: 3048, train_loss: 1.108411192893982, acc: 0.5315999984741211, val_loss: 1.1883189678192139, val_acc: 0.4968000054359436,  lr: 0.009294499310344827
step: 3049, train_loss: 1.1055090427398682, acc: 0.5281999707221985, val_loss: 1.1859019994735718, val_acc: 0.506600022315979,  lr: 0.009294154827586207
step: 3050, train_loss: 1.188037633895874, acc: 0.49720001220703125, val_loss: 1.1984299421310425, val_acc: 0.49239999055862427,  lr: 0.009293810344827586
step: 3051, train_loss: 1.2008352279663086, acc: 0.4860000014305115, val_loss: 1.1918163299560547, val_acc: 0.49559998512268066,  lr: 0.009293465862068966
step: 3052, train_loss: 1.13621985912323, acc: 0.5260000228881836, val_loss: 1.1958900690078735, val_acc: 0.49540001153945923,  lr: 0.009293121379310345
step: 3053, train_loss: 1.1118959188461304, acc: 0.5440000295639038, val_loss: 1.1926679611206055, val_acc: 0.4975999891757965,  lr: 0.009292776896551724
step: 3054, train_loss: 1.1071523427963257, acc: 0.545199990272522, val_loss: 1.1896227598190308, val_acc: 0.5005999803543091,  lr: 0.009292432413793104
step: 3055, train_loss: 1.0748530626296997, acc: 0.5486000180244446, val_loss: 1.1917219161987305, val_acc: 0.5016000270843506,  lr: 0.009292087931034483
step: 3056, train_loss: 1.1803115606307983, acc: 0.5019999742507935, val_loss: 1.1868480443954468, val_acc: 0.5031999945640564,  lr: 0.009291743448275862
step: 3057, train_loss: 1.0869817733764648, acc: 0.5493999719619751, val_loss: 1.1879024505615234, val_acc: 0.5023999810218811,  lr: 0.009291398965517242
step: 3058, train_loss: 1.1252810955047607, acc: 0.5189999938011169, val_loss: 1.1936392784118652, val_acc: 0.5037999749183655,  lr: 0.009291054482758621
step: 3059, train_loss: 1.090767502784729, acc: 0.5454000234603882, val_loss: 1.1892356872558594, val_acc: 0.5041999816894531,  lr: 0.00929071
step: 3060, train_loss: 1.1585755348205566, acc: 0.508400022983551, val_loss: 1.186376690864563, val_acc: 0.5081999897956848,  lr: 0.00929036551724138
step: 3061, train_loss: 1.1472622156143188, acc: 0.5120000243186951, val_loss: 1.191731333732605, val_acc: 0.5037999749183655,  lr: 0.00929002103448276
step: 3062, train_loss: 1.2303677797317505, acc: 0.4803999960422516, val_loss: 1.1911708116531372, val_acc: 0.4991999864578247,  lr: 0.009289676551724139
step: 3063, train_loss: 1.2482666969299316, acc: 0.477400004863739, val_loss: 1.188130497932434, val_acc: 0.503000020980835,  lr: 0.009289332068965518
step: 3064, train_loss: 1.1430413722991943, acc: 0.5217999815940857, val_loss: 1.171097993850708, val_acc: 0.5094000101089478,  lr: 0.009288987586206898
step: 3065, train_loss: 1.1980782747268677, acc: 0.49900001287460327, val_loss: 1.1694954633712769, val_acc: 0.5098000168800354,  lr: 0.009288643103448277
step: 3066, train_loss: 1.1210466623306274, acc: 0.527999997138977, val_loss: 1.1736196279525757, val_acc: 0.5001999735832214,  lr: 0.009288298620689655
step: 3067, train_loss: 1.2420778274536133, acc: 0.46939998865127563, val_loss: 1.1608936786651611, val_acc: 0.5145999789237976,  lr: 0.009287954137931034
step: 3068, train_loss: 1.1128895282745361, acc: 0.527400016784668, val_loss: 1.1544677019119263, val_acc: 0.5134000182151794,  lr: 0.009287609655172413
step: 3069, train_loss: 1.082356572151184, acc: 0.5383999943733215, val_loss: 1.1480084657669067, val_acc: 0.5145999789237976,  lr: 0.009287265172413793
step: 3070, train_loss: 1.160645604133606, acc: 0.51419997215271, val_loss: 1.143128752708435, val_acc: 0.521399974822998,  lr: 0.009286920689655172
step: 3071, train_loss: 1.1807061433792114, acc: 0.5055999755859375, val_loss: 1.148574948310852, val_acc: 0.5230000019073486,  lr: 0.009286576206896551
step: 3072, train_loss: 1.1746660470962524, acc: 0.5076000094413757, val_loss: 1.1504913568496704, val_acc: 0.5217999815940857,  lr: 0.009286231724137931
step: 3073, train_loss: 1.1106594800949097, acc: 0.5275999903678894, val_loss: 1.147779941558838, val_acc: 0.5156000256538391,  lr: 0.00928588724137931
step: 3074, train_loss: 1.1827116012573242, acc: 0.5130000114440918, val_loss: 1.1503033638000488, val_acc: 0.5134000182151794,  lr: 0.00928554275862069
step: 3075, train_loss: 1.1170326471328735, acc: 0.5299999713897705, val_loss: 1.1509051322937012, val_acc: 0.5181999802589417,  lr: 0.009285198275862069
step: 3076, train_loss: 1.1569024324417114, acc: 0.5127999782562256, val_loss: 1.1477606296539307, val_acc: 0.5203999876976013,  lr: 0.009284853793103448
step: 3077, train_loss: 1.1216270923614502, acc: 0.5248000025749207, val_loss: 1.1496562957763672, val_acc: 0.5224000215530396,  lr: 0.009284509310344828
step: 3078, train_loss: 1.1417957544326782, acc: 0.5212000012397766, val_loss: 1.1489923000335693, val_acc: 0.519599974155426,  lr: 0.009284164827586207
step: 3079, train_loss: 1.1443127393722534, acc: 0.5253999829292297, val_loss: 1.1533551216125488, val_acc: 0.5199999809265137,  lr: 0.009283820344827587
step: 3080, train_loss: 1.1054009199142456, acc: 0.5320000052452087, val_loss: 1.154941201210022, val_acc: 0.5162000060081482,  lr: 0.009283475862068966
step: 3081, train_loss: 1.150273323059082, acc: 0.5095999836921692, val_loss: 1.1527247428894043, val_acc: 0.5152000188827515,  lr: 0.009283131379310345
step: 3082, train_loss: 1.223825216293335, acc: 0.48739999532699585, val_loss: 1.1547328233718872, val_acc: 0.5199999809265137,  lr: 0.009282786896551725
step: 3083, train_loss: 1.1149640083312988, acc: 0.5311999917030334, val_loss: 1.1522068977355957, val_acc: 0.5174000263214111,  lr: 0.009282442413793104
step: 3084, train_loss: 1.1071218252182007, acc: 0.5343999862670898, val_loss: 1.1577011346817017, val_acc: 0.5139999985694885,  lr: 0.009282097931034483
step: 3085, train_loss: 1.2030316591262817, acc: 0.49799999594688416, val_loss: 1.1555798053741455, val_acc: 0.5206000208854675,  lr: 0.009281753448275863
step: 3086, train_loss: 1.1870620250701904, acc: 0.5013999938964844, val_loss: 1.1559678316116333, val_acc: 0.5153999924659729,  lr: 0.009281408965517242
step: 3087, train_loss: 1.1175144910812378, acc: 0.5270000100135803, val_loss: 1.1524124145507812, val_acc: 0.5126000046730042,  lr: 0.009281064482758622
step: 3088, train_loss: 1.1860748529434204, acc: 0.5009999871253967, val_loss: 1.1420952081680298, val_acc: 0.5230000019073486,  lr: 0.00928072
step: 3089, train_loss: 1.0831019878387451, acc: 0.5473999977111816, val_loss: 1.1446905136108398, val_acc: 0.5252000093460083,  lr: 0.009280375517241379
step: 3090, train_loss: 1.1362524032592773, acc: 0.5230000019073486, val_loss: 1.144033670425415, val_acc: 0.5264000296592712,  lr: 0.009280031034482758
step: 3091, train_loss: 1.1162164211273193, acc: 0.5278000235557556, val_loss: 1.1396995782852173, val_acc: 0.520799994468689,  lr: 0.009279686551724137
step: 3092, train_loss: 1.167763590812683, acc: 0.5073999762535095, val_loss: 1.137407660484314, val_acc: 0.5220000147819519,  lr: 0.009279342068965517
step: 3093, train_loss: 1.1032259464263916, acc: 0.5357999801635742, val_loss: 1.1478573083877563, val_acc: 0.5181999802589417,  lr: 0.009278997586206896
step: 3094, train_loss: 1.2311896085739136, acc: 0.48260000348091125, val_loss: 1.1598443984985352, val_acc: 0.520799994468689,  lr: 0.009278653103448276
step: 3095, train_loss: 1.1006337404251099, acc: 0.5365999937057495, val_loss: 1.1610575914382935, val_acc: 0.5230000019073486,  lr: 0.009278308620689655
step: 3096, train_loss: 1.2392492294311523, acc: 0.46860000491142273, val_loss: 1.1614387035369873, val_acc: 0.5162000060081482,  lr: 0.009277964137931034
step: 3097, train_loss: 1.2142271995544434, acc: 0.487199991941452, val_loss: 1.1661995649337769, val_acc: 0.5113999843597412,  lr: 0.009277619655172414
step: 3098, train_loss: 1.1209319829940796, acc: 0.5302000045776367, val_loss: 1.1664525270462036, val_acc: 0.5073999762535095,  lr: 0.009277275172413793
step: 3099, train_loss: 1.1645781993865967, acc: 0.5108000040054321, val_loss: 1.1704579591751099, val_acc: 0.503000020980835,  lr: 0.009276930689655172
step: 3100, train_loss: 1.123323678970337, acc: 0.5378000140190125, val_loss: 1.17274010181427, val_acc: 0.5085999965667725,  lr: 0.009276586206896552
step: 3101, train_loss: 1.1587191820144653, acc: 0.5162000060081482, val_loss: 1.1711982488632202, val_acc: 0.5131999850273132,  lr: 0.009276241724137931
step: 3102, train_loss: 1.173251748085022, acc: 0.4970000088214874, val_loss: 1.1761666536331177, val_acc: 0.5112000107765198,  lr: 0.00927589724137931
step: 3103, train_loss: 1.1270586252212524, acc: 0.5264000296592712, val_loss: 1.1790353059768677, val_acc: 0.5041999816894531,  lr: 0.00927555275862069
step: 3104, train_loss: 1.0955880880355835, acc: 0.5383999943733215, val_loss: 1.1742279529571533, val_acc: 0.5072000026702881,  lr: 0.00927520827586207
step: 3105, train_loss: 1.156752347946167, acc: 0.5170000195503235, val_loss: 1.173584222793579, val_acc: 0.5045999884605408,  lr: 0.009274863793103449
step: 3106, train_loss: 1.0805284976959229, acc: 0.5455999970436096, val_loss: 1.1710190773010254, val_acc: 0.5072000026702881,  lr: 0.009274519310344828
step: 3107, train_loss: 1.1203206777572632, acc: 0.5278000235557556, val_loss: 1.1647236347198486, val_acc: 0.5076000094413757,  lr: 0.009274174827586208
step: 3108, train_loss: 1.061254620552063, acc: 0.5515999794006348, val_loss: 1.1622389554977417, val_acc: 0.5091999769210815,  lr: 0.009273830344827587
step: 3109, train_loss: 1.1172369718551636, acc: 0.5299999713897705, val_loss: 1.1643211841583252, val_acc: 0.5072000026702881,  lr: 0.009273485862068966
step: 3110, train_loss: 1.1293526887893677, acc: 0.5217999815940857, val_loss: 1.166161060333252, val_acc: 0.5116000175476074,  lr: 0.009273141379310346
step: 3111, train_loss: 1.1188076734542847, acc: 0.5360000133514404, val_loss: 1.1654716730117798, val_acc: 0.5130000114440918,  lr: 0.009272796896551725
step: 3112, train_loss: 1.0586689710617065, acc: 0.5569999814033508, val_loss: 1.1742911338806152, val_acc: 0.5121999979019165,  lr: 0.009272452413793104
step: 3113, train_loss: 1.0842946767807007, acc: 0.5475999712944031, val_loss: 1.1755034923553467, val_acc: 0.5001999735832214,  lr: 0.009272107931034484
step: 3114, train_loss: 1.0462335348129272, acc: 0.5576000213623047, val_loss: 1.1717370748519897, val_acc: 0.5054000020027161,  lr: 0.009271763448275861
step: 3115, train_loss: 1.1815106868743896, acc: 0.5091999769210815, val_loss: 1.1822304725646973, val_acc: 0.501800000667572,  lr: 0.00927141896551724
step: 3116, train_loss: 1.1967717409133911, acc: 0.49939998984336853, val_loss: 1.1780493259429932, val_acc: 0.5040000081062317,  lr: 0.00927107448275862
step: 3117, train_loss: 1.0972973108291626, acc: 0.5465999841690063, val_loss: 1.1816545724868774, val_acc: 0.5085999965667725,  lr: 0.00927073
step: 3118, train_loss: 1.23270583152771, acc: 0.4828000068664551, val_loss: 1.1911975145339966, val_acc: 0.5001999735832214,  lr: 0.009270385517241379
step: 3119, train_loss: 1.1675236225128174, acc: 0.5044000148773193, val_loss: 1.1907562017440796, val_acc: 0.49939998984336853,  lr: 0.009270041034482758
step: 3120, train_loss: 1.04024338722229, acc: 0.5673999786376953, val_loss: 1.1827472448349, val_acc: 0.5072000026702881,  lr: 0.009269696551724138
step: 3121, train_loss: 1.116943359375, acc: 0.5357999801635742, val_loss: 1.1781080961227417, val_acc: 0.508400022983551,  lr: 0.009269352068965517
step: 3122, train_loss: 1.1882259845733643, acc: 0.5052000284194946, val_loss: 1.1865975856781006, val_acc: 0.5005999803543091,  lr: 0.009269007586206897
step: 3123, train_loss: 1.1375681161880493, acc: 0.524399995803833, val_loss: 1.1914429664611816, val_acc: 0.5022000074386597,  lr: 0.009268663103448276
step: 3124, train_loss: 1.1221988201141357, acc: 0.5321999788284302, val_loss: 1.1827389001846313, val_acc: 0.5023999810218811,  lr: 0.009268318620689655
step: 3125, train_loss: 1.1347957849502563, acc: 0.5174000263214111, val_loss: 1.1860448122024536, val_acc: 0.5027999877929688,  lr: 0.009267974137931035
step: 3126, train_loss: 1.2313661575317383, acc: 0.4862000048160553, val_loss: 1.1946734189987183, val_acc: 0.49639999866485596,  lr: 0.009267629655172414
step: 3127, train_loss: 1.1325621604919434, acc: 0.5297999978065491, val_loss: 1.1998348236083984, val_acc: 0.4950000047683716,  lr: 0.009267285172413793
step: 3128, train_loss: 1.053952693939209, acc: 0.5608000159263611, val_loss: 1.1901447772979736, val_acc: 0.5040000081062317,  lr: 0.009266940689655173
step: 3129, train_loss: 1.067198395729065, acc: 0.5529999732971191, val_loss: 1.1876513957977295, val_acc: 0.5005999803543091,  lr: 0.009266596206896552
step: 3130, train_loss: 1.1632503271102905, acc: 0.5174000263214111, val_loss: 1.1974835395812988, val_acc: 0.4941999912261963,  lr: 0.009266251724137932
step: 3131, train_loss: 1.1373053789138794, acc: 0.526199996471405, val_loss: 1.1977699995040894, val_acc: 0.49219998717308044,  lr: 0.009265907241379311
step: 3132, train_loss: 1.115025520324707, acc: 0.5306000113487244, val_loss: 1.1933599710464478, val_acc: 0.490200012922287,  lr: 0.00926556275862069
step: 3133, train_loss: 1.112748622894287, acc: 0.5396000146865845, val_loss: 1.1969118118286133, val_acc: 0.49720001220703125,  lr: 0.00926521827586207
step: 3134, train_loss: 1.2388705015182495, acc: 0.47859999537467957, val_loss: 1.1992511749267578, val_acc: 0.5,  lr: 0.009264873793103449
step: 3135, train_loss: 1.1672261953353882, acc: 0.5022000074386597, val_loss: 1.1977266073226929, val_acc: 0.5016000270843506,  lr: 0.009264529310344828
step: 3136, train_loss: 1.2722872495651245, acc: 0.46799999475479126, val_loss: 1.1904826164245605, val_acc: 0.5031999945640564,  lr: 0.009264184827586208
step: 3137, train_loss: 1.1127769947052002, acc: 0.5332000255584717, val_loss: 1.1943708658218384, val_acc: 0.5027999877929688,  lr: 0.009263840344827586
step: 3138, train_loss: 1.2462738752365112, acc: 0.4779999852180481, val_loss: 1.177308201789856, val_acc: 0.5072000026702881,  lr: 0.009263495862068965
step: 3139, train_loss: 1.1901516914367676, acc: 0.4936000108718872, val_loss: 1.1717331409454346, val_acc: 0.5045999884605408,  lr: 0.009263151379310344
step: 3140, train_loss: 1.1962169408798218, acc: 0.4941999912261963, val_loss: 1.1624646186828613, val_acc: 0.5081999897956848,  lr: 0.009262806896551724
step: 3141, train_loss: 1.1887235641479492, acc: 0.5019999742507935, val_loss: 1.16669762134552, val_acc: 0.5152000188827515,  lr: 0.009262462413793103
step: 3142, train_loss: 1.1856615543365479, acc: 0.5073999762535095, val_loss: 1.1685987710952759, val_acc: 0.5192000269889832,  lr: 0.009262117931034482
step: 3143, train_loss: 1.12363862991333, acc: 0.5339999794960022, val_loss: 1.1625677347183228, val_acc: 0.5231999754905701,  lr: 0.009261773448275862
step: 3144, train_loss: 1.0913752317428589, acc: 0.5418000221252441, val_loss: 1.164839267730713, val_acc: 0.5145999789237976,  lr: 0.009261428965517241
step: 3145, train_loss: 1.2388694286346436, acc: 0.47440001368522644, val_loss: 1.1602046489715576, val_acc: 0.5121999979019165,  lr: 0.00926108448275862
step: 3146, train_loss: 1.22185480594635, acc: 0.4878000020980835, val_loss: 1.1611217260360718, val_acc: 0.5130000114440918,  lr: 0.00926074
step: 3147, train_loss: 1.1995441913604736, acc: 0.49239999055862427, val_loss: 1.1669877767562866, val_acc: 0.5080000162124634,  lr: 0.00926039551724138
step: 3148, train_loss: 1.144108533859253, acc: 0.5171999931335449, val_loss: 1.1636091470718384, val_acc: 0.5130000114440918,  lr: 0.009260051034482759
step: 3149, train_loss: 1.2056974172592163, acc: 0.49300000071525574, val_loss: 1.158737063407898, val_acc: 0.515999972820282,  lr: 0.009259706551724138
step: 3150, train_loss: 1.137208342552185, acc: 0.5192000269889832, val_loss: 1.1552979946136475, val_acc: 0.5252000093460083,  lr: 0.009259362068965517
step: 3151, train_loss: 1.16048002243042, acc: 0.5072000026702881, val_loss: 1.1579220294952393, val_acc: 0.5224000215530396,  lr: 0.009259017586206897
step: 3152, train_loss: 1.14638090133667, acc: 0.5152000188827515, val_loss: 1.1578322649002075, val_acc: 0.5231999754905701,  lr: 0.009258673103448276
step: 3153, train_loss: 1.1133537292480469, acc: 0.5307999849319458, val_loss: 1.159361481666565, val_acc: 0.5131999850273132,  lr: 0.009258328620689656
step: 3154, train_loss: 1.1186689138412476, acc: 0.5285999774932861, val_loss: 1.1664965152740479, val_acc: 0.5103999972343445,  lr: 0.009257984137931035
step: 3155, train_loss: 1.149728775024414, acc: 0.5157999992370605, val_loss: 1.1734880208969116, val_acc: 0.5062000155448914,  lr: 0.009257639655172414
step: 3156, train_loss: 1.1497448682785034, acc: 0.5127999782562256, val_loss: 1.1759554147720337, val_acc: 0.5059999823570251,  lr: 0.009257295172413794
step: 3157, train_loss: 1.1336190700531006, acc: 0.5239999890327454, val_loss: 1.1707359552383423, val_acc: 0.5126000046730042,  lr: 0.009256950689655173
step: 3158, train_loss: 1.1052143573760986, acc: 0.5297999978065491, val_loss: 1.1694657802581787, val_acc: 0.510200023651123,  lr: 0.009256606206896553
step: 3159, train_loss: 1.0794129371643066, acc: 0.5473999977111816, val_loss: 1.1656079292297363, val_acc: 0.51419997215271,  lr: 0.009256261724137932
step: 3160, train_loss: 1.1432198286056519, acc: 0.515999972820282, val_loss: 1.1707662343978882, val_acc: 0.5116000175476074,  lr: 0.009255917241379311
step: 3161, train_loss: 1.108579158782959, acc: 0.5335999727249146, val_loss: 1.1612626314163208, val_acc: 0.5174000263214111,  lr: 0.00925557275862069
step: 3162, train_loss: 1.1072860956192017, acc: 0.5264000296592712, val_loss: 1.1647902727127075, val_acc: 0.5134000182151794,  lr: 0.009255228275862068
step: 3163, train_loss: 1.1311721801757812, acc: 0.5239999890327454, val_loss: 1.1757694482803345, val_acc: 0.5073999762535095,  lr: 0.009254883793103448
step: 3164, train_loss: 1.0918835401535034, acc: 0.5418000221252441, val_loss: 1.1743080615997314, val_acc: 0.5045999884605408,  lr: 0.009254539310344827
step: 3165, train_loss: 1.2330001592636108, acc: 0.47440001368522644, val_loss: 1.162956953048706, val_acc: 0.5070000290870667,  lr: 0.009254194827586206
step: 3166, train_loss: 1.1296595335006714, acc: 0.5350000262260437, val_loss: 1.1738225221633911, val_acc: 0.5072000026702881,  lr: 0.009253850344827586
step: 3167, train_loss: 1.2351510524749756, acc: 0.4869999885559082, val_loss: 1.1782218217849731, val_acc: 0.504800021648407,  lr: 0.009253505862068965
step: 3168, train_loss: 1.1112436056137085, acc: 0.531000018119812, val_loss: 1.175291895866394, val_acc: 0.5070000290870667,  lr: 0.009253161379310345
step: 3169, train_loss: 1.168257236480713, acc: 0.503600001335144, val_loss: 1.1705031394958496, val_acc: 0.5094000101089478,  lr: 0.009252816896551724
step: 3170, train_loss: 1.091289758682251, acc: 0.5411999821662903, val_loss: 1.1734799146652222, val_acc: 0.5023999810218811,  lr: 0.009252472413793103
step: 3171, train_loss: 1.155300259590149, acc: 0.5055999755859375, val_loss: 1.1787835359573364, val_acc: 0.5049999952316284,  lr: 0.009252127931034483
step: 3172, train_loss: 1.1095303297042847, acc: 0.5351999998092651, val_loss: 1.169161081314087, val_acc: 0.5085999965667725,  lr: 0.009251783448275862
step: 3173, train_loss: 1.1771824359893799, acc: 0.5, val_loss: 1.1625266075134277, val_acc: 0.501800000667572,  lr: 0.009251438965517242
step: 3174, train_loss: 1.1879459619522095, acc: 0.4997999966144562, val_loss: 1.1555780172348022, val_acc: 0.5081999897956848,  lr: 0.009251094482758621
step: 3175, train_loss: 1.1441587209701538, acc: 0.5088000297546387, val_loss: 1.1515560150146484, val_acc: 0.5126000046730042,  lr: 0.00925075
step: 3176, train_loss: 1.1806243658065796, acc: 0.5040000081062317, val_loss: 1.1500240564346313, val_acc: 0.5180000066757202,  lr: 0.00925040551724138
step: 3177, train_loss: 1.1242552995681763, acc: 0.5317999720573425, val_loss: 1.147492527961731, val_acc: 0.515999972820282,  lr: 0.009250061034482759
step: 3178, train_loss: 1.1381773948669434, acc: 0.522599995136261, val_loss: 1.1506989002227783, val_acc: 0.5220000147819519,  lr: 0.009249716551724138
step: 3179, train_loss: 1.1099300384521484, acc: 0.5383999943733215, val_loss: 1.1505509614944458, val_acc: 0.5228000283241272,  lr: 0.009249372068965518
step: 3180, train_loss: 1.162048578262329, acc: 0.5156000256538391, val_loss: 1.1544876098632812, val_acc: 0.5202000141143799,  lr: 0.009249027586206897
step: 3181, train_loss: 1.145208477973938, acc: 0.5181999802589417, val_loss: 1.159087896347046, val_acc: 0.5108000040054321,  lr: 0.009248683103448277
step: 3182, train_loss: 1.140669584274292, acc: 0.524399995803833, val_loss: 1.163738489151001, val_acc: 0.5073999762535095,  lr: 0.009248338620689656
step: 3183, train_loss: 1.091322660446167, acc: 0.5425999760627747, val_loss: 1.160493016242981, val_acc: 0.5076000094413757,  lr: 0.009247994137931035
step: 3184, train_loss: 1.0814321041107178, acc: 0.5460000038146973, val_loss: 1.159708857536316, val_acc: 0.5152000188827515,  lr: 0.009247649655172415
step: 3185, train_loss: 1.1689921617507935, acc: 0.5034000277519226, val_loss: 1.1652309894561768, val_acc: 0.5123999714851379,  lr: 0.009247305172413792
step: 3186, train_loss: 1.087617039680481, acc: 0.5437999963760376, val_loss: 1.1603163480758667, val_acc: 0.5145999789237976,  lr: 0.009246960689655172
step: 3187, train_loss: 1.0887643098831177, acc: 0.5415999889373779, val_loss: 1.169329047203064, val_acc: 0.503000020980835,  lr: 0.009246616206896551
step: 3188, train_loss: 1.0690735578536987, acc: 0.5522000193595886, val_loss: 1.1608613729476929, val_acc: 0.5052000284194946,  lr: 0.00924627172413793
step: 3189, train_loss: 1.1775966882705688, acc: 0.5034000277519226, val_loss: 1.1612818241119385, val_acc: 0.5080000162124634,  lr: 0.00924592724137931
step: 3190, train_loss: 1.1288384199142456, acc: 0.5307999849319458, val_loss: 1.1643368005752563, val_acc: 0.5144000053405762,  lr: 0.00924558275862069
step: 3191, train_loss: 1.232438564300537, acc: 0.4758000075817108, val_loss: 1.161224603652954, val_acc: 0.5113999843597412,  lr: 0.009245238275862069
step: 3192, train_loss: 1.127598762512207, acc: 0.5212000012397766, val_loss: 1.1483734846115112, val_acc: 0.5198000073432922,  lr: 0.009244893793103448
step: 3193, train_loss: 1.1094238758087158, acc: 0.531000018119812, val_loss: 1.1491879224777222, val_acc: 0.5167999863624573,  lr: 0.009244549310344827
step: 3194, train_loss: 1.1174427270889282, acc: 0.5324000120162964, val_loss: 1.148301124572754, val_acc: 0.5153999924659729,  lr: 0.009244204827586207
step: 3195, train_loss: 1.1701600551605225, acc: 0.5144000053405762, val_loss: 1.143264889717102, val_acc: 0.5203999876976013,  lr: 0.009243860344827586
step: 3196, train_loss: 1.1898577213287354, acc: 0.4909999966621399, val_loss: 1.1566046476364136, val_acc: 0.5120000243186951,  lr: 0.009243515862068966
step: 3197, train_loss: 1.1746456623077393, acc: 0.5054000020027161, val_loss: 1.15264093875885, val_acc: 0.515999972820282,  lr: 0.009243171379310345
step: 3198, train_loss: 1.160295009613037, acc: 0.5153999924659729, val_loss: 1.1558024883270264, val_acc: 0.5080000162124634,  lr: 0.009242826896551724
step: 3199, train_loss: 1.1503074169158936, acc: 0.5152000188827515, val_loss: 1.1621047258377075, val_acc: 0.5045999884605408,  lr: 0.009242482413793104
step: 3200, train_loss: 1.08033287525177, acc: 0.5515999794006348, val_loss: 1.165010929107666, val_acc: 0.5040000081062317,  lr: 0.009242137931034483
step: 3201, train_loss: 1.140920639038086, acc: 0.5202000141143799, val_loss: 1.158357858657837, val_acc: 0.5109999775886536,  lr: 0.009241793448275862
step: 3202, train_loss: 1.1300245523452759, acc: 0.5315999984741211, val_loss: 1.160871148109436, val_acc: 0.503600001335144,  lr: 0.009241448965517242
step: 3203, train_loss: 1.1248527765274048, acc: 0.5293999910354614, val_loss: 1.1671440601348877, val_acc: 0.5059999823570251,  lr: 0.009241104482758621
step: 3204, train_loss: 1.1353663206100464, acc: 0.5221999883651733, val_loss: 1.166922688484192, val_acc: 0.5076000094413757,  lr: 0.00924076
step: 3205, train_loss: 1.1553329229354858, acc: 0.5145999789237976, val_loss: 1.1690738201141357, val_acc: 0.5044000148773193,  lr: 0.00924041551724138
step: 3206, train_loss: 1.1243436336517334, acc: 0.534600019454956, val_loss: 1.166117548942566, val_acc: 0.506600022315979,  lr: 0.00924007103448276
step: 3207, train_loss: 1.1157047748565674, acc: 0.5260000228881836, val_loss: 1.1702526807785034, val_acc: 0.5103999972343445,  lr: 0.009239726551724139
step: 3208, train_loss: 1.1889095306396484, acc: 0.4986000061035156, val_loss: 1.183138370513916, val_acc: 0.510200023651123,  lr: 0.009239382068965518
step: 3209, train_loss: 1.0958173274993896, acc: 0.551800012588501, val_loss: 1.1809800863265991, val_acc: 0.5054000020027161,  lr: 0.009239037586206898
step: 3210, train_loss: 1.1953359842300415, acc: 0.49300000071525574, val_loss: 1.1770365238189697, val_acc: 0.506600022315979,  lr: 0.009238693103448277
step: 3211, train_loss: 1.1207609176635742, acc: 0.5335999727249146, val_loss: 1.1757549047470093, val_acc: 0.5080000162124634,  lr: 0.009238348620689655
step: 3212, train_loss: 1.1692513227462769, acc: 0.5031999945640564, val_loss: 1.1865136623382568, val_acc: 0.504800021648407,  lr: 0.009238004137931034
step: 3213, train_loss: 1.1000964641571045, acc: 0.5339999794960022, val_loss: 1.1807403564453125, val_acc: 0.5126000046730042,  lr: 0.009237659655172413
step: 3214, train_loss: 1.1391119956970215, acc: 0.5188000202178955, val_loss: 1.195765733718872, val_acc: 0.48840001225471497,  lr: 0.009237315172413793
step: 3215, train_loss: 1.0860472917556763, acc: 0.5432000160217285, val_loss: 1.1808412075042725, val_acc: 0.5054000020027161,  lr: 0.009236970689655172
step: 3216, train_loss: 1.137770414352417, acc: 0.519599974155426, val_loss: 1.1878856420516968, val_acc: 0.4984000027179718,  lr: 0.009236626206896551
step: 3217, train_loss: 1.2498019933700562, acc: 0.4803999960422516, val_loss: 1.1826955080032349, val_acc: 0.5013999938964844,  lr: 0.009236281724137931
step: 3218, train_loss: 1.137796401977539, acc: 0.524399995803833, val_loss: 1.175843596458435, val_acc: 0.5076000094413757,  lr: 0.00923593724137931
step: 3219, train_loss: 1.1081215143203735, acc: 0.5404000282287598, val_loss: 1.1859512329101562, val_acc: 0.48820000886917114,  lr: 0.00923559275862069
step: 3220, train_loss: 1.1220178604125977, acc: 0.5242000222206116, val_loss: 1.183289885520935, val_acc: 0.48820000886917114,  lr: 0.009235248275862069
step: 3221, train_loss: 1.0951391458511353, acc: 0.5401999950408936, val_loss: 1.1765776872634888, val_acc: 0.5031999945640564,  lr: 0.009234903793103448
step: 3222, train_loss: 1.2117873430252075, acc: 0.48840001225471497, val_loss: 1.1677148342132568, val_acc: 0.5121999979019165,  lr: 0.009234559310344828
step: 3223, train_loss: 1.2277398109436035, acc: 0.48420000076293945, val_loss: 1.1688812971115112, val_acc: 0.5062000155448914,  lr: 0.009234214827586207
step: 3224, train_loss: 1.2062345743179321, acc: 0.48500001430511475, val_loss: 1.1694635152816772, val_acc: 0.5027999877929688,  lr: 0.009233870344827587
step: 3225, train_loss: 1.093170166015625, acc: 0.5401999950408936, val_loss: 1.1690720319747925, val_acc: 0.5085999965667725,  lr: 0.009233525862068966
step: 3226, train_loss: 1.1214624643325806, acc: 0.5271999835968018, val_loss: 1.1716994047164917, val_acc: 0.5019999742507935,  lr: 0.009233181379310345
step: 3227, train_loss: 1.1350980997085571, acc: 0.5123999714851379, val_loss: 1.1618692874908447, val_acc: 0.5063999891281128,  lr: 0.009232836896551725
step: 3228, train_loss: 1.1490470170974731, acc: 0.5220000147819519, val_loss: 1.1678142547607422, val_acc: 0.5016000270843506,  lr: 0.009232492413793104
step: 3229, train_loss: 1.2391990423202515, acc: 0.4697999954223633, val_loss: 1.1755121946334839, val_acc: 0.5034000277519226,  lr: 0.009232147931034483
step: 3230, train_loss: 1.1139259338378906, acc: 0.5320000052452087, val_loss: 1.1604335308074951, val_acc: 0.5134000182151794,  lr: 0.009231803448275863
step: 3231, train_loss: 1.2452257871627808, acc: 0.4742000102996826, val_loss: 1.1624914407730103, val_acc: 0.5139999985694885,  lr: 0.009231458965517242
step: 3232, train_loss: 1.1212236881256104, acc: 0.5281999707221985, val_loss: 1.1698724031448364, val_acc: 0.5116000175476074,  lr: 0.009231114482758622
step: 3233, train_loss: 1.1231913566589355, acc: 0.5351999998092651, val_loss: 1.1736699342727661, val_acc: 0.5085999965667725,  lr: 0.00923077
step: 3234, train_loss: 1.189581036567688, acc: 0.49540001153945923, val_loss: 1.1813665628433228, val_acc: 0.5040000081062317,  lr: 0.009230425517241379
step: 3235, train_loss: 1.1284191608428955, acc: 0.5249999761581421, val_loss: 1.1787670850753784, val_acc: 0.5045999884605408,  lr: 0.009230081034482758
step: 3236, train_loss: 1.2098020315170288, acc: 0.4819999933242798, val_loss: 1.1859941482543945, val_acc: 0.5067999958992004,  lr: 0.009229736551724137
step: 3237, train_loss: 1.2182049751281738, acc: 0.4893999993801117, val_loss: 1.2010835409164429, val_acc: 0.4959999918937683,  lr: 0.009229392068965517
step: 3238, train_loss: 1.1310646533966064, acc: 0.5257999897003174, val_loss: 1.1830393075942993, val_acc: 0.5031999945640564,  lr: 0.009229047586206896
step: 3239, train_loss: 1.1106367111206055, acc: 0.5306000113487244, val_loss: 1.1895365715026855, val_acc: 0.49880000948905945,  lr: 0.009228703103448276
step: 3240, train_loss: 1.2250101566314697, acc: 0.4830000102519989, val_loss: 1.1924397945404053, val_acc: 0.49059998989105225,  lr: 0.009228358620689655
step: 3241, train_loss: 1.126084804534912, acc: 0.5320000052452087, val_loss: 1.1865909099578857, val_acc: 0.4991999864578247,  lr: 0.009228014137931034
step: 3242, train_loss: 1.1342358589172363, acc: 0.5184000134468079, val_loss: 1.1886881589889526, val_acc: 0.4952000081539154,  lr: 0.009227669655172414
step: 3243, train_loss: 1.1618683338165283, acc: 0.517799973487854, val_loss: 1.1833280324935913, val_acc: 0.5026000142097473,  lr: 0.009227325172413793
step: 3244, train_loss: 1.1122843027114868, acc: 0.532800018787384, val_loss: 1.1861990690231323, val_acc: 0.49880000948905945,  lr: 0.009226980689655172
step: 3245, train_loss: 1.166913390159607, acc: 0.5123999714851379, val_loss: 1.1984307765960693, val_acc: 0.49799999594688416,  lr: 0.009226636206896552
step: 3246, train_loss: 1.1501418352127075, acc: 0.522599995136261, val_loss: 1.1991726160049438, val_acc: 0.5019999742507935,  lr: 0.009226291724137931
step: 3247, train_loss: 1.1174441576004028, acc: 0.527999997138977, val_loss: 1.1924662590026855, val_acc: 0.5012000203132629,  lr: 0.00922594724137931
step: 3248, train_loss: 1.1964963674545288, acc: 0.5045999884605408, val_loss: 1.190409541130066, val_acc: 0.49559998512268066,  lr: 0.00922560275862069
step: 3249, train_loss: 1.2167292833328247, acc: 0.4844000041484833, val_loss: 1.19334077835083, val_acc: 0.49540001153945923,  lr: 0.00922525827586207
step: 3250, train_loss: 1.150175929069519, acc: 0.517799973487854, val_loss: 1.2012429237365723, val_acc: 0.4941999912261963,  lr: 0.009224913793103449
step: 3251, train_loss: 1.086877465248108, acc: 0.5473999977111816, val_loss: 1.1897882223129272, val_acc: 0.4984000027179718,  lr: 0.009224569310344828
step: 3252, train_loss: 1.1579740047454834, acc: 0.51419997215271, val_loss: 1.1881991624832153, val_acc: 0.49779999256134033,  lr: 0.009224224827586208
step: 3253, train_loss: 1.1243300437927246, acc: 0.531000018119812, val_loss: 1.1978012323379517, val_acc: 0.4968000054359436,  lr: 0.009223880344827587
step: 3254, train_loss: 1.1437216997146606, acc: 0.5174000263214111, val_loss: 1.1967387199401855, val_acc: 0.49239999055862427,  lr: 0.009223535862068966
step: 3255, train_loss: 1.103331446647644, acc: 0.5335999727249146, val_loss: 1.1867101192474365, val_acc: 0.4975999891757965,  lr: 0.009223191379310346
step: 3256, train_loss: 1.1317001581192017, acc: 0.5230000019073486, val_loss: 1.1895312070846558, val_acc: 0.49720001220703125,  lr: 0.009222846896551725
step: 3257, train_loss: 1.0955309867858887, acc: 0.5388000011444092, val_loss: 1.1888763904571533, val_acc: 0.4986000061035156,  lr: 0.009222502413793104
step: 3258, train_loss: 1.1890934705734253, acc: 0.5, val_loss: 1.186033844947815, val_acc: 0.49619999527931213,  lr: 0.009222157931034484
step: 3259, train_loss: 1.2063325643539429, acc: 0.49320000410079956, val_loss: 1.1846786737442017, val_acc: 0.49639999866485596,  lr: 0.009221813448275861
step: 3260, train_loss: 1.1305432319641113, acc: 0.52920001745224, val_loss: 1.1763664484024048, val_acc: 0.5013999938964844,  lr: 0.00922146896551724
step: 3261, train_loss: 1.1797128915786743, acc: 0.4959999918937683, val_loss: 1.1729642152786255, val_acc: 0.5059999823570251,  lr: 0.00922112448275862
step: 3262, train_loss: 1.0715453624725342, acc: 0.546999990940094, val_loss: 1.1685850620269775, val_acc: 0.5070000290870667,  lr: 0.00922078
step: 3263, train_loss: 1.1399648189544678, acc: 0.5113999843597412, val_loss: 1.1622209548950195, val_acc: 0.508400022983551,  lr: 0.009220435517241379
step: 3264, train_loss: 1.0944278240203857, acc: 0.5357999801635742, val_loss: 1.1713835000991821, val_acc: 0.49939998984336853,  lr: 0.009220091034482758
step: 3265, train_loss: 1.1150859594345093, acc: 0.5299999713897705, val_loss: 1.164035677909851, val_acc: 0.5067999958992004,  lr: 0.009219746551724138
step: 3266, train_loss: 1.08988356590271, acc: 0.5515999794006348, val_loss: 1.165649175643921, val_acc: 0.5067999958992004,  lr: 0.009219402068965517
step: 3267, train_loss: 1.0576781034469604, acc: 0.5591999888420105, val_loss: 1.1768927574157715, val_acc: 0.5004000067710876,  lr: 0.009219057586206897
step: 3268, train_loss: 1.138041615486145, acc: 0.52920001745224, val_loss: 1.1741052865982056, val_acc: 0.4997999966144562,  lr: 0.009218713103448276
step: 3269, train_loss: 1.089137315750122, acc: 0.5404000282287598, val_loss: 1.1647398471832275, val_acc: 0.5085999965667725,  lr: 0.009218368620689655
step: 3270, train_loss: 1.1093430519104004, acc: 0.5303999781608582, val_loss: 1.170956015586853, val_acc: 0.5037999749183655,  lr: 0.009218024137931035
step: 3271, train_loss: 1.0867094993591309, acc: 0.5490000247955322, val_loss: 1.173396348953247, val_acc: 0.5081999897956848,  lr: 0.009217679655172414
step: 3272, train_loss: 1.0760353803634644, acc: 0.550000011920929, val_loss: 1.168431043624878, val_acc: 0.506600022315979,  lr: 0.009217335172413793
step: 3273, train_loss: 1.1548889875411987, acc: 0.5116000175476074, val_loss: 1.1832228899002075, val_acc: 0.4991999864578247,  lr: 0.009216990689655173
step: 3274, train_loss: 1.1729727983474731, acc: 0.5034000277519226, val_loss: 1.1806378364562988, val_acc: 0.49540001153945923,  lr: 0.009216646206896552
step: 3275, train_loss: 1.1418119668960571, acc: 0.5157999992370605, val_loss: 1.1842608451843262, val_acc: 0.5009999871253967,  lr: 0.009216301724137932
step: 3276, train_loss: 1.068554401397705, acc: 0.550599992275238, val_loss: 1.1920099258422852, val_acc: 0.4934000074863434,  lr: 0.009215957241379311
step: 3277, train_loss: 1.1749471426010132, acc: 0.5019999742507935, val_loss: 1.1868865489959717, val_acc: 0.4952000081539154,  lr: 0.00921561275862069
step: 3278, train_loss: 1.0841422080993652, acc: 0.5486000180244446, val_loss: 1.1810661554336548, val_acc: 0.4984000027179718,  lr: 0.00921526827586207
step: 3279, train_loss: 1.2486330270767212, acc: 0.4740000069141388, val_loss: 1.1778485774993896, val_acc: 0.5009999871253967,  lr: 0.009214923793103449
step: 3280, train_loss: 1.091399908065796, acc: 0.5440000295639038, val_loss: 1.1792166233062744, val_acc: 0.504800021648407,  lr: 0.009214579310344828
step: 3281, train_loss: 1.0623220205307007, acc: 0.550000011920929, val_loss: 1.17293381690979, val_acc: 0.5045999884605408,  lr: 0.009214234827586208
step: 3282, train_loss: 1.2170119285583496, acc: 0.48500001430511475, val_loss: 1.1641961336135864, val_acc: 0.5091999769210815,  lr: 0.009213890344827586
step: 3283, train_loss: 1.054962158203125, acc: 0.5515999794006348, val_loss: 1.1623417139053345, val_acc: 0.5095999836921692,  lr: 0.009213545862068965
step: 3284, train_loss: 1.1258022785186768, acc: 0.5252000093460083, val_loss: 1.1649855375289917, val_acc: 0.5098000168800354,  lr: 0.009213201379310344
step: 3285, train_loss: 1.1176211833953857, acc: 0.5246000289916992, val_loss: 1.1607935428619385, val_acc: 0.510200023651123,  lr: 0.009212856896551724
step: 3286, train_loss: 1.1025149822235107, acc: 0.5307999849319458, val_loss: 1.1587176322937012, val_acc: 0.5099999904632568,  lr: 0.009212512413793103
step: 3287, train_loss: 1.1528077125549316, acc: 0.5126000046730042, val_loss: 1.165296196937561, val_acc: 0.5113999843597412,  lr: 0.009212167931034482
step: 3288, train_loss: 1.0944831371307373, acc: 0.5404000282287598, val_loss: 1.1583195924758911, val_acc: 0.510200023651123,  lr: 0.009211823448275862
step: 3289, train_loss: 1.1316879987716675, acc: 0.5080000162124634, val_loss: 1.1601542234420776, val_acc: 0.5040000081062317,  lr: 0.009211478965517241
step: 3290, train_loss: 1.184473991394043, acc: 0.5034000277519226, val_loss: 1.1559181213378906, val_acc: 0.5139999985694885,  lr: 0.00921113448275862
step: 3291, train_loss: 1.1463977098464966, acc: 0.5044000148773193, val_loss: 1.1619129180908203, val_acc: 0.5067999958992004,  lr: 0.00921079
step: 3292, train_loss: 1.1305336952209473, acc: 0.5338000059127808, val_loss: 1.1556748151779175, val_acc: 0.5076000094413757,  lr: 0.00921044551724138
step: 3293, train_loss: 1.2236379384994507, acc: 0.4763999879360199, val_loss: 1.151398777961731, val_acc: 0.5126000046730042,  lr: 0.009210101034482759
step: 3294, train_loss: 1.2068932056427002, acc: 0.4909999966621399, val_loss: 1.1540443897247314, val_acc: 0.5116000175476074,  lr: 0.009209756551724138
step: 3295, train_loss: 1.1375800371170044, acc: 0.5210000276565552, val_loss: 1.1543231010437012, val_acc: 0.5144000053405762,  lr: 0.009209412068965517
step: 3296, train_loss: 1.2583644390106201, acc: 0.46619999408721924, val_loss: 1.1736730337142944, val_acc: 0.5055999755859375,  lr: 0.009209067586206897
step: 3297, train_loss: 1.1786123514175415, acc: 0.5004000067710876, val_loss: 1.1628130674362183, val_acc: 0.5135999917984009,  lr: 0.009208723103448276
step: 3298, train_loss: 1.2378274202346802, acc: 0.4722000062465668, val_loss: 1.1597951650619507, val_acc: 0.5152000188827515,  lr: 0.009208378620689656
step: 3299, train_loss: 1.21184504032135, acc: 0.4885999858379364, val_loss: 1.159266471862793, val_acc: 0.5145999789237976,  lr: 0.009208034137931035
step: 3300, train_loss: 1.131229043006897, acc: 0.5371999740600586, val_loss: 1.177348017692566, val_acc: 0.49939998984336853,  lr: 0.009207689655172414
step: 3301, train_loss: 1.1520774364471436, acc: 0.5185999870300293, val_loss: 1.1686804294586182, val_acc: 0.5113999843597412,  lr: 0.009207345172413794
step: 3302, train_loss: 1.1362992525100708, acc: 0.5267999768257141, val_loss: 1.1692969799041748, val_acc: 0.49900001287460327,  lr: 0.009207000689655173
step: 3303, train_loss: 1.2328404188156128, acc: 0.48019999265670776, val_loss: 1.168402075767517, val_acc: 0.5052000284194946,  lr: 0.009206656206896553
step: 3304, train_loss: 1.0994305610656738, acc: 0.5379999876022339, val_loss: 1.1684767007827759, val_acc: 0.5052000284194946,  lr: 0.009206311724137932
step: 3305, train_loss: 1.089734435081482, acc: 0.5335999727249146, val_loss: 1.165812611579895, val_acc: 0.5031999945640564,  lr: 0.009205967241379311
step: 3306, train_loss: 1.1140824556350708, acc: 0.5315999984741211, val_loss: 1.157608151435852, val_acc: 0.5103999972343445,  lr: 0.00920562275862069
step: 3307, train_loss: 1.13026762008667, acc: 0.5231999754905701, val_loss: 1.1668384075164795, val_acc: 0.5034000277519226,  lr: 0.009205278275862068
step: 3308, train_loss: 1.1254979372024536, acc: 0.5297999978065491, val_loss: 1.1710526943206787, val_acc: 0.504800021648407,  lr: 0.009204933793103448
step: 3309, train_loss: 1.1786472797393799, acc: 0.5004000067710876, val_loss: 1.1678587198257446, val_acc: 0.5077999830245972,  lr: 0.009204589310344827
step: 3310, train_loss: 1.158265471458435, acc: 0.515999972820282, val_loss: 1.177162766456604, val_acc: 0.5044000148773193,  lr: 0.009204244827586206
step: 3311, train_loss: 1.1734353303909302, acc: 0.5054000020027161, val_loss: 1.1770167350769043, val_acc: 0.4984000027179718,  lr: 0.009203900344827586
step: 3312, train_loss: 1.165234088897705, acc: 0.5188000202178955, val_loss: 1.176958441734314, val_acc: 0.5009999871253967,  lr: 0.009203555862068965
step: 3313, train_loss: 1.1730366945266724, acc: 0.504800021648407, val_loss: 1.173431396484375, val_acc: 0.5026000142097473,  lr: 0.009203211379310345
step: 3314, train_loss: 1.1523525714874268, acc: 0.5203999876976013, val_loss: 1.1769529581069946, val_acc: 0.5034000277519226,  lr: 0.009202866896551724
step: 3315, train_loss: 1.1727089881896973, acc: 0.508400022983551, val_loss: 1.175825834274292, val_acc: 0.5073999762535095,  lr: 0.009202522413793103
step: 3316, train_loss: 1.083526611328125, acc: 0.5475999712944031, val_loss: 1.1807116270065308, val_acc: 0.49939998984336853,  lr: 0.009202177931034483
step: 3317, train_loss: 1.0982693433761597, acc: 0.5397999882698059, val_loss: 1.1891741752624512, val_acc: 0.4934000074863434,  lr: 0.009201833448275862
step: 3318, train_loss: 1.095558524131775, acc: 0.5464000105857849, val_loss: 1.1804691553115845, val_acc: 0.5004000067710876,  lr: 0.009201488965517242
step: 3319, train_loss: 1.1139017343521118, acc: 0.5365999937057495, val_loss: 1.1810499429702759, val_acc: 0.49959999322891235,  lr: 0.009201144482758621
step: 3320, train_loss: 1.1209452152252197, acc: 0.5281999707221985, val_loss: 1.1871087551116943, val_acc: 0.5040000081062317,  lr: 0.0092008
step: 3321, train_loss: 1.1993545293807983, acc: 0.49000000953674316, val_loss: 1.177795171737671, val_acc: 0.5063999891281128,  lr: 0.00920045551724138
step: 3322, train_loss: 1.0866447687149048, acc: 0.550000011920929, val_loss: 1.1783438920974731, val_acc: 0.5031999945640564,  lr: 0.009200111034482759
step: 3323, train_loss: 1.1976665258407593, acc: 0.4970000088214874, val_loss: 1.1772902011871338, val_acc: 0.5055999755859375,  lr: 0.009199766551724138
step: 3324, train_loss: 1.2306870222091675, acc: 0.48899999260902405, val_loss: 1.1759300231933594, val_acc: 0.5091999769210815,  lr: 0.009199422068965518
step: 3325, train_loss: 1.2059237957000732, acc: 0.4970000088214874, val_loss: 1.1698620319366455, val_acc: 0.5080000162124634,  lr: 0.009199077586206897
step: 3326, train_loss: 1.159075379371643, acc: 0.5149999856948853, val_loss: 1.1630637645721436, val_acc: 0.5162000060081482,  lr: 0.009198733103448277
step: 3327, train_loss: 1.1162630319595337, acc: 0.5267999768257141, val_loss: 1.167337417602539, val_acc: 0.5094000101089478,  lr: 0.009198388620689656
step: 3328, train_loss: 1.0734184980392456, acc: 0.5562000274658203, val_loss: 1.1555185317993164, val_acc: 0.5181999802589417,  lr: 0.009198044137931035
step: 3329, train_loss: 1.1542866230010986, acc: 0.5109999775886536, val_loss: 1.1671299934387207, val_acc: 0.5144000053405762,  lr: 0.009197699655172415
step: 3330, train_loss: 1.132126808166504, acc: 0.5224000215530396, val_loss: 1.1624438762664795, val_acc: 0.515999972820282,  lr: 0.009197355172413792
step: 3331, train_loss: 1.0986121892929077, acc: 0.5429999828338623, val_loss: 1.1660363674163818, val_acc: 0.5117999911308289,  lr: 0.009197010689655172
step: 3332, train_loss: 1.182992696762085, acc: 0.5073999762535095, val_loss: 1.167951226234436, val_acc: 0.503600001335144,  lr: 0.009196666206896551
step: 3333, train_loss: 1.1369099617004395, acc: 0.5314000248908997, val_loss: 1.1739816665649414, val_acc: 0.4997999966144562,  lr: 0.00919632172413793
step: 3334, train_loss: 1.1919336318969727, acc: 0.5054000020027161, val_loss: 1.1750280857086182, val_acc: 0.5023999810218811,  lr: 0.00919597724137931
step: 3335, train_loss: 1.1142438650131226, acc: 0.5299999713897705, val_loss: 1.1727455854415894, val_acc: 0.5062000155448914,  lr: 0.00919563275862069
step: 3336, train_loss: 1.0768985748291016, acc: 0.54339998960495, val_loss: 1.176841378211975, val_acc: 0.501800000667572,  lr: 0.009195288275862069
step: 3337, train_loss: 1.120549201965332, acc: 0.527400016784668, val_loss: 1.1747089624404907, val_acc: 0.5004000067710876,  lr: 0.009194943793103448
step: 3338, train_loss: 1.1942273378372192, acc: 0.49160000681877136, val_loss: 1.179368495941162, val_acc: 0.49880000948905945,  lr: 0.009194599310344827
step: 3339, train_loss: 1.1513055562973022, acc: 0.5144000053405762, val_loss: 1.1781286001205444, val_acc: 0.5004000067710876,  lr: 0.009194254827586207
step: 3340, train_loss: 1.139338493347168, acc: 0.5157999992370605, val_loss: 1.1826152801513672, val_acc: 0.5012000203132629,  lr: 0.009193910344827586
step: 3341, train_loss: 1.097731590270996, acc: 0.5371999740600586, val_loss: 1.1703380346298218, val_acc: 0.504800021648407,  lr: 0.009193565862068966
step: 3342, train_loss: 1.0934224128723145, acc: 0.5407999753952026, val_loss: 1.175721526145935, val_acc: 0.5019999742507935,  lr: 0.009193221379310345
step: 3343, train_loss: 1.1565477848052979, acc: 0.5145999789237976, val_loss: 1.172644853591919, val_acc: 0.503600001335144,  lr: 0.009192876896551724
step: 3344, train_loss: 1.137712001800537, acc: 0.5289999842643738, val_loss: 1.1809450387954712, val_acc: 0.49939998984336853,  lr: 0.009192532413793104
step: 3345, train_loss: 1.118350625038147, acc: 0.5303999781608582, val_loss: 1.175087571144104, val_acc: 0.5091999769210815,  lr: 0.009192187931034483
step: 3346, train_loss: 1.1908552646636963, acc: 0.49399998784065247, val_loss: 1.183394432067871, val_acc: 0.5013999938964844,  lr: 0.009191843448275862
step: 3347, train_loss: 1.1371164321899414, acc: 0.525600016117096, val_loss: 1.1800127029418945, val_acc: 0.5022000074386597,  lr: 0.009191498965517242
step: 3348, train_loss: 1.1541699171066284, acc: 0.5072000026702881, val_loss: 1.1820557117462158, val_acc: 0.5004000067710876,  lr: 0.009191154482758621
step: 3349, train_loss: 1.0969964265823364, acc: 0.5379999876022339, val_loss: 1.1803810596466064, val_acc: 0.5,  lr: 0.00919081
step: 3350, train_loss: 1.1522748470306396, acc: 0.5185999870300293, val_loss: 1.1766639947891235, val_acc: 0.5044000148773193,  lr: 0.00919046551724138
step: 3351, train_loss: 1.093636393547058, acc: 0.5454000234603882, val_loss: 1.1771283149719238, val_acc: 0.5052000284194946,  lr: 0.00919012103448276
step: 3352, train_loss: 1.070427417755127, acc: 0.5626000165939331, val_loss: 1.1773542165756226, val_acc: 0.503000020980835,  lr: 0.009189776551724139
step: 3353, train_loss: 1.1773602962493896, acc: 0.5041999816894531, val_loss: 1.1805667877197266, val_acc: 0.5037999749183655,  lr: 0.009189432068965518
step: 3354, train_loss: 1.098524570465088, acc: 0.5356000065803528, val_loss: 1.1814384460449219, val_acc: 0.5026000142097473,  lr: 0.009189087586206898
step: 3355, train_loss: 1.0899126529693604, acc: 0.5418000221252441, val_loss: 1.1799489259719849, val_acc: 0.5016000270843506,  lr: 0.009188743103448277
step: 3356, train_loss: 1.0976696014404297, acc: 0.5407999753952026, val_loss: 1.180505394935608, val_acc: 0.4950000047683716,  lr: 0.009188398620689655
step: 3357, train_loss: 1.0685087442398071, acc: 0.5491999983787537, val_loss: 1.1878399848937988, val_acc: 0.49779999256134033,  lr: 0.009188054137931034
step: 3358, train_loss: 1.2144325971603394, acc: 0.49880000948905945, val_loss: 1.1928465366363525, val_acc: 0.5023999810218811,  lr: 0.009187709655172413
step: 3359, train_loss: 1.0901193618774414, acc: 0.5415999889373779, val_loss: 1.1869683265686035, val_acc: 0.5022000074386597,  lr: 0.009187365172413793
step: 3360, train_loss: 1.1428552865982056, acc: 0.519599974155426, val_loss: 1.18474280834198, val_acc: 0.5005999803543091,  lr: 0.009187020689655172
step: 3361, train_loss: 1.0856012105941772, acc: 0.5496000051498413, val_loss: 1.1910932064056396, val_acc: 0.5016000270843506,  lr: 0.009186676206896551
step: 3362, train_loss: 1.151285171508789, acc: 0.5109999775886536, val_loss: 1.187366008758545, val_acc: 0.49900001287460327,  lr: 0.009186331724137931
step: 3363, train_loss: 1.150427222251892, acc: 0.5085999965667725, val_loss: 1.1952693462371826, val_acc: 0.4952000081539154,  lr: 0.00918598724137931
step: 3364, train_loss: 1.1369407176971436, acc: 0.515999972820282, val_loss: 1.2089736461639404, val_acc: 0.487199991941452,  lr: 0.00918564275862069
step: 3365, train_loss: 1.121840476989746, acc: 0.5297999978065491, val_loss: 1.2152454853057861, val_acc: 0.48840001225471497,  lr: 0.009185298275862069
step: 3366, train_loss: 1.1054630279541016, acc: 0.5361999869346619, val_loss: 1.2140231132507324, val_acc: 0.4875999987125397,  lr: 0.009184953793103448
step: 3367, train_loss: 1.2219302654266357, acc: 0.48159998655319214, val_loss: 1.202276349067688, val_acc: 0.49459999799728394,  lr: 0.009184609310344828
step: 3368, train_loss: 1.0974829196929932, acc: 0.5375999808311462, val_loss: 1.1978503465652466, val_acc: 0.4970000088214874,  lr: 0.009184264827586207
step: 3369, train_loss: 1.1351490020751953, acc: 0.5299999713897705, val_loss: 1.1970232725143433, val_acc: 0.4966000020503998,  lr: 0.009183920344827587
step: 3370, train_loss: 1.2457836866378784, acc: 0.47099998593330383, val_loss: 1.1901159286499023, val_acc: 0.4952000081539154,  lr: 0.009183575862068966
step: 3371, train_loss: 1.0719704627990723, acc: 0.5504000186920166, val_loss: 1.1883445978164673, val_acc: 0.5031999945640564,  lr: 0.009183231379310345
step: 3372, train_loss: 1.1834512948989868, acc: 0.5040000081062317, val_loss: 1.190968632698059, val_acc: 0.5001999735832214,  lr: 0.009182886896551725
step: 3373, train_loss: 1.0849794149398804, acc: 0.5424000024795532, val_loss: 1.1827934980392456, val_acc: 0.5062000155448914,  lr: 0.009182542413793104
step: 3374, train_loss: 1.1634621620178223, acc: 0.5103999972343445, val_loss: 1.1815646886825562, val_acc: 0.5026000142097473,  lr: 0.009182197931034483
step: 3375, train_loss: 1.2362486124038696, acc: 0.4772000014781952, val_loss: 1.1892387866973877, val_acc: 0.501800000667572,  lr: 0.009181853448275863
step: 3376, train_loss: 1.1055368185043335, acc: 0.5302000045776367, val_loss: 1.18686044216156, val_acc: 0.4970000088214874,  lr: 0.009181508965517242
step: 3377, train_loss: 1.0814558267593384, acc: 0.5455999970436096, val_loss: 1.175737977027893, val_acc: 0.5073999762535095,  lr: 0.009181164482758622
step: 3378, train_loss: 1.0842654705047607, acc: 0.5479999780654907, val_loss: 1.1852173805236816, val_acc: 0.5045999884605408,  lr: 0.00918082
step: 3379, train_loss: 1.1740962266921997, acc: 0.498199999332428, val_loss: 1.1869078874588013, val_acc: 0.503600001335144,  lr: 0.009180475517241379
step: 3380, train_loss: 1.0955511331558228, acc: 0.5364000201225281, val_loss: 1.19605553150177, val_acc: 0.4957999885082245,  lr: 0.009180131034482758
step: 3381, train_loss: 1.1691464185714722, acc: 0.5027999877929688, val_loss: 1.1867871284484863, val_acc: 0.5013999938964844,  lr: 0.009179786551724137
step: 3382, train_loss: 1.1570329666137695, acc: 0.5156000256538391, val_loss: 1.1901880502700806, val_acc: 0.498199999332428,  lr: 0.009179442068965517
step: 3383, train_loss: 1.1756808757781982, acc: 0.5023999810218811, val_loss: 1.191685676574707, val_acc: 0.49160000681877136,  lr: 0.009179097586206896
step: 3384, train_loss: 1.143913984298706, acc: 0.519599974155426, val_loss: 1.1970114707946777, val_acc: 0.4893999993801117,  lr: 0.009178753103448276
step: 3385, train_loss: 1.2280679941177368, acc: 0.4814000129699707, val_loss: 1.1954203844070435, val_acc: 0.4966000020503998,  lr: 0.009178408620689655
step: 3386, train_loss: 1.221848487854004, acc: 0.4830000102519989, val_loss: 1.195555329322815, val_acc: 0.4885999858379364,  lr: 0.009178064137931034
step: 3387, train_loss: 1.2160825729370117, acc: 0.4830000102519989, val_loss: 1.1954922676086426, val_acc: 0.48899999260902405,  lr: 0.009177719655172414
step: 3388, train_loss: 1.131357192993164, acc: 0.5220000147819519, val_loss: 1.1977919340133667, val_acc: 0.4896000027656555,  lr: 0.009177375172413793
step: 3389, train_loss: 1.176252007484436, acc: 0.5088000297546387, val_loss: 1.1946054697036743, val_acc: 0.49059998989105225,  lr: 0.009177030689655172
step: 3390, train_loss: 1.1319750547409058, acc: 0.5293999910354614, val_loss: 1.1898188591003418, val_acc: 0.49559998512268066,  lr: 0.009176686206896552
step: 3391, train_loss: 1.096510410308838, acc: 0.5325999855995178, val_loss: 1.1867048740386963, val_acc: 0.4887999892234802,  lr: 0.009176341724137931
step: 3392, train_loss: 1.1637039184570312, acc: 0.5004000067710876, val_loss: 1.1884127855300903, val_acc: 0.48739999532699585,  lr: 0.00917599724137931
step: 3393, train_loss: 1.2643859386444092, acc: 0.46939998865127563, val_loss: 1.1851286888122559, val_acc: 0.49160000681877136,  lr: 0.00917565275862069
step: 3394, train_loss: 1.1321824789047241, acc: 0.5281999707221985, val_loss: 1.1835246086120605, val_acc: 0.490200012922287,  lr: 0.00917530827586207
step: 3395, train_loss: 1.09556245803833, acc: 0.5350000262260437, val_loss: 1.1882193088531494, val_acc: 0.49059998989105225,  lr: 0.009174963793103449
step: 3396, train_loss: 1.1239932775497437, acc: 0.5171999931335449, val_loss: 1.178861379623413, val_acc: 0.4941999912261963,  lr: 0.009174619310344828
step: 3397, train_loss: 1.2430919408798218, acc: 0.47999998927116394, val_loss: 1.1800625324249268, val_acc: 0.5081999897956848,  lr: 0.009174274827586208
step: 3398, train_loss: 1.2635303735733032, acc: 0.4596000015735626, val_loss: 1.1558716297149658, val_acc: 0.5134000182151794,  lr: 0.009173930344827587
step: 3399, train_loss: 1.2253150939941406, acc: 0.4821999967098236, val_loss: 1.1759679317474365, val_acc: 0.5063999891281128,  lr: 0.009173585862068966
step: 3400, train_loss: 1.1573386192321777, acc: 0.5139999985694885, val_loss: 1.1573100090026855, val_acc: 0.5121999979019165,  lr: 0.009173241379310346
step: 3401, train_loss: 1.1398420333862305, acc: 0.5203999876976013, val_loss: 1.158811330795288, val_acc: 0.5117999911308289,  lr: 0.009172896896551725
step: 3402, train_loss: 1.1159448623657227, acc: 0.5284000039100647, val_loss: 1.1542038917541504, val_acc: 0.5170000195503235,  lr: 0.009172552413793104
step: 3403, train_loss: 1.1885844469070435, acc: 0.4968000054359436, val_loss: 1.1573909521102905, val_acc: 0.5174000263214111,  lr: 0.009172207931034484
step: 3404, train_loss: 1.2211194038391113, acc: 0.49000000953674316, val_loss: 1.1496148109436035, val_acc: 0.5242000222206116,  lr: 0.009171863448275861
step: 3405, train_loss: 1.149143099784851, acc: 0.5170000195503235, val_loss: 1.1413785219192505, val_acc: 0.5174000263214111,  lr: 0.00917151896551724
step: 3406, train_loss: 1.1347129344940186, acc: 0.5217999815940857, val_loss: 1.1417421102523804, val_acc: 0.5206000208854675,  lr: 0.00917117448275862
step: 3407, train_loss: 1.1766268014907837, acc: 0.5076000094413757, val_loss: 1.140792965888977, val_acc: 0.5242000222206116,  lr: 0.00917083
step: 3408, train_loss: 1.1055819988250732, acc: 0.5357999801635742, val_loss: 1.1513742208480835, val_acc: 0.5135999917984009,  lr: 0.009170485517241379
step: 3409, train_loss: 1.1439024209976196, acc: 0.5296000242233276, val_loss: 1.1491780281066895, val_acc: 0.515999972820282,  lr: 0.009170141034482758
step: 3410, train_loss: 1.0835822820663452, acc: 0.5450000166893005, val_loss: 1.152475357055664, val_acc: 0.5188000202178955,  lr: 0.009169796551724138
step: 3411, train_loss: 1.1650018692016602, acc: 0.5175999999046326, val_loss: 1.1507067680358887, val_acc: 0.5224000215530396,  lr: 0.009169452068965517
step: 3412, train_loss: 1.088114857673645, acc: 0.5496000051498413, val_loss: 1.151928424835205, val_acc: 0.5174000263214111,  lr: 0.009169107586206897
step: 3413, train_loss: 1.0835601091384888, acc: 0.5472000241279602, val_loss: 1.1522183418273926, val_acc: 0.5112000107765198,  lr: 0.009168763103448276
step: 3414, train_loss: 1.098935842514038, acc: 0.525600016117096, val_loss: 1.1516045331954956, val_acc: 0.5138000249862671,  lr: 0.009168418620689655
step: 3415, train_loss: 1.185046672821045, acc: 0.4934000074863434, val_loss: 1.1514701843261719, val_acc: 0.5175999999046326,  lr: 0.009168074137931035
step: 3416, train_loss: 1.1028035879135132, acc: 0.531000018119812, val_loss: 1.1571753025054932, val_acc: 0.5149999856948853,  lr: 0.009167729655172414
step: 3417, train_loss: 1.1636810302734375, acc: 0.5055999755859375, val_loss: 1.1612296104431152, val_acc: 0.5112000107765198,  lr: 0.009167385172413793
step: 3418, train_loss: 1.1932915449142456, acc: 0.4966000020503998, val_loss: 1.1621723175048828, val_acc: 0.5112000107765198,  lr: 0.009167040689655173
step: 3419, train_loss: 1.1356122493743896, acc: 0.520799994468689, val_loss: 1.1613059043884277, val_acc: 0.5072000026702881,  lr: 0.009166696206896552
step: 3420, train_loss: 1.1496381759643555, acc: 0.5134000182151794, val_loss: 1.1669913530349731, val_acc: 0.5016000270843506,  lr: 0.009166351724137932
step: 3421, train_loss: 1.2304284572601318, acc: 0.4803999960422516, val_loss: 1.1623306274414062, val_acc: 0.5081999897956848,  lr: 0.009166007241379311
step: 3422, train_loss: 1.17341148853302, acc: 0.5012000203132629, val_loss: 1.1544532775878906, val_acc: 0.5202000141143799,  lr: 0.00916566275862069
step: 3423, train_loss: 1.0924229621887207, acc: 0.5365999937057495, val_loss: 1.149950385093689, val_acc: 0.5157999992370605,  lr: 0.00916531827586207
step: 3424, train_loss: 1.1193691492080688, acc: 0.5370000004768372, val_loss: 1.1535465717315674, val_acc: 0.5080000162124634,  lr: 0.009164973793103449
step: 3425, train_loss: 1.184416651725769, acc: 0.4986000061035156, val_loss: 1.147034764289856, val_acc: 0.5121999979019165,  lr: 0.009164629310344828
step: 3426, train_loss: 1.1179970502853394, acc: 0.5296000242233276, val_loss: 1.1490963697433472, val_acc: 0.5098000168800354,  lr: 0.009164284827586208
step: 3427, train_loss: 1.1533994674682617, acc: 0.521399974822998, val_loss: 1.1432363986968994, val_acc: 0.5156000256538391,  lr: 0.009163940344827586
step: 3428, train_loss: 1.1392284631729126, acc: 0.5220000147819519, val_loss: 1.1474632024765015, val_acc: 0.5157999992370605,  lr: 0.009163595862068965
step: 3429, train_loss: 1.0888887643814087, acc: 0.5465999841690063, val_loss: 1.149450421333313, val_acc: 0.5139999985694885,  lr: 0.009163251379310344
step: 3430, train_loss: 1.094193935394287, acc: 0.5386000275611877, val_loss: 1.1517466306686401, val_acc: 0.5156000256538391,  lr: 0.009162906896551724
step: 3431, train_loss: 1.1468054056167603, acc: 0.5203999876976013, val_loss: 1.153114914894104, val_acc: 0.5109999775886536,  lr: 0.009162562413793103
step: 3432, train_loss: 1.1491652727127075, acc: 0.5198000073432922, val_loss: 1.1551958322525024, val_acc: 0.506600022315979,  lr: 0.009162217931034482
step: 3433, train_loss: 1.1204934120178223, acc: 0.5271999835968018, val_loss: 1.1528657674789429, val_acc: 0.5067999958992004,  lr: 0.009161873448275862
step: 3434, train_loss: 1.1988681554794312, acc: 0.48559999465942383, val_loss: 1.1482373476028442, val_acc: 0.5095999836921692,  lr: 0.009161528965517241
step: 3435, train_loss: 1.1705440282821655, acc: 0.508400022983551, val_loss: 1.1471540927886963, val_acc: 0.510200023651123,  lr: 0.00916118448275862
step: 3436, train_loss: 1.0729780197143555, acc: 0.553600013256073, val_loss: 1.1484715938568115, val_acc: 0.5152000188827515,  lr: 0.00916084
step: 3437, train_loss: 1.0822540521621704, acc: 0.545199990272522, val_loss: 1.1509236097335815, val_acc: 0.5134000182151794,  lr: 0.00916049551724138
step: 3438, train_loss: 1.2172191143035889, acc: 0.4878000020980835, val_loss: 1.144234538078308, val_acc: 0.5134000182151794,  lr: 0.009160151034482759
step: 3439, train_loss: 1.0433622598648071, acc: 0.5616000294685364, val_loss: 1.1474080085754395, val_acc: 0.51419997215271,  lr: 0.009159806551724138
step: 3440, train_loss: 1.054779052734375, acc: 0.5608000159263611, val_loss: 1.1557137966156006, val_acc: 0.5139999985694885,  lr: 0.009159462068965517
step: 3441, train_loss: 1.1263855695724487, acc: 0.5266000032424927, val_loss: 1.1600998640060425, val_acc: 0.5080000162124634,  lr: 0.009159117586206897
step: 3442, train_loss: 1.1571096181869507, acc: 0.5085999965667725, val_loss: 1.1525816917419434, val_acc: 0.5117999911308289,  lr: 0.009158773103448276
step: 3443, train_loss: 1.1197896003723145, acc: 0.5267999768257141, val_loss: 1.1548579931259155, val_acc: 0.5108000040054321,  lr: 0.009158428620689656
step: 3444, train_loss: 1.1187433004379272, acc: 0.5185999870300293, val_loss: 1.1550630331039429, val_acc: 0.5166000127792358,  lr: 0.009158084137931035
step: 3445, train_loss: 1.1414592266082764, acc: 0.5234000086784363, val_loss: 1.1490439176559448, val_acc: 0.5162000060081482,  lr: 0.009157739655172414
step: 3446, train_loss: 1.1074004173278809, acc: 0.5324000120162964, val_loss: 1.1529048681259155, val_acc: 0.5134000182151794,  lr: 0.009157395172413794
step: 3447, train_loss: 1.1544054746627808, acc: 0.5144000053405762, val_loss: 1.1570658683776855, val_acc: 0.5090000033378601,  lr: 0.009157050689655173
step: 3448, train_loss: 1.236617922782898, acc: 0.4787999987602234, val_loss: 1.1644432544708252, val_acc: 0.5027999877929688,  lr: 0.009156706206896553
step: 3449, train_loss: 1.1004247665405273, acc: 0.5350000262260437, val_loss: 1.165789008140564, val_acc: 0.5073999762535095,  lr: 0.009156361724137932
step: 3450, train_loss: 1.1915719509124756, acc: 0.4918000102043152, val_loss: 1.1675772666931152, val_acc: 0.5067999958992004,  lr: 0.009156017241379311
step: 3451, train_loss: 1.156065821647644, acc: 0.5131999850273132, val_loss: 1.1661944389343262, val_acc: 0.5090000033378601,  lr: 0.00915567275862069
step: 3452, train_loss: 1.0957390069961548, acc: 0.5388000011444092, val_loss: 1.16103196144104, val_acc: 0.5049999952316284,  lr: 0.009155328275862068
step: 3453, train_loss: 1.1465058326721191, acc: 0.5185999870300293, val_loss: 1.1645748615264893, val_acc: 0.5045999884605408,  lr: 0.009154983793103448
step: 3454, train_loss: 1.0868878364562988, acc: 0.5307999849319458, val_loss: 1.1611860990524292, val_acc: 0.5088000297546387,  lr: 0.009154639310344827
step: 3455, train_loss: 1.1399916410446167, acc: 0.522599995136261, val_loss: 1.163947343826294, val_acc: 0.5085999965667725,  lr: 0.009154294827586206
step: 3456, train_loss: 1.1640299558639526, acc: 0.506600022315979, val_loss: 1.1614881753921509, val_acc: 0.506600022315979,  lr: 0.009153950344827586
step: 3457, train_loss: 1.0983141660690308, acc: 0.5365999937057495, val_loss: 1.1712931394577026, val_acc: 0.5009999871253967,  lr: 0.009153605862068965
step: 3458, train_loss: 1.1043881177902222, acc: 0.5342000126838684, val_loss: 1.1636548042297363, val_acc: 0.501800000667572,  lr: 0.009153261379310345
step: 3459, train_loss: 1.0871639251708984, acc: 0.5529999732971191, val_loss: 1.1617231369018555, val_acc: 0.5040000081062317,  lr: 0.009152916896551724
step: 3460, train_loss: 1.0770176649093628, acc: 0.5546000003814697, val_loss: 1.1628241539001465, val_acc: 0.5049999952316284,  lr: 0.009152572413793103
step: 3461, train_loss: 1.114856481552124, acc: 0.5306000113487244, val_loss: 1.1684167385101318, val_acc: 0.5041999816894531,  lr: 0.009152227931034483
step: 3462, train_loss: 1.2383273839950562, acc: 0.47699999809265137, val_loss: 1.1626341342926025, val_acc: 0.5105999708175659,  lr: 0.009151883448275862
step: 3463, train_loss: 1.0770772695541382, acc: 0.5493999719619751, val_loss: 1.1569286584854126, val_acc: 0.510200023651123,  lr: 0.009151538965517242
step: 3464, train_loss: 1.1382988691329956, acc: 0.5289999842643738, val_loss: 1.1510823965072632, val_acc: 0.5163999795913696,  lr: 0.009151194482758621
step: 3465, train_loss: 1.168196201324463, acc: 0.5135999917984009, val_loss: 1.1443614959716797, val_acc: 0.5184000134468079,  lr: 0.00915085
step: 3466, train_loss: 1.143898367881775, acc: 0.5284000039100647, val_loss: 1.1467217206954956, val_acc: 0.5126000046730042,  lr: 0.00915050551724138
step: 3467, train_loss: 1.1201250553131104, acc: 0.5303999781608582, val_loss: 1.1472173929214478, val_acc: 0.5138000249862671,  lr: 0.009150161034482759
step: 3468, train_loss: 1.10697603225708, acc: 0.5392000079154968, val_loss: 1.1391242742538452, val_acc: 0.5166000127792358,  lr: 0.009149816551724138
step: 3469, train_loss: 1.0451560020446777, acc: 0.5703999996185303, val_loss: 1.138412356376648, val_acc: 0.5228000283241272,  lr: 0.009149472068965518
step: 3470, train_loss: 1.1658051013946533, acc: 0.5121999979019165, val_loss: 1.1385473012924194, val_acc: 0.520799994468689,  lr: 0.009149127586206897
step: 3471, train_loss: 1.1753243207931519, acc: 0.5112000107765198, val_loss: 1.1406891345977783, val_acc: 0.5162000060081482,  lr: 0.009148783103448277
step: 3472, train_loss: 1.1087894439697266, acc: 0.5299999713897705, val_loss: 1.144285798072815, val_acc: 0.5235999822616577,  lr: 0.009148438620689656
step: 3473, train_loss: 1.130021333694458, acc: 0.517799973487854, val_loss: 1.1457663774490356, val_acc: 0.520799994468689,  lr: 0.009148094137931035
step: 3474, train_loss: 1.1890921592712402, acc: 0.5063999891281128, val_loss: 1.146682858467102, val_acc: 0.517799973487854,  lr: 0.009147749655172415
step: 3475, train_loss: 1.134218692779541, acc: 0.5224000215530396, val_loss: 1.1439582109451294, val_acc: 0.5170000195503235,  lr: 0.009147405172413792
step: 3476, train_loss: 1.1251204013824463, acc: 0.525600016117096, val_loss: 1.1462669372558594, val_acc: 0.5181999802589417,  lr: 0.009147060689655172
step: 3477, train_loss: 1.0957987308502197, acc: 0.5343999862670898, val_loss: 1.1484003067016602, val_acc: 0.5157999992370605,  lr: 0.009146716206896551
step: 3478, train_loss: 1.087558388710022, acc: 0.5396000146865845, val_loss: 1.1466732025146484, val_acc: 0.515999972820282,  lr: 0.00914637172413793
step: 3479, train_loss: 1.1901148557662964, acc: 0.49619999527931213, val_loss: 1.149638056755066, val_acc: 0.5138000249862671,  lr: 0.00914602724137931
step: 3480, train_loss: 1.0961500406265259, acc: 0.5414000153541565, val_loss: 1.1514859199523926, val_acc: 0.5116000175476074,  lr: 0.00914568275862069
step: 3481, train_loss: 1.0916588306427002, acc: 0.5370000004768372, val_loss: 1.1541093587875366, val_acc: 0.5131999850273132,  lr: 0.009145338275862069
step: 3482, train_loss: 1.0668343305587769, acc: 0.545799970626831, val_loss: 1.158874273300171, val_acc: 0.5085999965667725,  lr: 0.009144993793103448
step: 3483, train_loss: 1.195647120475769, acc: 0.49939998984336853, val_loss: 1.1556059122085571, val_acc: 0.5117999911308289,  lr: 0.009144649310344827
step: 3484, train_loss: 1.1732521057128906, acc: 0.5034000277519226, val_loss: 1.152754306793213, val_acc: 0.5185999870300293,  lr: 0.009144304827586207
step: 3485, train_loss: 1.2157082557678223, acc: 0.48820000886917114, val_loss: 1.1535515785217285, val_acc: 0.5156000256538391,  lr: 0.009143960344827586
step: 3486, train_loss: 1.1250927448272705, acc: 0.5189999938011169, val_loss: 1.146933674812317, val_acc: 0.5135999917984009,  lr: 0.009143615862068966
step: 3487, train_loss: 1.0755053758621216, acc: 0.5546000003814697, val_loss: 1.147659420967102, val_acc: 0.5091999769210815,  lr: 0.009143271379310345
step: 3488, train_loss: 1.1093647480010986, acc: 0.5364000201225281, val_loss: 1.1469329595565796, val_acc: 0.5162000060081482,  lr: 0.009142926896551724
step: 3489, train_loss: 1.1746283769607544, acc: 0.5108000040054321, val_loss: 1.146530032157898, val_acc: 0.5185999870300293,  lr: 0.009142582413793104
step: 3490, train_loss: 1.256038784980774, acc: 0.475600004196167, val_loss: 1.154615879058838, val_acc: 0.5175999999046326,  lr: 0.009142237931034483
step: 3491, train_loss: 1.0870749950408936, acc: 0.5442000031471252, val_loss: 1.1513144969940186, val_acc: 0.5163999795913696,  lr: 0.009141893448275863
step: 3492, train_loss: 1.0967211723327637, acc: 0.5393999814987183, val_loss: 1.1514827013015747, val_acc: 0.5076000094413757,  lr: 0.009141548965517242
step: 3493, train_loss: 1.149414300918579, acc: 0.5171999931335449, val_loss: 1.1532176733016968, val_acc: 0.5116000175476074,  lr: 0.009141204482758621
step: 3494, train_loss: 1.1366578340530396, acc: 0.5103999972343445, val_loss: 1.143660068511963, val_acc: 0.5130000114440918,  lr: 0.00914086
step: 3495, train_loss: 1.0836676359176636, acc: 0.5468000173568726, val_loss: 1.1555243730545044, val_acc: 0.5072000026702881,  lr: 0.00914051551724138
step: 3496, train_loss: 1.2070844173431396, acc: 0.4851999878883362, val_loss: 1.1488685607910156, val_acc: 0.5138000249862671,  lr: 0.00914017103448276
step: 3497, train_loss: 1.1496026515960693, acc: 0.5235999822616577, val_loss: 1.1525516510009766, val_acc: 0.5145999789237976,  lr: 0.009139826551724139
step: 3498, train_loss: 1.128320574760437, acc: 0.527400016784668, val_loss: 1.1647562980651855, val_acc: 0.5145999789237976,  lr: 0.009139482068965518
step: 3499, train_loss: 1.1379785537719727, acc: 0.5224000215530396, val_loss: 1.1549288034439087, val_acc: 0.5162000060081482,  lr: 0.009139137586206898
step: 3500, train_loss: 1.1731964349746704, acc: 0.5127999782562256, val_loss: 1.152818202972412, val_acc: 0.5210000276565552,  lr: 0.009138793103448277
step: 3501, train_loss: 1.2067159414291382, acc: 0.4803999960422516, val_loss: 1.1621025800704956, val_acc: 0.5162000060081482,  lr: 0.009138448620689655
step: 3502, train_loss: 1.1483054161071777, acc: 0.515999972820282, val_loss: 1.161379337310791, val_acc: 0.519599974155426,  lr: 0.009138104137931034
step: 3503, train_loss: 1.0754868984222412, acc: 0.5482000112533569, val_loss: 1.1646980047225952, val_acc: 0.5105999708175659,  lr: 0.009137759655172413
step: 3504, train_loss: 1.09443998336792, acc: 0.5320000052452087, val_loss: 1.1649664640426636, val_acc: 0.5121999979019165,  lr: 0.009137415172413793
step: 3505, train_loss: 1.1566200256347656, acc: 0.5144000053405762, val_loss: 1.1663779020309448, val_acc: 0.5127999782562256,  lr: 0.009137070689655172
step: 3506, train_loss: 1.243546724319458, acc: 0.4821999967098236, val_loss: 1.165909767150879, val_acc: 0.5105999708175659,  lr: 0.009136726206896552
step: 3507, train_loss: 1.1398582458496094, acc: 0.5144000053405762, val_loss: 1.1704978942871094, val_acc: 0.5081999897956848,  lr: 0.009136381724137931
step: 3508, train_loss: 1.1262421607971191, acc: 0.5260000228881836, val_loss: 1.163089394569397, val_acc: 0.5139999985694885,  lr: 0.00913603724137931
step: 3509, train_loss: 1.1439543962478638, acc: 0.5181999802589417, val_loss: 1.1481233835220337, val_acc: 0.5152000188827515,  lr: 0.00913569275862069
step: 3510, train_loss: 1.0836589336395264, acc: 0.5440000295639038, val_loss: 1.144752860069275, val_acc: 0.5112000107765198,  lr: 0.009135348275862069
step: 3511, train_loss: 1.115290880203247, acc: 0.52920001745224, val_loss: 1.1432465314865112, val_acc: 0.5180000066757202,  lr: 0.009135003793103448
step: 3512, train_loss: 1.1063523292541504, acc: 0.5375999808311462, val_loss: 1.1462914943695068, val_acc: 0.51419997215271,  lr: 0.009134659310344828
step: 3513, train_loss: 1.1032347679138184, acc: 0.5333999991416931, val_loss: 1.15126633644104, val_acc: 0.5145999789237976,  lr: 0.009134314827586207
step: 3514, train_loss: 1.133773684501648, acc: 0.519599974155426, val_loss: 1.1468602418899536, val_acc: 0.5184000134468079,  lr: 0.009133970344827587
step: 3515, train_loss: 1.061926245689392, acc: 0.5482000112533569, val_loss: 1.1519335508346558, val_acc: 0.5157999992370605,  lr: 0.009133625862068966
step: 3516, train_loss: 1.1061301231384277, acc: 0.5404000282287598, val_loss: 1.1504048109054565, val_acc: 0.5126000046730042,  lr: 0.009133281379310345
step: 3517, train_loss: 1.1302986145019531, acc: 0.5288000106811523, val_loss: 1.1511738300323486, val_acc: 0.5153999924659729,  lr: 0.009132936896551725
step: 3518, train_loss: 1.185819387435913, acc: 0.503000020980835, val_loss: 1.1514471769332886, val_acc: 0.5202000141143799,  lr: 0.009132592413793104
step: 3519, train_loss: 1.1251369714736938, acc: 0.5271999835968018, val_loss: 1.1544896364212036, val_acc: 0.5198000073432922,  lr: 0.009132247931034483
step: 3520, train_loss: 1.1990710496902466, acc: 0.48899999260902405, val_loss: 1.1511811017990112, val_acc: 0.5175999999046326,  lr: 0.009131903448275863
step: 3521, train_loss: 1.163695216178894, acc: 0.515999972820282, val_loss: 1.150964379310608, val_acc: 0.5126000046730042,  lr: 0.009131558965517242
step: 3522, train_loss: 1.1424919366836548, acc: 0.5278000235557556, val_loss: 1.151656985282898, val_acc: 0.5144000053405762,  lr: 0.009131214482758622
step: 3523, train_loss: 1.0788166522979736, acc: 0.5515999794006348, val_loss: 1.14634108543396, val_acc: 0.5175999999046326,  lr: 0.00913087
step: 3524, train_loss: 1.0857244729995728, acc: 0.5411999821662903, val_loss: 1.1500144004821777, val_acc: 0.519599974155426,  lr: 0.009130525517241379
step: 3525, train_loss: 1.160824179649353, acc: 0.5027999877929688, val_loss: 1.1472301483154297, val_acc: 0.5188000202178955,  lr: 0.009130181034482758
step: 3526, train_loss: 1.0574889183044434, acc: 0.5522000193595886, val_loss: 1.1531877517700195, val_acc: 0.5152000188827515,  lr: 0.009129836551724137
step: 3527, train_loss: 1.2135396003723145, acc: 0.49480000138282776, val_loss: 1.1430283784866333, val_acc: 0.5189999938011169,  lr: 0.009129492068965517
step: 3528, train_loss: 1.0922383069992065, acc: 0.5414000153541565, val_loss: 1.139402985572815, val_acc: 0.5184000134468079,  lr: 0.009129147586206896
step: 3529, train_loss: 1.1114532947540283, acc: 0.5315999984741211, val_loss: 1.1421208381652832, val_acc: 0.5174000263214111,  lr: 0.009128803103448276
step: 3530, train_loss: 1.2310863733291626, acc: 0.47859999537467957, val_loss: 1.1430118083953857, val_acc: 0.5153999924659729,  lr: 0.009128458620689655
step: 3531, train_loss: 1.126847743988037, acc: 0.5267999768257141, val_loss: 1.1468884944915771, val_acc: 0.5112000107765198,  lr: 0.009128114137931034
step: 3532, train_loss: 1.067348837852478, acc: 0.5540000200271606, val_loss: 1.1473894119262695, val_acc: 0.5153999924659729,  lr: 0.009127769655172414
step: 3533, train_loss: 1.0654330253601074, acc: 0.5497999787330627, val_loss: 1.1605907678604126, val_acc: 0.5063999891281128,  lr: 0.009127425172413793
step: 3534, train_loss: 1.1253907680511475, acc: 0.5267999768257141, val_loss: 1.1503173112869263, val_acc: 0.5120000243186951,  lr: 0.009127080689655172
step: 3535, train_loss: 1.1041345596313477, acc: 0.5379999876022339, val_loss: 1.1485741138458252, val_acc: 0.5184000134468079,  lr: 0.009126736206896552
step: 3536, train_loss: 1.0830620527267456, acc: 0.545799970626831, val_loss: 1.1456325054168701, val_acc: 0.5156000256538391,  lr: 0.009126391724137931
step: 3537, train_loss: 1.153335690498352, acc: 0.5185999870300293, val_loss: 1.1554681062698364, val_acc: 0.5156000256538391,  lr: 0.00912604724137931
step: 3538, train_loss: 1.1952333450317383, acc: 0.49160000681877136, val_loss: 1.157976746559143, val_acc: 0.5131999850273132,  lr: 0.00912570275862069
step: 3539, train_loss: 1.0861241817474365, acc: 0.5393999814987183, val_loss: 1.1617968082427979, val_acc: 0.5108000040054321,  lr: 0.00912535827586207
step: 3540, train_loss: 1.1889946460723877, acc: 0.4893999993801117, val_loss: 1.162448763847351, val_acc: 0.5080000162124634,  lr: 0.009125013793103449
step: 3541, train_loss: 1.0735450983047485, acc: 0.5547999739646912, val_loss: 1.1658457517623901, val_acc: 0.5016000270843506,  lr: 0.009124669310344828
step: 3542, train_loss: 1.131683349609375, acc: 0.5306000113487244, val_loss: 1.166249394416809, val_acc: 0.5037999749183655,  lr: 0.009124324827586208
step: 3543, train_loss: 1.0934462547302246, acc: 0.5396000146865845, val_loss: 1.1727700233459473, val_acc: 0.5031999945640564,  lr: 0.009123980344827587
step: 3544, train_loss: 1.0911694765090942, acc: 0.5343999862670898, val_loss: 1.1752344369888306, val_acc: 0.5080000162124634,  lr: 0.009123635862068966
step: 3545, train_loss: 1.0991110801696777, acc: 0.534600019454956, val_loss: 1.1755496263504028, val_acc: 0.501800000667572,  lr: 0.009123291379310346
step: 3546, train_loss: 1.2016246318817139, acc: 0.4885999858379364, val_loss: 1.176594614982605, val_acc: 0.4991999864578247,  lr: 0.009122946896551725
step: 3547, train_loss: 1.1526050567626953, acc: 0.5127999782562256, val_loss: 1.173034429550171, val_acc: 0.5095999836921692,  lr: 0.009122602413793104
step: 3548, train_loss: 1.097506046295166, acc: 0.5442000031471252, val_loss: 1.1699010133743286, val_acc: 0.5094000101089478,  lr: 0.009122257931034484
step: 3549, train_loss: 1.186720848083496, acc: 0.5001999735832214, val_loss: 1.192369818687439, val_acc: 0.4997999966144562,  lr: 0.009121913448275861
step: 3550, train_loss: 1.275704264640808, acc: 0.46320000290870667, val_loss: 1.1787792444229126, val_acc: 0.5081999897956848,  lr: 0.00912156896551724
step: 3551, train_loss: 1.1924684047698975, acc: 0.49219998717308044, val_loss: 1.1916614770889282, val_acc: 0.4970000088214874,  lr: 0.00912122448275862
step: 3552, train_loss: 1.2237071990966797, acc: 0.4805999994277954, val_loss: 1.1970744132995605, val_acc: 0.4875999987125397,  lr: 0.00912088
step: 3553, train_loss: 1.1568979024887085, acc: 0.5134000182151794, val_loss: 1.1878465414047241, val_acc: 0.5008000135421753,  lr: 0.009120535517241379
step: 3554, train_loss: 1.121527075767517, acc: 0.526199996471405, val_loss: 1.187328815460205, val_acc: 0.5005999803543091,  lr: 0.009120191034482758
step: 3555, train_loss: 1.1644294261932373, acc: 0.5049999952316284, val_loss: 1.184152603149414, val_acc: 0.5,  lr: 0.009119846551724138
step: 3556, train_loss: 1.1274784803390503, acc: 0.5315999984741211, val_loss: 1.1879111528396606, val_acc: 0.5027999877929688,  lr: 0.009119502068965517
step: 3557, train_loss: 1.1617627143859863, acc: 0.5163999795913696, val_loss: 1.1780446767807007, val_acc: 0.506600022315979,  lr: 0.009119157586206897
step: 3558, train_loss: 1.0837217569351196, acc: 0.5472000241279602, val_loss: 1.176110863685608, val_acc: 0.501800000667572,  lr: 0.009118813103448276
step: 3559, train_loss: 1.1047375202178955, acc: 0.5460000038146973, val_loss: 1.1754658222198486, val_acc: 0.503600001335144,  lr: 0.009118468620689655
step: 3560, train_loss: 1.2202179431915283, acc: 0.4832000136375427, val_loss: 1.186575174331665, val_acc: 0.5041999816894531,  lr: 0.009118124137931035
step: 3561, train_loss: 1.1365057229995728, acc: 0.5216000080108643, val_loss: 1.1759312152862549, val_acc: 0.5013999938964844,  lr: 0.009117779655172414
step: 3562, train_loss: 1.1575744152069092, acc: 0.5170000195503235, val_loss: 1.1727205514907837, val_acc: 0.503000020980835,  lr: 0.009117435172413793
step: 3563, train_loss: 1.1321070194244385, acc: 0.525600016117096, val_loss: 1.177626609802246, val_acc: 0.5037999749183655,  lr: 0.009117090689655173
step: 3564, train_loss: 1.1338214874267578, acc: 0.5249999761581421, val_loss: 1.1784590482711792, val_acc: 0.5049999952316284,  lr: 0.009116746206896552
step: 3565, train_loss: 1.1723085641860962, acc: 0.5199999809265137, val_loss: 1.180676817893982, val_acc: 0.49880000948905945,  lr: 0.009116401724137932
step: 3566, train_loss: 1.122253179550171, acc: 0.5228000283241272, val_loss: 1.1791260242462158, val_acc: 0.5088000297546387,  lr: 0.009116057241379311
step: 3567, train_loss: 1.1102622747421265, acc: 0.5388000011444092, val_loss: 1.1778643131256104, val_acc: 0.5054000020027161,  lr: 0.00911571275862069
step: 3568, train_loss: 1.2207003831863403, acc: 0.487199991941452, val_loss: 1.174729585647583, val_acc: 0.5031999945640564,  lr: 0.00911536827586207
step: 3569, train_loss: 1.1550238132476807, acc: 0.5127999782562256, val_loss: 1.175942301750183, val_acc: 0.5063999891281128,  lr: 0.009115023793103449
step: 3570, train_loss: 1.1041008234024048, acc: 0.5356000065803528, val_loss: 1.175153374671936, val_acc: 0.5031999945640564,  lr: 0.009114679310344828
step: 3571, train_loss: 1.164339303970337, acc: 0.5120000243186951, val_loss: 1.1737936735153198, val_acc: 0.5037999749183655,  lr: 0.009114334827586206
step: 3572, train_loss: 1.1125428676605225, acc: 0.5360000133514404, val_loss: 1.1853407621383667, val_acc: 0.4975999891757965,  lr: 0.009113990344827586
step: 3573, train_loss: 1.1349093914031982, acc: 0.527999997138977, val_loss: 1.1811363697052002, val_acc: 0.4925999939441681,  lr: 0.009113645862068965
step: 3574, train_loss: 1.2007768154144287, acc: 0.4957999885082245, val_loss: 1.1744771003723145, val_acc: 0.5,  lr: 0.009113301379310344
step: 3575, train_loss: 1.1699583530426025, acc: 0.5098000168800354, val_loss: 1.1789458990097046, val_acc: 0.5005999803543091,  lr: 0.009112956896551724
step: 3576, train_loss: 1.150521993637085, acc: 0.5108000040054321, val_loss: 1.1711573600769043, val_acc: 0.5001999735832214,  lr: 0.009112612413793103
step: 3577, train_loss: 1.1870428323745728, acc: 0.49459999799728394, val_loss: 1.176863670349121, val_acc: 0.5054000020027161,  lr: 0.009112267931034482
step: 3578, train_loss: 1.129689335823059, acc: 0.5224000215530396, val_loss: 1.1766318082809448, val_acc: 0.5080000162124634,  lr: 0.009111923448275862
step: 3579, train_loss: 1.2234821319580078, acc: 0.4869999885559082, val_loss: 1.1705576181411743, val_acc: 0.5175999999046326,  lr: 0.009111578965517241
step: 3580, train_loss: 1.14591383934021, acc: 0.5112000107765198, val_loss: 1.1661430597305298, val_acc: 0.5153999924659729,  lr: 0.00911123448275862
step: 3581, train_loss: 1.1451040506362915, acc: 0.5134000182151794, val_loss: 1.1632585525512695, val_acc: 0.5181999802589417,  lr: 0.00911089
step: 3582, train_loss: 1.1154954433441162, acc: 0.52920001745224, val_loss: 1.1670125722885132, val_acc: 0.5144000053405762,  lr: 0.00911054551724138
step: 3583, train_loss: 1.0943677425384521, acc: 0.5461999773979187, val_loss: 1.1718515157699585, val_acc: 0.506600022315979,  lr: 0.009110201034482759
step: 3584, train_loss: 1.0947939157485962, acc: 0.5511999726295471, val_loss: 1.1727994680404663, val_acc: 0.506600022315979,  lr: 0.009109856551724138
step: 3585, train_loss: 1.1132745742797852, acc: 0.5356000065803528, val_loss: 1.168044924736023, val_acc: 0.5113999843597412,  lr: 0.009109512068965517
step: 3586, train_loss: 1.0805758237838745, acc: 0.5450000166893005, val_loss: 1.1719727516174316, val_acc: 0.5052000284194946,  lr: 0.009109167586206897
step: 3587, train_loss: 1.0931687355041504, acc: 0.5397999882698059, val_loss: 1.1641913652420044, val_acc: 0.510200023651123,  lr: 0.009108823103448276
step: 3588, train_loss: 1.1115899085998535, acc: 0.5303999781608582, val_loss: 1.1656596660614014, val_acc: 0.5090000033378601,  lr: 0.009108478620689656
step: 3589, train_loss: 1.1023460626602173, acc: 0.5400000214576721, val_loss: 1.1713231801986694, val_acc: 0.5058000087738037,  lr: 0.009108134137931035
step: 3590, train_loss: 1.1509147882461548, acc: 0.5220000147819519, val_loss: 1.1708346605300903, val_acc: 0.5135999917984009,  lr: 0.009107789655172414
step: 3591, train_loss: 1.1002135276794434, acc: 0.5371999740600586, val_loss: 1.1746265888214111, val_acc: 0.5157999992370605,  lr: 0.009107445172413794
step: 3592, train_loss: 1.1047134399414062, acc: 0.5419999957084656, val_loss: 1.1698755025863647, val_acc: 0.5157999992370605,  lr: 0.009107100689655173
step: 3593, train_loss: 1.0864784717559814, acc: 0.5425999760627747, val_loss: 1.1663334369659424, val_acc: 0.5156000256538391,  lr: 0.009106756206896553
step: 3594, train_loss: 1.0405125617980957, acc: 0.5673999786376953, val_loss: 1.1715928316116333, val_acc: 0.5123999714851379,  lr: 0.009106411724137932
step: 3595, train_loss: 1.08744215965271, acc: 0.5491999983787537, val_loss: 1.1720119714736938, val_acc: 0.5076000094413757,  lr: 0.009106067241379311
step: 3596, train_loss: 1.237072229385376, acc: 0.48080000281333923, val_loss: 1.1719156503677368, val_acc: 0.5156000256538391,  lr: 0.00910572275862069
step: 3597, train_loss: 1.1584361791610718, acc: 0.508400022983551, val_loss: 1.1693967580795288, val_acc: 0.5099999904632568,  lr: 0.009105378275862068
step: 3598, train_loss: 1.1219009160995483, acc: 0.5235999822616577, val_loss: 1.1507563591003418, val_acc: 0.517799973487854,  lr: 0.009105033793103448
step: 3599, train_loss: 1.0873396396636963, acc: 0.5483999848365784, val_loss: 1.1470229625701904, val_acc: 0.5185999870300293,  lr: 0.009104689310344827
step: 3600, train_loss: 1.0777658224105835, acc: 0.5493999719619751, val_loss: 1.1659833192825317, val_acc: 0.5134000182151794,  lr: 0.009104344827586206
step: 3601, train_loss: 1.158355474472046, acc: 0.5180000066757202, val_loss: 1.1509617567062378, val_acc: 0.5174000263214111,  lr: 0.009104000344827586
step: 3602, train_loss: 1.1714093685150146, acc: 0.5117999911308289, val_loss: 1.160275936126709, val_acc: 0.5105999708175659,  lr: 0.009103655862068965
step: 3603, train_loss: 1.199326992034912, acc: 0.48080000281333923, val_loss: 1.1555994749069214, val_acc: 0.5130000114440918,  lr: 0.009103311379310345
step: 3604, train_loss: 1.0824171304702759, acc: 0.5424000024795532, val_loss: 1.1668041944503784, val_acc: 0.5073999762535095,  lr: 0.009102966896551724
step: 3605, train_loss: 1.1121245622634888, acc: 0.5302000045776367, val_loss: 1.1553946733474731, val_acc: 0.5131999850273132,  lr: 0.009102622413793103
step: 3606, train_loss: 1.080780029296875, acc: 0.5479999780654907, val_loss: 1.1526738405227661, val_acc: 0.520799994468689,  lr: 0.009102277931034483
step: 3607, train_loss: 1.1609852313995361, acc: 0.5175999999046326, val_loss: 1.1527109146118164, val_acc: 0.5216000080108643,  lr: 0.009101933448275862
step: 3608, train_loss: 1.212389349937439, acc: 0.49000000953674316, val_loss: 1.1641361713409424, val_acc: 0.5095999836921692,  lr: 0.009101588965517242
step: 3609, train_loss: 1.1334506273269653, acc: 0.527999997138977, val_loss: 1.1690499782562256, val_acc: 0.5067999958992004,  lr: 0.009101244482758621
step: 3610, train_loss: 1.1912574768066406, acc: 0.5022000074386597, val_loss: 1.1590781211853027, val_acc: 0.5062000155448914,  lr: 0.0091009
step: 3611, train_loss: 1.1697591543197632, acc: 0.5109999775886536, val_loss: 1.164227843284607, val_acc: 0.5120000243186951,  lr: 0.00910055551724138
step: 3612, train_loss: 1.0807887315750122, acc: 0.5437999963760376, val_loss: 1.1673976182937622, val_acc: 0.5098000168800354,  lr: 0.009100211034482759
step: 3613, train_loss: 1.1364058256149292, acc: 0.5228000283241272, val_loss: 1.1677889823913574, val_acc: 0.5073999762535095,  lr: 0.009099866551724138
step: 3614, train_loss: 1.1179025173187256, acc: 0.5321999788284302, val_loss: 1.168032169342041, val_acc: 0.5081999897956848,  lr: 0.009099522068965518
step: 3615, train_loss: 1.2395395040512085, acc: 0.4772000014781952, val_loss: 1.161285161972046, val_acc: 0.5162000060081482,  lr: 0.009099177586206897
step: 3616, train_loss: 1.0874426364898682, acc: 0.5378000140190125, val_loss: 1.1554532051086426, val_acc: 0.5152000188827515,  lr: 0.009098833103448277
step: 3617, train_loss: 1.1042381525039673, acc: 0.5370000004768372, val_loss: 1.1469930410385132, val_acc: 0.51419997215271,  lr: 0.009098488620689656
step: 3618, train_loss: 1.1661862134933472, acc: 0.5103999972343445, val_loss: 1.1459892988204956, val_acc: 0.520799994468689,  lr: 0.009098144137931035
step: 3619, train_loss: 1.1485577821731567, acc: 0.5139999985694885, val_loss: 1.153991937637329, val_acc: 0.5163999795913696,  lr: 0.009097799655172415
step: 3620, train_loss: 1.1381018161773682, acc: 0.5285999774932861, val_loss: 1.156328797340393, val_acc: 0.5109999775886536,  lr: 0.009097455172413792
step: 3621, train_loss: 1.0693367719650269, acc: 0.5501999855041504, val_loss: 1.165024757385254, val_acc: 0.5073999762535095,  lr: 0.009097110689655172
step: 3622, train_loss: 1.1014811992645264, acc: 0.5371999740600586, val_loss: 1.1561821699142456, val_acc: 0.5067999958992004,  lr: 0.009096766206896551
step: 3623, train_loss: 1.0641385316848755, acc: 0.5590000152587891, val_loss: 1.1555652618408203, val_acc: 0.5045999884605408,  lr: 0.00909642172413793
step: 3624, train_loss: 1.1027103662490845, acc: 0.5415999889373779, val_loss: 1.1581956148147583, val_acc: 0.5095999836921692,  lr: 0.00909607724137931
step: 3625, train_loss: 1.1419100761413574, acc: 0.5231999754905701, val_loss: 1.1576591730117798, val_acc: 0.5113999843597412,  lr: 0.00909573275862069
step: 3626, train_loss: 1.103963851928711, acc: 0.5473999977111816, val_loss: 1.166628122329712, val_acc: 0.5004000067710876,  lr: 0.009095388275862069
step: 3627, train_loss: 1.0754835605621338, acc: 0.5491999983787537, val_loss: 1.1575312614440918, val_acc: 0.5026000142097473,  lr: 0.009095043793103448
step: 3628, train_loss: 1.1587642431259155, acc: 0.5116000175476074, val_loss: 1.1608997583389282, val_acc: 0.5077999830245972,  lr: 0.009094699310344827
step: 3629, train_loss: 1.2116502523422241, acc: 0.4819999933242798, val_loss: 1.1541417837142944, val_acc: 0.5117999911308289,  lr: 0.009094354827586207
step: 3630, train_loss: 1.1949503421783447, acc: 0.4909999966621399, val_loss: 1.1601372957229614, val_acc: 0.5109999775886536,  lr: 0.009094010344827586
step: 3631, train_loss: 1.1358697414398193, acc: 0.5175999999046326, val_loss: 1.1671638488769531, val_acc: 0.5131999850273132,  lr: 0.009093665862068966
step: 3632, train_loss: 1.1385246515274048, acc: 0.5189999938011169, val_loss: 1.1677969694137573, val_acc: 0.5037999749183655,  lr: 0.009093321379310345
step: 3633, train_loss: 1.2367043495178223, acc: 0.4819999933242798, val_loss: 1.162931203842163, val_acc: 0.503000020980835,  lr: 0.009092976896551724
step: 3634, train_loss: 1.1814814805984497, acc: 0.49720001220703125, val_loss: 1.1557738780975342, val_acc: 0.5085999965667725,  lr: 0.009092632413793104
step: 3635, train_loss: 1.2309147119522095, acc: 0.47540000081062317, val_loss: 1.1484038829803467, val_acc: 0.5113999843597412,  lr: 0.009092287931034483
step: 3636, train_loss: 1.1323847770690918, acc: 0.520799994468689, val_loss: 1.1441805362701416, val_acc: 0.520799994468689,  lr: 0.009091943448275863
step: 3637, train_loss: 1.1117368936538696, acc: 0.5302000045776367, val_loss: 1.140832543373108, val_acc: 0.5198000073432922,  lr: 0.009091598965517242
step: 3638, train_loss: 1.1883306503295898, acc: 0.4966000020503998, val_loss: 1.1410741806030273, val_acc: 0.5166000127792358,  lr: 0.009091254482758621
step: 3639, train_loss: 1.1440411806106567, acc: 0.5202000141143799, val_loss: 1.1474984884262085, val_acc: 0.5149999856948853,  lr: 0.00909091
step: 3640, train_loss: 1.1129306554794312, acc: 0.5246000289916992, val_loss: 1.1363186836242676, val_acc: 0.5192000269889832,  lr: 0.00909056551724138
step: 3641, train_loss: 1.1092256307601929, acc: 0.5410000085830688, val_loss: 1.1388044357299805, val_acc: 0.5175999999046326,  lr: 0.00909022103448276
step: 3642, train_loss: 1.10204017162323, acc: 0.5460000038146973, val_loss: 1.1394673585891724, val_acc: 0.5188000202178955,  lr: 0.009089876551724139
step: 3643, train_loss: 1.1503400802612305, acc: 0.5156000256538391, val_loss: 1.1469063758850098, val_acc: 0.5194000005722046,  lr: 0.009089532068965518
step: 3644, train_loss: 1.1520191431045532, acc: 0.5221999883651733, val_loss: 1.1470664739608765, val_acc: 0.524399995803833,  lr: 0.009089187586206898
step: 3645, train_loss: 1.2173566818237305, acc: 0.487199991941452, val_loss: 1.1438679695129395, val_acc: 0.5231999754905701,  lr: 0.009088843103448275
step: 3646, train_loss: 1.1035680770874023, acc: 0.5347999930381775, val_loss: 1.1468267440795898, val_acc: 0.5180000066757202,  lr: 0.009088498620689655
step: 3647, train_loss: 1.1108551025390625, acc: 0.5353999733924866, val_loss: 1.150225043296814, val_acc: 0.5135999917984009,  lr: 0.009088154137931034
step: 3648, train_loss: 1.1725600957870483, acc: 0.5063999891281128, val_loss: 1.1436436176300049, val_acc: 0.5194000005722046,  lr: 0.009087809655172413
step: 3649, train_loss: 1.1046435832977295, acc: 0.5288000106811523, val_loss: 1.142945647239685, val_acc: 0.521399974822998,  lr: 0.009087465172413793
step: 3650, train_loss: 1.107113242149353, acc: 0.5297999978065491, val_loss: 1.1447832584381104, val_acc: 0.5188000202178955,  lr: 0.009087120689655172
step: 3651, train_loss: 1.2300794124603271, acc: 0.48159998655319214, val_loss: 1.1526275873184204, val_acc: 0.5235999822616577,  lr: 0.009086776206896552
step: 3652, train_loss: 1.1073859930038452, acc: 0.5342000126838684, val_loss: 1.1560537815093994, val_acc: 0.5194000005722046,  lr: 0.009086431724137931
step: 3653, train_loss: 1.1251565217971802, acc: 0.5297999978065491, val_loss: 1.1621417999267578, val_acc: 0.517799973487854,  lr: 0.00908608724137931
step: 3654, train_loss: 1.1425578594207764, acc: 0.5199999809265137, val_loss: 1.1452763080596924, val_acc: 0.5230000019073486,  lr: 0.00908574275862069
step: 3655, train_loss: 1.1016199588775635, acc: 0.5374000072479248, val_loss: 1.1502742767333984, val_acc: 0.5116000175476074,  lr: 0.009085398275862069
step: 3656, train_loss: 1.0995426177978516, acc: 0.5356000065803528, val_loss: 1.1445224285125732, val_acc: 0.5099999904632568,  lr: 0.009085053793103448
step: 3657, train_loss: 1.1352777481079102, acc: 0.5202000141143799, val_loss: 1.1392662525177002, val_acc: 0.5189999938011169,  lr: 0.009084709310344828
step: 3658, train_loss: 1.1422995328903198, acc: 0.5206000208854675, val_loss: 1.1426334381103516, val_acc: 0.520799994468689,  lr: 0.009084364827586207
step: 3659, train_loss: 1.144477128982544, acc: 0.5184000134468079, val_loss: 1.1505136489868164, val_acc: 0.5210000276565552,  lr: 0.009084020344827587
step: 3660, train_loss: 1.0848634243011475, acc: 0.5450000166893005, val_loss: 1.1546752452850342, val_acc: 0.5121999979019165,  lr: 0.009083675862068966
step: 3661, train_loss: 1.0930653810501099, acc: 0.5411999821662903, val_loss: 1.1534950733184814, val_acc: 0.5224000215530396,  lr: 0.009083331379310345
step: 3662, train_loss: 1.0808954238891602, acc: 0.5368000268936157, val_loss: 1.148622989654541, val_acc: 0.5266000032424927,  lr: 0.009082986896551725
step: 3663, train_loss: 1.151675820350647, acc: 0.5081999897956848, val_loss: 1.147925853729248, val_acc: 0.5230000019073486,  lr: 0.009082642413793104
step: 3664, train_loss: 1.062883734703064, acc: 0.5582000017166138, val_loss: 1.1547294855117798, val_acc: 0.515999972820282,  lr: 0.009082297931034483
step: 3665, train_loss: 1.2239668369293213, acc: 0.4903999865055084, val_loss: 1.1482317447662354, val_acc: 0.517799973487854,  lr: 0.009081953448275863
step: 3666, train_loss: 1.1388949155807495, acc: 0.5248000025749207, val_loss: 1.1517633199691772, val_acc: 0.5194000005722046,  lr: 0.009081608965517242
step: 3667, train_loss: 1.0535247325897217, acc: 0.5669999718666077, val_loss: 1.1563215255737305, val_acc: 0.5163999795913696,  lr: 0.009081264482758622
step: 3668, train_loss: 1.0830934047698975, acc: 0.5460000038146973, val_loss: 1.156851887702942, val_acc: 0.5138000249862671,  lr: 0.00908092
step: 3669, train_loss: 1.10194730758667, acc: 0.5382000207901001, val_loss: 1.1543203592300415, val_acc: 0.5156000256538391,  lr: 0.009080575517241379
step: 3670, train_loss: 1.071668267250061, acc: 0.5482000112533569, val_loss: 1.156191349029541, val_acc: 0.515999972820282,  lr: 0.009080231034482758
step: 3671, train_loss: 1.2219332456588745, acc: 0.4875999987125397, val_loss: 1.1605708599090576, val_acc: 0.5138000249862671,  lr: 0.009079886551724137
step: 3672, train_loss: 1.1479250192642212, acc: 0.5148000121116638, val_loss: 1.1561311483383179, val_acc: 0.5163999795913696,  lr: 0.009079542068965517
step: 3673, train_loss: 1.1943373680114746, acc: 0.49380001425743103, val_loss: 1.1588596105575562, val_acc: 0.5153999924659729,  lr: 0.009079197586206896
step: 3674, train_loss: 1.193664312362671, acc: 0.4893999993801117, val_loss: 1.1567628383636475, val_acc: 0.5152000188827515,  lr: 0.009078853103448276
step: 3675, train_loss: 1.0650502443313599, acc: 0.5475999712944031, val_loss: 1.1531096696853638, val_acc: 0.5184000134468079,  lr: 0.009078508620689655
step: 3676, train_loss: 1.1444458961486816, acc: 0.5234000086784363, val_loss: 1.150412678718567, val_acc: 0.5242000222206116,  lr: 0.009078164137931034
step: 3677, train_loss: 1.1260606050491333, acc: 0.5320000052452087, val_loss: 1.1647261381149292, val_acc: 0.5109999775886536,  lr: 0.009077819655172414
step: 3678, train_loss: 1.1478744745254517, acc: 0.5108000040054321, val_loss: 1.157827377319336, val_acc: 0.5130000114440918,  lr: 0.009077475172413793
step: 3679, train_loss: 1.1215646266937256, acc: 0.5368000268936157, val_loss: 1.1516388654708862, val_acc: 0.5145999789237976,  lr: 0.009077130689655172
step: 3680, train_loss: 1.1010240316390991, acc: 0.5440000295639038, val_loss: 1.1523703336715698, val_acc: 0.5138000249862671,  lr: 0.009076786206896552
step: 3681, train_loss: 1.0830299854278564, acc: 0.5428000092506409, val_loss: 1.1659940481185913, val_acc: 0.5098000168800354,  lr: 0.009076441724137931
step: 3682, train_loss: 1.1025434732437134, acc: 0.5315999984741211, val_loss: 1.1688059568405151, val_acc: 0.5098000168800354,  lr: 0.00907609724137931
step: 3683, train_loss: 1.2197108268737793, acc: 0.49079999327659607, val_loss: 1.1687912940979004, val_acc: 0.5081999897956848,  lr: 0.00907575275862069
step: 3684, train_loss: 1.096699595451355, acc: 0.5351999998092651, val_loss: 1.1518734693527222, val_acc: 0.5120000243186951,  lr: 0.00907540827586207
step: 3685, train_loss: 1.068926215171814, acc: 0.5532000064849854, val_loss: 1.1481449604034424, val_acc: 0.5198000073432922,  lr: 0.009075063793103449
step: 3686, train_loss: 1.1324028968811035, acc: 0.5289999842643738, val_loss: 1.1473337411880493, val_acc: 0.5199999809265137,  lr: 0.009074719310344828
step: 3687, train_loss: 1.137368083000183, acc: 0.5260000228881836, val_loss: 1.149163842201233, val_acc: 0.5174000263214111,  lr: 0.009074374827586208
step: 3688, train_loss: 1.2381848096847534, acc: 0.47200000286102295, val_loss: 1.1617391109466553, val_acc: 0.5044000148773193,  lr: 0.009074030344827587
step: 3689, train_loss: 1.1178478002548218, acc: 0.5270000100135803, val_loss: 1.1507089138031006, val_acc: 0.5170000195503235,  lr: 0.009073685862068966
step: 3690, train_loss: 1.0563859939575195, acc: 0.5604000091552734, val_loss: 1.1505485773086548, val_acc: 0.517799973487854,  lr: 0.009073341379310346
step: 3691, train_loss: 1.185090184211731, acc: 0.5019999742507935, val_loss: 1.1482876539230347, val_acc: 0.5149999856948853,  lr: 0.009072996896551725
step: 3692, train_loss: 1.0521607398986816, acc: 0.5600000023841858, val_loss: 1.1555066108703613, val_acc: 0.5199999809265137,  lr: 0.009072652413793104
step: 3693, train_loss: 1.0765998363494873, acc: 0.5397999882698059, val_loss: 1.1623867750167847, val_acc: 0.5103999972343445,  lr: 0.009072307931034484
step: 3694, train_loss: 1.1124972105026245, acc: 0.5264000296592712, val_loss: 1.1645050048828125, val_acc: 0.5080000162124634,  lr: 0.009071963448275861
step: 3695, train_loss: 1.1398723125457764, acc: 0.5252000093460083, val_loss: 1.1659034490585327, val_acc: 0.5031999945640564,  lr: 0.00907161896551724
step: 3696, train_loss: 1.0473421812057495, acc: 0.5644000172615051, val_loss: 1.1600598096847534, val_acc: 0.5073999762535095,  lr: 0.00907127448275862
step: 3697, train_loss: 1.0818167924880981, acc: 0.5491999983787537, val_loss: 1.166515827178955, val_acc: 0.5,  lr: 0.00907093
step: 3698, train_loss: 1.2522797584533691, acc: 0.4745999872684479, val_loss: 1.1654460430145264, val_acc: 0.4966000020503998,  lr: 0.009070585517241379
step: 3699, train_loss: 1.11851167678833, acc: 0.5306000113487244, val_loss: 1.156664252281189, val_acc: 0.5116000175476074,  lr: 0.009070241034482758
step: 3700, train_loss: 1.1097458600997925, acc: 0.5332000255584717, val_loss: 1.1535269021987915, val_acc: 0.5175999999046326,  lr: 0.009069896551724138
step: 3701, train_loss: 1.099702000617981, acc: 0.5419999957084656, val_loss: 1.159690022468567, val_acc: 0.5094000101089478,  lr: 0.009069552068965517
step: 3702, train_loss: 1.2173703908920288, acc: 0.487199991941452, val_loss: 1.159103274345398, val_acc: 0.5034000277519226,  lr: 0.009069207586206897
step: 3703, train_loss: 1.2106047868728638, acc: 0.49480000138282776, val_loss: 1.1598213911056519, val_acc: 0.5090000033378601,  lr: 0.009068863103448276
step: 3704, train_loss: 1.1685187816619873, acc: 0.4984000027179718, val_loss: 1.1631245613098145, val_acc: 0.508400022983551,  lr: 0.009068518620689655
step: 3705, train_loss: 1.2033463716506958, acc: 0.49079999327659607, val_loss: 1.160416841506958, val_acc: 0.5126000046730042,  lr: 0.009068174137931035
step: 3706, train_loss: 1.136979579925537, acc: 0.5271999835968018, val_loss: 1.1571831703186035, val_acc: 0.508400022983551,  lr: 0.009067829655172414
step: 3707, train_loss: 1.1552921533584595, acc: 0.5242000222206116, val_loss: 1.1599700450897217, val_acc: 0.5077999830245972,  lr: 0.009067485172413793
step: 3708, train_loss: 1.1356232166290283, acc: 0.5271999835968018, val_loss: 1.157347559928894, val_acc: 0.5052000284194946,  lr: 0.009067140689655173
step: 3709, train_loss: 1.1111117601394653, acc: 0.5379999876022339, val_loss: 1.161773920059204, val_acc: 0.5034000277519226,  lr: 0.009066796206896552
step: 3710, train_loss: 1.2468066215515137, acc: 0.4697999954223633, val_loss: 1.1540868282318115, val_acc: 0.5090000033378601,  lr: 0.009066451724137932
step: 3711, train_loss: 1.147120714187622, acc: 0.5138000249862671, val_loss: 1.156448483467102, val_acc: 0.5105999708175659,  lr: 0.009066107241379311
step: 3712, train_loss: 1.1545478105545044, acc: 0.5185999870300293, val_loss: 1.1624289751052856, val_acc: 0.5055999755859375,  lr: 0.00906576275862069
step: 3713, train_loss: 1.15195894241333, acc: 0.5189999938011169, val_loss: 1.163097858428955, val_acc: 0.5085999965667725,  lr: 0.00906541827586207
step: 3714, train_loss: 1.139061450958252, acc: 0.5198000073432922, val_loss: 1.1646331548690796, val_acc: 0.5109999775886536,  lr: 0.009065073793103449
step: 3715, train_loss: 1.0639594793319702, acc: 0.5564000010490417, val_loss: 1.1625500917434692, val_acc: 0.5091999769210815,  lr: 0.009064729310344828
step: 3716, train_loss: 1.0617049932479858, acc: 0.5527999997138977, val_loss: 1.154415488243103, val_acc: 0.5153999924659729,  lr: 0.009064384827586206
step: 3717, train_loss: 1.0870919227600098, acc: 0.5493999719619751, val_loss: 1.1522268056869507, val_acc: 0.5149999856948853,  lr: 0.009064040344827586
step: 3718, train_loss: 1.22305166721344, acc: 0.4832000136375427, val_loss: 1.149898886680603, val_acc: 0.5116000175476074,  lr: 0.009063695862068965
step: 3719, train_loss: 1.102431297302246, acc: 0.5321999788284302, val_loss: 1.150732159614563, val_acc: 0.5163999795913696,  lr: 0.009063351379310344
step: 3720, train_loss: 1.2071552276611328, acc: 0.49219998717308044, val_loss: 1.1708295345306396, val_acc: 0.5072000026702881,  lr: 0.009063006896551724
step: 3721, train_loss: 1.1113165616989136, acc: 0.5307999849319458, val_loss: 1.1615302562713623, val_acc: 0.5049999952316284,  lr: 0.009062662413793103
step: 3722, train_loss: 1.1142455339431763, acc: 0.5296000242233276, val_loss: 1.1690247058868408, val_acc: 0.5040000081062317,  lr: 0.009062317931034482
step: 3723, train_loss: 1.129379153251648, acc: 0.5317999720573425, val_loss: 1.1784757375717163, val_acc: 0.5009999871253967,  lr: 0.009061973448275862
step: 3724, train_loss: 1.188463568687439, acc: 0.49959999322891235, val_loss: 1.1720081567764282, val_acc: 0.5098000168800354,  lr: 0.009061628965517241
step: 3725, train_loss: 1.1925257444381714, acc: 0.4968000054359436, val_loss: 1.1651926040649414, val_acc: 0.51419997215271,  lr: 0.00906128448275862
step: 3726, train_loss: 1.1612017154693604, acc: 0.5157999992370605, val_loss: 1.1672838926315308, val_acc: 0.504800021648407,  lr: 0.00906094
step: 3727, train_loss: 1.1244654655456543, acc: 0.5281999707221985, val_loss: 1.1646032333374023, val_acc: 0.5088000297546387,  lr: 0.00906059551724138
step: 3728, train_loss: 1.1517161130905151, acc: 0.5112000107765198, val_loss: 1.1567180156707764, val_acc: 0.5171999931335449,  lr: 0.009060251034482759
step: 3729, train_loss: 1.1048846244812012, acc: 0.5338000059127808, val_loss: 1.1600408554077148, val_acc: 0.5162000060081482,  lr: 0.009059906551724138
step: 3730, train_loss: 1.1302881240844727, acc: 0.5266000032424927, val_loss: 1.1572837829589844, val_acc: 0.5202000141143799,  lr: 0.009059562068965517
step: 3731, train_loss: 1.1302659511566162, acc: 0.5321999788284302, val_loss: 1.1542258262634277, val_acc: 0.5175999999046326,  lr: 0.009059217586206897
step: 3732, train_loss: 1.0854874849319458, acc: 0.5468000173568726, val_loss: 1.1579599380493164, val_acc: 0.5108000040054321,  lr: 0.009058873103448276
step: 3733, train_loss: 1.0713510513305664, acc: 0.5486000180244446, val_loss: 1.1566522121429443, val_acc: 0.5054000020027161,  lr: 0.009058528620689656
step: 3734, train_loss: 1.0925809144973755, acc: 0.5422000288963318, val_loss: 1.1588447093963623, val_acc: 0.5073999762535095,  lr: 0.009058184137931035
step: 3735, train_loss: 1.1975078582763672, acc: 0.5052000284194946, val_loss: 1.1632161140441895, val_acc: 0.5095999836921692,  lr: 0.009057839655172414
step: 3736, train_loss: 1.1657919883728027, acc: 0.5144000053405762, val_loss: 1.1730414628982544, val_acc: 0.5063999891281128,  lr: 0.009057495172413794
step: 3737, train_loss: 1.0871028900146484, acc: 0.5388000011444092, val_loss: 1.175087809562683, val_acc: 0.5001999735832214,  lr: 0.009057150689655173
step: 3738, train_loss: 1.1717499494552612, acc: 0.4966000020503998, val_loss: 1.1724687814712524, val_acc: 0.4991999864578247,  lr: 0.009056806206896553
step: 3739, train_loss: 1.1277554035186768, acc: 0.5212000012397766, val_loss: 1.1632136106491089, val_acc: 0.5070000290870667,  lr: 0.009056461724137932
step: 3740, train_loss: 1.0883126258850098, acc: 0.5496000051498413, val_loss: 1.1670459508895874, val_acc: 0.5054000020027161,  lr: 0.009056117241379311
step: 3741, train_loss: 1.1090922355651855, acc: 0.5320000052452087, val_loss: 1.1574455499649048, val_acc: 0.5108000040054321,  lr: 0.00905577275862069
step: 3742, train_loss: 1.0962483882904053, acc: 0.5383999943733215, val_loss: 1.164132833480835, val_acc: 0.5070000290870667,  lr: 0.009055428275862068
step: 3743, train_loss: 1.1871118545532227, acc: 0.49480000138282776, val_loss: 1.167523741722107, val_acc: 0.5117999911308289,  lr: 0.009055083793103448
step: 3744, train_loss: 1.1809346675872803, acc: 0.49799999594688416, val_loss: 1.1666151285171509, val_acc: 0.508400022983551,  lr: 0.009054739310344827
step: 3745, train_loss: 1.1493905782699585, acc: 0.5163999795913696, val_loss: 1.1750150918960571, val_acc: 0.5041999816894531,  lr: 0.009054394827586206
step: 3746, train_loss: 1.0585585832595825, acc: 0.5627999901771545, val_loss: 1.1675554513931274, val_acc: 0.5095999836921692,  lr: 0.009054050344827586
step: 3747, train_loss: 1.1009938716888428, acc: 0.5447999835014343, val_loss: 1.1682218313217163, val_acc: 0.510200023651123,  lr: 0.009053705862068965
step: 3748, train_loss: 1.0933483839035034, acc: 0.5414000153541565, val_loss: 1.1679143905639648, val_acc: 0.5112000107765198,  lr: 0.009053361379310345
step: 3749, train_loss: 1.085180640220642, acc: 0.5460000038146973, val_loss: 1.1763919591903687, val_acc: 0.5034000277519226,  lr: 0.009053016896551724
step: 3750, train_loss: 1.1478376388549805, acc: 0.5212000012397766, val_loss: 1.1777353286743164, val_acc: 0.49959999322891235,  lr: 0.009052672413793103
step: 3751, train_loss: 1.0912624597549438, acc: 0.5360000133514404, val_loss: 1.1611448526382446, val_acc: 0.510200023651123,  lr: 0.009052327931034483
step: 3752, train_loss: 1.0822280645370483, acc: 0.5396000146865845, val_loss: 1.1621752977371216, val_acc: 0.5091999769210815,  lr: 0.009051983448275862
step: 3753, train_loss: 1.0747851133346558, acc: 0.5428000092506409, val_loss: 1.1772454977035522, val_acc: 0.4970000088214874,  lr: 0.009051638965517242
step: 3754, train_loss: 1.1512696743011475, acc: 0.5174000263214111, val_loss: 1.175187110900879, val_acc: 0.49559998512268066,  lr: 0.009051294482758621
step: 3755, train_loss: 1.0704996585845947, acc: 0.5483999848365784, val_loss: 1.1782277822494507, val_acc: 0.498199999332428,  lr: 0.00905095
step: 3756, train_loss: 1.2369722127914429, acc: 0.4803999960422516, val_loss: 1.1849710941314697, val_acc: 0.49619999527931213,  lr: 0.00905060551724138
step: 3757, train_loss: 1.0967040061950684, acc: 0.5410000085830688, val_loss: 1.1789110898971558, val_acc: 0.5041999816894531,  lr: 0.009050261034482759
step: 3758, train_loss: 1.0945757627487183, acc: 0.5415999889373779, val_loss: 1.1726605892181396, val_acc: 0.49900001287460327,  lr: 0.009049916551724138
step: 3759, train_loss: 1.0833295583724976, acc: 0.5515999794006348, val_loss: 1.1812654733657837, val_acc: 0.5009999871253967,  lr: 0.009049572068965518
step: 3760, train_loss: 1.0643513202667236, acc: 0.5493999719619751, val_loss: 1.180694580078125, val_acc: 0.503600001335144,  lr: 0.009049227586206897
step: 3761, train_loss: 1.0657826662063599, acc: 0.5475999712944031, val_loss: 1.1933220624923706, val_acc: 0.49799999594688416,  lr: 0.009048883103448277
step: 3762, train_loss: 1.1555589437484741, acc: 0.5117999911308289, val_loss: 1.1835871934890747, val_acc: 0.49779999256134033,  lr: 0.009048538620689656
step: 3763, train_loss: 1.0877926349639893, acc: 0.5407999753952026, val_loss: 1.1868559122085571, val_acc: 0.5,  lr: 0.009048194137931035
step: 3764, train_loss: 1.1007989645004272, acc: 0.5446000099182129, val_loss: 1.185356616973877, val_acc: 0.503600001335144,  lr: 0.009047849655172415
step: 3765, train_loss: 1.0477263927459717, acc: 0.5594000220298767, val_loss: 1.198337435722351, val_acc: 0.49720001220703125,  lr: 0.009047505172413792
step: 3766, train_loss: 1.0914398431777954, acc: 0.5446000099182129, val_loss: 1.194891333580017, val_acc: 0.4986000061035156,  lr: 0.009047160689655172
step: 3767, train_loss: 1.0805341005325317, acc: 0.5428000092506409, val_loss: 1.1951172351837158, val_acc: 0.5,  lr: 0.009046816206896551
step: 3768, train_loss: 1.0952707529067993, acc: 0.5422000288963318, val_loss: 1.1903177499771118, val_acc: 0.49900001287460327,  lr: 0.00904647172413793
step: 3769, train_loss: 1.1489113569259644, acc: 0.5112000107765198, val_loss: 1.1928057670593262, val_acc: 0.49779999256134033,  lr: 0.00904612724137931
step: 3770, train_loss: 1.1826744079589844, acc: 0.4991999864578247, val_loss: 1.195820927619934, val_acc: 0.4952000081539154,  lr: 0.00904578275862069
step: 3771, train_loss: 1.0846095085144043, acc: 0.5440000295639038, val_loss: 1.1868897676467896, val_acc: 0.49900001287460327,  lr: 0.009045438275862069
step: 3772, train_loss: 1.2081398963928223, acc: 0.48899999260902405, val_loss: 1.1980388164520264, val_acc: 0.4896000027656555,  lr: 0.009045093793103448
step: 3773, train_loss: 1.0575872659683228, acc: 0.5511999726295471, val_loss: 1.1872466802597046, val_acc: 0.49880000948905945,  lr: 0.009044749310344827
step: 3774, train_loss: 1.1340521574020386, acc: 0.5278000235557556, val_loss: 1.2044947147369385, val_acc: 0.4968000054359436,  lr: 0.009044404827586207
step: 3775, train_loss: 1.1712297201156616, acc: 0.5034000277519226, val_loss: 1.1909407377243042, val_acc: 0.4959999918937683,  lr: 0.009044060344827586
step: 3776, train_loss: 1.0790350437164307, acc: 0.5478000044822693, val_loss: 1.190695881843567, val_acc: 0.498199999332428,  lr: 0.009043715862068966
step: 3777, train_loss: 1.1395576000213623, acc: 0.5231999754905701, val_loss: 1.1928287744522095, val_acc: 0.5001999735832214,  lr: 0.009043371379310345
step: 3778, train_loss: 1.1287343502044678, acc: 0.5180000066757202, val_loss: 1.1860811710357666, val_acc: 0.4970000088214874,  lr: 0.009043026896551724
step: 3779, train_loss: 1.061641812324524, acc: 0.5428000092506409, val_loss: 1.1892045736312866, val_acc: 0.4943999946117401,  lr: 0.009042682413793104
step: 3780, train_loss: 1.1079270839691162, acc: 0.5343999862670898, val_loss: 1.1842843294143677, val_acc: 0.4943999946117401,  lr: 0.009042337931034483
step: 3781, train_loss: 1.1202768087387085, acc: 0.5293999910354614, val_loss: 1.1888965368270874, val_acc: 0.4997999966144562,  lr: 0.009041993448275863
step: 3782, train_loss: 1.0611836910247803, acc: 0.5483999848365784, val_loss: 1.1835500001907349, val_acc: 0.501800000667572,  lr: 0.009041648965517242
step: 3783, train_loss: 1.2631773948669434, acc: 0.46000000834465027, val_loss: 1.1823933124542236, val_acc: 0.5008000135421753,  lr: 0.009041304482758621
step: 3784, train_loss: 1.134047269821167, acc: 0.5257999897003174, val_loss: 1.182559609413147, val_acc: 0.49799999594688416,  lr: 0.00904096
step: 3785, train_loss: 1.2083539962768555, acc: 0.4903999865055084, val_loss: 1.1896283626556396, val_acc: 0.49799999594688416,  lr: 0.00904061551724138
step: 3786, train_loss: 1.1539051532745361, acc: 0.5077999830245972, val_loss: 1.1973075866699219, val_acc: 0.4880000054836273,  lr: 0.00904027103448276
step: 3787, train_loss: 1.1062266826629639, acc: 0.5360000133514404, val_loss: 1.1824615001678467, val_acc: 0.5023999810218811,  lr: 0.009039926551724139
step: 3788, train_loss: 1.0858732461929321, acc: 0.5386000275611877, val_loss: 1.1677610874176025, val_acc: 0.508400022983551,  lr: 0.009039582068965518
step: 3789, train_loss: 1.1397621631622314, acc: 0.5216000080108643, val_loss: 1.1678717136383057, val_acc: 0.5070000290870667,  lr: 0.009039237586206898
step: 3790, train_loss: 1.153351902961731, acc: 0.5127999782562256, val_loss: 1.1713794469833374, val_acc: 0.5073999762535095,  lr: 0.009038893103448275
step: 3791, train_loss: 1.2431390285491943, acc: 0.487199991941452, val_loss: 1.1522992849349976, val_acc: 0.521399974822998,  lr: 0.009038548620689655
step: 3792, train_loss: 1.086174488067627, acc: 0.5464000105857849, val_loss: 1.1480441093444824, val_acc: 0.524399995803833,  lr: 0.009038204137931034
step: 3793, train_loss: 1.0750296115875244, acc: 0.5511999726295471, val_loss: 1.1464396715164185, val_acc: 0.5171999931335449,  lr: 0.009037859655172413
step: 3794, train_loss: 1.0933319330215454, acc: 0.5442000031471252, val_loss: 1.140232801437378, val_acc: 0.5189999938011169,  lr: 0.009037515172413793
step: 3795, train_loss: 1.1708773374557495, acc: 0.503000020980835, val_loss: 1.1361043453216553, val_acc: 0.5220000147819519,  lr: 0.009037170689655172
step: 3796, train_loss: 1.186187744140625, acc: 0.49939998984336853, val_loss: 1.1286600828170776, val_acc: 0.5271999835968018,  lr: 0.009036826206896552
step: 3797, train_loss: 1.1133322715759277, acc: 0.5370000004768372, val_loss: 1.1305971145629883, val_acc: 0.5249999761581421,  lr: 0.009036481724137931
step: 3798, train_loss: 1.168737530708313, acc: 0.4957999885082245, val_loss: 1.1276291608810425, val_acc: 0.5296000242233276,  lr: 0.00903613724137931
step: 3799, train_loss: 1.0726553201675415, acc: 0.5577999949455261, val_loss: 1.1236414909362793, val_acc: 0.5278000235557556,  lr: 0.00903579275862069
step: 3800, train_loss: 1.0912779569625854, acc: 0.5454000234603882, val_loss: 1.1203749179840088, val_acc: 0.5296000242233276,  lr: 0.009035448275862069
step: 3801, train_loss: 1.077604055404663, acc: 0.5497999787330627, val_loss: 1.1272563934326172, val_acc: 0.5249999761581421,  lr: 0.009035103793103448
step: 3802, train_loss: 1.0975323915481567, acc: 0.5374000072479248, val_loss: 1.122589111328125, val_acc: 0.5260000228881836,  lr: 0.009034759310344828
step: 3803, train_loss: 1.1384960412979126, acc: 0.5235999822616577, val_loss: 1.129177212715149, val_acc: 0.5221999883651733,  lr: 0.009034414827586207
step: 3804, train_loss: 1.2261981964111328, acc: 0.4747999906539917, val_loss: 1.1332124471664429, val_acc: 0.5203999876976013,  lr: 0.009034070344827587
step: 3805, train_loss: 1.125975489616394, acc: 0.5278000235557556, val_loss: 1.1443235874176025, val_acc: 0.5184000134468079,  lr: 0.009033725862068966
step: 3806, train_loss: 1.172953486442566, acc: 0.504800021648407, val_loss: 1.1433504819869995, val_acc: 0.5116000175476074,  lr: 0.009033381379310345
step: 3807, train_loss: 1.1326993703842163, acc: 0.5293999910354614, val_loss: 1.144387125968933, val_acc: 0.5123999714851379,  lr: 0.009033036896551725
step: 3808, train_loss: 1.1459697484970093, acc: 0.5166000127792358, val_loss: 1.1483051776885986, val_acc: 0.5099999904632568,  lr: 0.009032692413793104
step: 3809, train_loss: 1.1488289833068848, acc: 0.5174000263214111, val_loss: 1.150508165359497, val_acc: 0.5157999992370605,  lr: 0.009032347931034483
step: 3810, train_loss: 1.1332108974456787, acc: 0.5216000080108643, val_loss: 1.15042245388031, val_acc: 0.5127999782562256,  lr: 0.009032003448275863
step: 3811, train_loss: 1.1161437034606934, acc: 0.526199996471405, val_loss: 1.1563441753387451, val_acc: 0.5059999823570251,  lr: 0.009031658965517242
step: 3812, train_loss: 1.060626745223999, acc: 0.5465999841690063, val_loss: 1.1543511152267456, val_acc: 0.5099999904632568,  lr: 0.009031314482758622
step: 3813, train_loss: 1.1373285055160522, acc: 0.519599974155426, val_loss: 1.1442309617996216, val_acc: 0.5170000195503235,  lr: 0.00903097
step: 3814, train_loss: 1.0840059518814087, acc: 0.5397999882698059, val_loss: 1.1499533653259277, val_acc: 0.5139999985694885,  lr: 0.009030625517241379
step: 3815, train_loss: 1.1101040840148926, acc: 0.5365999937057495, val_loss: 1.1532502174377441, val_acc: 0.5055999755859375,  lr: 0.009030281034482758
step: 3816, train_loss: 1.0934559106826782, acc: 0.5356000065803528, val_loss: 1.1504180431365967, val_acc: 0.51419997215271,  lr: 0.009029936551724137
step: 3817, train_loss: 1.1175919771194458, acc: 0.5365999937057495, val_loss: 1.1532421112060547, val_acc: 0.5153999924659729,  lr: 0.009029592068965517
step: 3818, train_loss: 1.1771317720413208, acc: 0.5131999850273132, val_loss: 1.155127763748169, val_acc: 0.5127999782562256,  lr: 0.009029247586206896
step: 3819, train_loss: 1.1298390626907349, acc: 0.5224000215530396, val_loss: 1.158783197402954, val_acc: 0.5076000094413757,  lr: 0.009028903103448276
step: 3820, train_loss: 1.1370891332626343, acc: 0.5238000154495239, val_loss: 1.1562068462371826, val_acc: 0.5105999708175659,  lr: 0.009028558620689655
step: 3821, train_loss: 1.1123114824295044, acc: 0.5374000072479248, val_loss: 1.1553832292556763, val_acc: 0.5090000033378601,  lr: 0.009028214137931034
step: 3822, train_loss: 1.124527931213379, acc: 0.5311999917030334, val_loss: 1.1554673910140991, val_acc: 0.5166000127792358,  lr: 0.009027869655172414
step: 3823, train_loss: 1.1429500579833984, acc: 0.5189999938011169, val_loss: 1.1624534130096436, val_acc: 0.5098000168800354,  lr: 0.009027525172413793
step: 3824, train_loss: 1.1223710775375366, acc: 0.5252000093460083, val_loss: 1.1570603847503662, val_acc: 0.5123999714851379,  lr: 0.009027180689655172
step: 3825, train_loss: 1.1105846166610718, acc: 0.5332000255584717, val_loss: 1.1507824659347534, val_acc: 0.5130000114440918,  lr: 0.009026836206896552
step: 3826, train_loss: 1.0771678686141968, acc: 0.551800012588501, val_loss: 1.1545132398605347, val_acc: 0.5109999775886536,  lr: 0.009026491724137931
step: 3827, train_loss: 1.1149541139602661, acc: 0.5234000086784363, val_loss: 1.161720633506775, val_acc: 0.5052000284194946,  lr: 0.00902614724137931
step: 3828, train_loss: 1.0896114110946655, acc: 0.5379999876022339, val_loss: 1.1670233011245728, val_acc: 0.5062000155448914,  lr: 0.00902580275862069
step: 3829, train_loss: 1.0978310108184814, acc: 0.5338000059127808, val_loss: 1.1649036407470703, val_acc: 0.5059999823570251,  lr: 0.00902545827586207
step: 3830, train_loss: 1.187040090560913, acc: 0.49559998512268066, val_loss: 1.172721028327942, val_acc: 0.5008000135421753,  lr: 0.009025113793103449
step: 3831, train_loss: 1.2192184925079346, acc: 0.4934000074863434, val_loss: 1.1953600645065308, val_acc: 0.4925999939441681,  lr: 0.009024769310344828
step: 3832, train_loss: 1.0861088037490845, acc: 0.5375999808311462, val_loss: 1.1789504289627075, val_acc: 0.49880000948905945,  lr: 0.009024424827586208
step: 3833, train_loss: 1.073945164680481, acc: 0.546999990940094, val_loss: 1.1805427074432373, val_acc: 0.49880000948905945,  lr: 0.009024080344827587
step: 3834, train_loss: 1.0699702501296997, acc: 0.5450000166893005, val_loss: 1.1808515787124634, val_acc: 0.5,  lr: 0.009023735862068966
step: 3835, train_loss: 1.0518407821655273, acc: 0.5568000078201294, val_loss: 1.1830826997756958, val_acc: 0.5019999742507935,  lr: 0.009023391379310346
step: 3836, train_loss: 1.1402488946914673, acc: 0.5235999822616577, val_loss: 1.1807925701141357, val_acc: 0.5027999877929688,  lr: 0.009023046896551725
step: 3837, train_loss: 1.152754783630371, acc: 0.5138000249862671, val_loss: 1.1744030714035034, val_acc: 0.4991999864578247,  lr: 0.009022702413793104
step: 3838, train_loss: 1.1219981908798218, acc: 0.5230000019073486, val_loss: 1.1673694849014282, val_acc: 0.503000020980835,  lr: 0.009022357931034484
step: 3839, train_loss: 1.1242411136627197, acc: 0.5220000147819519, val_loss: 1.163111925125122, val_acc: 0.5044000148773193,  lr: 0.009022013448275861
step: 3840, train_loss: 1.07612943649292, acc: 0.5483999848365784, val_loss: 1.1639904975891113, val_acc: 0.5090000033378601,  lr: 0.00902166896551724
step: 3841, train_loss: 1.0961908102035522, acc: 0.5432000160217285, val_loss: 1.1543262004852295, val_acc: 0.5138000249862671,  lr: 0.00902132448275862
step: 3842, train_loss: 1.0422552824020386, acc: 0.5616000294685364, val_loss: 1.1597548723220825, val_acc: 0.5070000290870667,  lr: 0.00902098
step: 3843, train_loss: 1.0269932746887207, acc: 0.5662000179290771, val_loss: 1.15842866897583, val_acc: 0.5091999769210815,  lr: 0.009020635517241379
step: 3844, train_loss: 1.189133882522583, acc: 0.5009999871253967, val_loss: 1.1660643815994263, val_acc: 0.5044000148773193,  lr: 0.009020291034482758
step: 3845, train_loss: 1.1044379472732544, acc: 0.5325999855995178, val_loss: 1.152472734451294, val_acc: 0.5081999897956848,  lr: 0.009019946551724138
step: 3846, train_loss: 1.1042261123657227, acc: 0.5461999773979187, val_loss: 1.1596256494522095, val_acc: 0.5023999810218811,  lr: 0.009019602068965517
step: 3847, train_loss: 1.2006229162216187, acc: 0.4934000074863434, val_loss: 1.1584770679473877, val_acc: 0.5088000297546387,  lr: 0.009019257586206897
step: 3848, train_loss: 1.0599474906921387, acc: 0.550000011920929, val_loss: 1.1641113758087158, val_acc: 0.5112000107765198,  lr: 0.009018913103448276
step: 3849, train_loss: 1.1448420286178589, acc: 0.5194000005722046, val_loss: 1.1727794408798218, val_acc: 0.501800000667572,  lr: 0.009018568620689655
step: 3850, train_loss: 1.2307684421539307, acc: 0.47119998931884766, val_loss: 1.1572997570037842, val_acc: 0.5112000107765198,  lr: 0.009018224137931035
step: 3851, train_loss: 1.143694281578064, acc: 0.5216000080108643, val_loss: 1.149874210357666, val_acc: 0.510200023651123,  lr: 0.009017879655172414
step: 3852, train_loss: 1.1355630159378052, acc: 0.5289999842643738, val_loss: 1.1504606008529663, val_acc: 0.5103999972343445,  lr: 0.009017535172413793
step: 3853, train_loss: 1.2181816101074219, acc: 0.48399999737739563, val_loss: 1.1421235799789429, val_acc: 0.5162000060081482,  lr: 0.009017190689655173
step: 3854, train_loss: 1.1046357154846191, acc: 0.5386000275611877, val_loss: 1.1311042308807373, val_acc: 0.5180000066757202,  lr: 0.009016846206896552
step: 3855, train_loss: 1.1823536157608032, acc: 0.5040000081062317, val_loss: 1.1265751123428345, val_acc: 0.5224000215530396,  lr: 0.009016501724137932
step: 3856, train_loss: 1.1647390127182007, acc: 0.5139999985694885, val_loss: 1.1347345113754272, val_acc: 0.5202000141143799,  lr: 0.009016157241379311
step: 3857, train_loss: 1.2171189785003662, acc: 0.4812000095844269, val_loss: 1.1292146444320679, val_acc: 0.526199996471405,  lr: 0.00901581275862069
step: 3858, train_loss: 1.2305946350097656, acc: 0.4747999906539917, val_loss: 1.1243103742599487, val_acc: 0.5324000120162964,  lr: 0.00901546827586207
step: 3859, train_loss: 1.1158727407455444, acc: 0.5271999835968018, val_loss: 1.1155741214752197, val_acc: 0.5317999720573425,  lr: 0.009015123793103449
step: 3860, train_loss: 1.0743443965911865, acc: 0.5491999983787537, val_loss: 1.1150224208831787, val_acc: 0.5281999707221985,  lr: 0.009014779310344827
step: 3861, train_loss: 1.1645748615264893, acc: 0.5055999755859375, val_loss: 1.11750066280365, val_acc: 0.532800018787384,  lr: 0.009014434827586206
step: 3862, train_loss: 1.1015342473983765, acc: 0.5311999917030334, val_loss: 1.1162992715835571, val_acc: 0.5357999801635742,  lr: 0.009014090344827586
step: 3863, train_loss: 1.0883119106292725, acc: 0.5461999773979187, val_loss: 1.1220821142196655, val_acc: 0.5285999774932861,  lr: 0.009013745862068965
step: 3864, train_loss: 1.1216119527816772, acc: 0.5281999707221985, val_loss: 1.1224665641784668, val_acc: 0.5275999903678894,  lr: 0.009013401379310344
step: 3865, train_loss: 1.1530966758728027, acc: 0.51419997215271, val_loss: 1.1218324899673462, val_acc: 0.5293999910354614,  lr: 0.009013056896551724
step: 3866, train_loss: 1.085895299911499, acc: 0.5419999957084656, val_loss: 1.1237478256225586, val_acc: 0.5317999720573425,  lr: 0.009012712413793103
step: 3867, train_loss: 1.1681894063949585, acc: 0.5144000053405762, val_loss: 1.1254476308822632, val_acc: 0.5289999842643738,  lr: 0.009012367931034482
step: 3868, train_loss: 1.0535211563110352, acc: 0.5694000124931335, val_loss: 1.1240599155426025, val_acc: 0.5288000106811523,  lr: 0.009012023448275862
step: 3869, train_loss: 1.1208839416503906, acc: 0.5343999862670898, val_loss: 1.1226853132247925, val_acc: 0.5293999910354614,  lr: 0.009011678965517241
step: 3870, train_loss: 1.0865709781646729, acc: 0.5432000160217285, val_loss: 1.1274741888046265, val_acc: 0.5278000235557556,  lr: 0.00901133448275862
step: 3871, train_loss: 1.1382421255111694, acc: 0.5239999890327454, val_loss: 1.1310995817184448, val_acc: 0.527400016784668,  lr: 0.00901099
step: 3872, train_loss: 1.1083829402923584, acc: 0.5335999727249146, val_loss: 1.132177710533142, val_acc: 0.5239999890327454,  lr: 0.00901064551724138
step: 3873, train_loss: 1.1075669527053833, acc: 0.5246000289916992, val_loss: 1.1347966194152832, val_acc: 0.5242000222206116,  lr: 0.009010301034482759
step: 3874, train_loss: 1.0996015071868896, acc: 0.5335999727249146, val_loss: 1.1350423097610474, val_acc: 0.5293999910354614,  lr: 0.009009956551724138
step: 3875, train_loss: 1.1372289657592773, acc: 0.5275999903678894, val_loss: 1.141188144683838, val_acc: 0.5264000296592712,  lr: 0.009009612068965518
step: 3876, train_loss: 1.063995599746704, acc: 0.5493999719619751, val_loss: 1.1428031921386719, val_acc: 0.5264000296592712,  lr: 0.009009267586206897
step: 3877, train_loss: 1.0641409158706665, acc: 0.5461999773979187, val_loss: 1.1440953016281128, val_acc: 0.5235999822616577,  lr: 0.009008923103448276
step: 3878, train_loss: 1.1279804706573486, acc: 0.5306000113487244, val_loss: 1.1526902914047241, val_acc: 0.515999972820282,  lr: 0.009008578620689656
step: 3879, train_loss: 1.0601056814193726, acc: 0.5577999949455261, val_loss: 1.1397790908813477, val_acc: 0.5185999870300293,  lr: 0.009008234137931035
step: 3880, train_loss: 1.0460401773452759, acc: 0.5576000213623047, val_loss: 1.1470590829849243, val_acc: 0.5153999924659729,  lr: 0.009007889655172414
step: 3881, train_loss: 1.1332350969314575, acc: 0.5270000100135803, val_loss: 1.1403398513793945, val_acc: 0.520799994468689,  lr: 0.009007545172413794
step: 3882, train_loss: 1.0628352165222168, acc: 0.5432000160217285, val_loss: 1.159287691116333, val_acc: 0.5189999938011169,  lr: 0.009007200689655173
step: 3883, train_loss: 1.2137060165405273, acc: 0.48840001225471497, val_loss: 1.1551876068115234, val_acc: 0.5156000256538391,  lr: 0.009006856206896553
step: 3884, train_loss: 1.227543592453003, acc: 0.48899999260902405, val_loss: 1.1612834930419922, val_acc: 0.5112000107765198,  lr: 0.009006511724137932
step: 3885, train_loss: 1.1017950773239136, acc: 0.5351999998092651, val_loss: 1.1683104038238525, val_acc: 0.5091999769210815,  lr: 0.009006167241379311
step: 3886, train_loss: 1.1047331094741821, acc: 0.5335999727249146, val_loss: 1.1636725664138794, val_acc: 0.5095999836921692,  lr: 0.00900582275862069
step: 3887, train_loss: 1.1546777486801147, acc: 0.5166000127792358, val_loss: 1.1596617698669434, val_acc: 0.5063999891281128,  lr: 0.00900547827586207
step: 3888, train_loss: 1.2301026582717896, acc: 0.4837999939918518, val_loss: 1.1605623960494995, val_acc: 0.5041999816894531,  lr: 0.009005133793103448
step: 3889, train_loss: 1.1130324602127075, acc: 0.5396000146865845, val_loss: 1.1510839462280273, val_acc: 0.5127999782562256,  lr: 0.009004789310344827
step: 3890, train_loss: 1.1523501873016357, acc: 0.5139999985694885, val_loss: 1.153934359550476, val_acc: 0.5109999775886536,  lr: 0.009004444827586206
step: 3891, train_loss: 1.206779956817627, acc: 0.4966000020503998, val_loss: 1.1496552228927612, val_acc: 0.5167999863624573,  lr: 0.009004100344827586
step: 3892, train_loss: 1.1941211223602295, acc: 0.4943999946117401, val_loss: 1.144454836845398, val_acc: 0.5267999768257141,  lr: 0.009003755862068965
step: 3893, train_loss: 1.1954892873764038, acc: 0.4912000000476837, val_loss: 1.1450932025909424, val_acc: 0.5180000066757202,  lr: 0.009003411379310345
step: 3894, train_loss: 1.1082205772399902, acc: 0.5424000024795532, val_loss: 1.1392711400985718, val_acc: 0.5212000012397766,  lr: 0.009003066896551724
step: 3895, train_loss: 1.1335201263427734, acc: 0.5271999835968018, val_loss: 1.1439368724822998, val_acc: 0.5199999809265137,  lr: 0.009002722413793103
step: 3896, train_loss: 1.1139178276062012, acc: 0.5324000120162964, val_loss: 1.1512993574142456, val_acc: 0.5157999992370605,  lr: 0.009002377931034483
step: 3897, train_loss: 1.2379939556121826, acc: 0.4708000123500824, val_loss: 1.1539167165756226, val_acc: 0.521399974822998,  lr: 0.009002033448275862
step: 3898, train_loss: 1.1123223304748535, acc: 0.5350000262260437, val_loss: 1.1495511531829834, val_acc: 0.5266000032424927,  lr: 0.009001688965517242
step: 3899, train_loss: 1.09978187084198, acc: 0.5375999808311462, val_loss: 1.1425429582595825, val_acc: 0.5231999754905701,  lr: 0.009001344482758621
step: 3900, train_loss: 1.1093647480010986, acc: 0.5415999889373779, val_loss: 1.1439093351364136, val_acc: 0.522599995136261,  lr: 0.009001
step: 3901, train_loss: 1.0683202743530273, acc: 0.5565999746322632, val_loss: 1.142859935760498, val_acc: 0.5189999938011169,  lr: 0.00900065551724138
step: 3902, train_loss: 1.0929057598114014, acc: 0.5379999876022339, val_loss: 1.1502976417541504, val_acc: 0.5210000276565552,  lr: 0.009000311034482759
step: 3903, train_loss: 1.0764000415802002, acc: 0.5526000261306763, val_loss: 1.1517435312271118, val_acc: 0.5221999883651733,  lr: 0.008999966551724138
step: 3904, train_loss: 1.0972747802734375, acc: 0.5479999780654907, val_loss: 1.1449992656707764, val_acc: 0.5199999809265137,  lr: 0.008999622068965518
step: 3905, train_loss: 1.1678003072738647, acc: 0.5088000297546387, val_loss: 1.1411137580871582, val_acc: 0.5248000025749207,  lr: 0.008999277586206897
step: 3906, train_loss: 1.0710121393203735, acc: 0.5544000267982483, val_loss: 1.140427589416504, val_acc: 0.5116000175476074,  lr: 0.008998933103448277
step: 3907, train_loss: 1.1775845289230347, acc: 0.5008000135421753, val_loss: 1.1446186304092407, val_acc: 0.5163999795913696,  lr: 0.008998588620689656
step: 3908, train_loss: 1.056237816810608, acc: 0.5595999956130981, val_loss: 1.1426979303359985, val_acc: 0.519599974155426,  lr: 0.008998244137931035
step: 3909, train_loss: 1.079537034034729, acc: 0.5497999787330627, val_loss: 1.1434134244918823, val_acc: 0.520799994468689,  lr: 0.008997899655172413
step: 3910, train_loss: 1.235153317451477, acc: 0.4745999872684479, val_loss: 1.1438428163528442, val_acc: 0.5174000263214111,  lr: 0.008997555172413792
step: 3911, train_loss: 1.1341943740844727, acc: 0.5192000269889832, val_loss: 1.1480375528335571, val_acc: 0.5206000208854675,  lr: 0.008997210689655172
step: 3912, train_loss: 1.1664378643035889, acc: 0.5019999742507935, val_loss: 1.15104079246521, val_acc: 0.5220000147819519,  lr: 0.008996866206896551
step: 3913, train_loss: 1.1083905696868896, acc: 0.5324000120162964, val_loss: 1.1518348455429077, val_acc: 0.51419997215271,  lr: 0.00899652172413793
step: 3914, train_loss: 1.1588244438171387, acc: 0.5116000175476074, val_loss: 1.1480249166488647, val_acc: 0.5224000215530396,  lr: 0.00899617724137931
step: 3915, train_loss: 1.1122246980667114, acc: 0.5293999910354614, val_loss: 1.1537216901779175, val_acc: 0.521399974822998,  lr: 0.00899583275862069
step: 3916, train_loss: 1.1002581119537354, acc: 0.5379999876022339, val_loss: 1.1549205780029297, val_acc: 0.5175999999046326,  lr: 0.008995488275862069
step: 3917, train_loss: 1.089971899986267, acc: 0.5475999712944031, val_loss: 1.147690773010254, val_acc: 0.520799994468689,  lr: 0.008995143793103448
step: 3918, train_loss: 1.1139603853225708, acc: 0.5371999740600586, val_loss: 1.1467173099517822, val_acc: 0.5166000127792358,  lr: 0.008994799310344827
step: 3919, train_loss: 1.173414945602417, acc: 0.5054000020027161, val_loss: 1.1533825397491455, val_acc: 0.5149999856948853,  lr: 0.008994454827586207
step: 3920, train_loss: 1.216624140739441, acc: 0.4943999946117401, val_loss: 1.159863829612732, val_acc: 0.5123999714851379,  lr: 0.008994110344827586
step: 3921, train_loss: 1.0827981233596802, acc: 0.5404000282287598, val_loss: 1.1601592302322388, val_acc: 0.5144000053405762,  lr: 0.008993765862068966
step: 3922, train_loss: 1.2265067100524902, acc: 0.48559999465942383, val_loss: 1.165981650352478, val_acc: 0.5134000182151794,  lr: 0.008993421379310345
step: 3923, train_loss: 1.2085801362991333, acc: 0.4880000054836273, val_loss: 1.1688735485076904, val_acc: 0.5085999965667725,  lr: 0.008993076896551724
step: 3924, train_loss: 1.1108412742614746, acc: 0.5335999727249146, val_loss: 1.1635777950286865, val_acc: 0.5188000202178955,  lr: 0.008992732413793104
step: 3925, train_loss: 1.0643713474273682, acc: 0.5573999881744385, val_loss: 1.1485759019851685, val_acc: 0.524399995803833,  lr: 0.008992387931034483
step: 3926, train_loss: 1.049582600593567, acc: 0.5613999962806702, val_loss: 1.1467814445495605, val_acc: 0.5210000276565552,  lr: 0.008992043448275863
step: 3927, train_loss: 1.0774565935134888, acc: 0.555400013923645, val_loss: 1.1424721479415894, val_acc: 0.5202000141143799,  lr: 0.008991698965517242
step: 3928, train_loss: 1.139887809753418, acc: 0.524399995803833, val_loss: 1.1555269956588745, val_acc: 0.5167999863624573,  lr: 0.008991354482758621
step: 3929, train_loss: 1.0699596405029297, acc: 0.551800012588501, val_loss: 1.1531747579574585, val_acc: 0.5198000073432922,  lr: 0.00899101
step: 3930, train_loss: 1.1495493650436401, acc: 0.5121999979019165, val_loss: 1.1565430164337158, val_acc: 0.5180000066757202,  lr: 0.00899066551724138
step: 3931, train_loss: 1.159935474395752, acc: 0.5220000147819519, val_loss: 1.1533429622650146, val_acc: 0.5095999836921692,  lr: 0.00899032103448276
step: 3932, train_loss: 1.119823694229126, acc: 0.5321999788284302, val_loss: 1.1574878692626953, val_acc: 0.5055999755859375,  lr: 0.008989976551724139
step: 3933, train_loss: 1.16361665725708, acc: 0.5117999911308289, val_loss: 1.1598469018936157, val_acc: 0.5127999782562256,  lr: 0.008989632068965518
step: 3934, train_loss: 1.0587422847747803, acc: 0.5622000098228455, val_loss: 1.1607877016067505, val_acc: 0.5145999789237976,  lr: 0.008989287586206898
step: 3935, train_loss: 1.2237169742584229, acc: 0.4878000020980835, val_loss: 1.1574941873550415, val_acc: 0.5157999992370605,  lr: 0.008988943103448277
step: 3936, train_loss: 1.0916317701339722, acc: 0.5446000099182129, val_loss: 1.159242033958435, val_acc: 0.5152000188827515,  lr: 0.008988598620689655
step: 3937, train_loss: 1.1332467794418335, acc: 0.5297999978065491, val_loss: 1.1555830240249634, val_acc: 0.5095999836921692,  lr: 0.008988254137931034
step: 3938, train_loss: 1.197404146194458, acc: 0.49059998989105225, val_loss: 1.1620194911956787, val_acc: 0.5027999877929688,  lr: 0.008987909655172413
step: 3939, train_loss: 1.1790833473205566, acc: 0.504800021648407, val_loss: 1.1505836248397827, val_acc: 0.5192000269889832,  lr: 0.008987565172413793
step: 3940, train_loss: 1.154663324356079, acc: 0.5095999836921692, val_loss: 1.1474969387054443, val_acc: 0.519599974155426,  lr: 0.008987220689655172
step: 3941, train_loss: 1.1204687356948853, acc: 0.522599995136261, val_loss: 1.1459306478500366, val_acc: 0.519599974155426,  lr: 0.008986876206896552
step: 3942, train_loss: 1.0816924571990967, acc: 0.5386000275611877, val_loss: 1.1506071090698242, val_acc: 0.5131999850273132,  lr: 0.008986531724137931
step: 3943, train_loss: 1.2017403841018677, acc: 0.4848000109195709, val_loss: 1.1352685689926147, val_acc: 0.5242000222206116,  lr: 0.00898618724137931
step: 3944, train_loss: 1.1409482955932617, acc: 0.5275999903678894, val_loss: 1.1423217058181763, val_acc: 0.5189999938011169,  lr: 0.00898584275862069
step: 3945, train_loss: 1.1656768321990967, acc: 0.5063999891281128, val_loss: 1.1421401500701904, val_acc: 0.5135999917984009,  lr: 0.008985498275862069
step: 3946, train_loss: 1.1326652765274048, acc: 0.5216000080108643, val_loss: 1.1366864442825317, val_acc: 0.5180000066757202,  lr: 0.008985153793103448
step: 3947, train_loss: 1.1138054132461548, acc: 0.5257999897003174, val_loss: 1.139195203781128, val_acc: 0.5189999938011169,  lr: 0.008984809310344828
step: 3948, train_loss: 1.1631152629852295, acc: 0.5091999769210815, val_loss: 1.142106294631958, val_acc: 0.5170000195503235,  lr: 0.008984464827586207
step: 3949, train_loss: 1.2069326639175415, acc: 0.4878000020980835, val_loss: 1.1387321949005127, val_acc: 0.5170000195503235,  lr: 0.008984120344827587
step: 3950, train_loss: 1.1322245597839355, acc: 0.5189999938011169, val_loss: 1.141098141670227, val_acc: 0.5192000269889832,  lr: 0.008983775862068966
step: 3951, train_loss: 1.1303173303604126, acc: 0.5203999876976013, val_loss: 1.1393486261367798, val_acc: 0.5210000276565552,  lr: 0.008983431379310345
step: 3952, train_loss: 1.078597903251648, acc: 0.5544000267982483, val_loss: 1.1354256868362427, val_acc: 0.5188000202178955,  lr: 0.008983086896551725
step: 3953, train_loss: 1.116945743560791, acc: 0.5335999727249146, val_loss: 1.1344001293182373, val_acc: 0.521399974822998,  lr: 0.008982742413793104
step: 3954, train_loss: 1.0782883167266846, acc: 0.5424000024795532, val_loss: 1.1370341777801514, val_acc: 0.522599995136261,  lr: 0.008982397931034483
step: 3955, train_loss: 1.0810210704803467, acc: 0.550000011920929, val_loss: 1.138655185699463, val_acc: 0.5242000222206116,  lr: 0.008982053448275863
step: 3956, train_loss: 1.1213037967681885, acc: 0.5293999910354614, val_loss: 1.1445916891098022, val_acc: 0.5206000208854675,  lr: 0.008981708965517242
step: 3957, train_loss: 1.0598490238189697, acc: 0.5514000058174133, val_loss: 1.1446338891983032, val_acc: 0.5203999876976013,  lr: 0.00898136448275862
step: 3958, train_loss: 1.0724098682403564, acc: 0.5491999983787537, val_loss: 1.1468381881713867, val_acc: 0.5156000256538391,  lr: 0.00898102
step: 3959, train_loss: 1.0783237218856812, acc: 0.5432000160217285, val_loss: 1.1492176055908203, val_acc: 0.5152000188827515,  lr: 0.008980675517241379
step: 3960, train_loss: 1.1451640129089355, acc: 0.5148000121116638, val_loss: 1.169607162475586, val_acc: 0.5052000284194946,  lr: 0.008980331034482758
step: 3961, train_loss: 1.1311979293823242, acc: 0.52920001745224, val_loss: 1.1555101871490479, val_acc: 0.5144000053405762,  lr: 0.008979986551724137
step: 3962, train_loss: 1.0641798973083496, acc: 0.5562000274658203, val_loss: 1.147630214691162, val_acc: 0.5095999836921692,  lr: 0.008979642068965517
step: 3963, train_loss: 1.1196837425231934, acc: 0.5315999984741211, val_loss: 1.1542620658874512, val_acc: 0.5063999891281128,  lr: 0.008979297586206896
step: 3964, train_loss: 1.0576539039611816, acc: 0.5559999942779541, val_loss: 1.1541119813919067, val_acc: 0.5120000243186951,  lr: 0.008978953103448276
step: 3965, train_loss: 1.1228210926055908, acc: 0.5302000045776367, val_loss: 1.161023497581482, val_acc: 0.5117999911308289,  lr: 0.008978608620689655
step: 3966, train_loss: 1.1904348134994507, acc: 0.49399998784065247, val_loss: 1.1617554426193237, val_acc: 0.5112000107765198,  lr: 0.008978264137931034
step: 3967, train_loss: 1.1178617477416992, acc: 0.526199996471405, val_loss: 1.1564546823501587, val_acc: 0.5138000249862671,  lr: 0.008977919655172414
step: 3968, train_loss: 1.1252700090408325, acc: 0.5288000106811523, val_loss: 1.1686763763427734, val_acc: 0.5037999749183655,  lr: 0.008977575172413793
step: 3969, train_loss: 1.113381266593933, acc: 0.5382000207901001, val_loss: 1.1592071056365967, val_acc: 0.5076000094413757,  lr: 0.008977230689655172
step: 3970, train_loss: 1.1431454420089722, acc: 0.5145999789237976, val_loss: 1.1529910564422607, val_acc: 0.5123999714851379,  lr: 0.008976886206896552
step: 3971, train_loss: 1.1559292078018188, acc: 0.51419997215271, val_loss: 1.1516600847244263, val_acc: 0.5091999769210815,  lr: 0.008976541724137931
step: 3972, train_loss: 1.0837197303771973, acc: 0.5440000295639038, val_loss: 1.1508313417434692, val_acc: 0.5180000066757202,  lr: 0.00897619724137931
step: 3973, train_loss: 1.1682406663894653, acc: 0.5085999965667725, val_loss: 1.150268316268921, val_acc: 0.5157999992370605,  lr: 0.00897585275862069
step: 3974, train_loss: 1.2001047134399414, acc: 0.4893999993801117, val_loss: 1.1566413640975952, val_acc: 0.5120000243186951,  lr: 0.00897550827586207
step: 3975, train_loss: 1.1228561401367188, acc: 0.5278000235557556, val_loss: 1.14806067943573, val_acc: 0.5135999917984009,  lr: 0.008975163793103449
step: 3976, train_loss: 1.18332839012146, acc: 0.508400022983551, val_loss: 1.1542484760284424, val_acc: 0.5120000243186951,  lr: 0.008974819310344828
step: 3977, train_loss: 1.2175354957580566, acc: 0.48539999127388, val_loss: 1.1405682563781738, val_acc: 0.5216000080108643,  lr: 0.008974474827586208
step: 3978, train_loss: 1.0924479961395264, acc: 0.5324000120162964, val_loss: 1.1412711143493652, val_acc: 0.5220000147819519,  lr: 0.008974130344827587
step: 3979, train_loss: 1.1795045137405396, acc: 0.4966000020503998, val_loss: 1.13930082321167, val_acc: 0.5278000235557556,  lr: 0.008973785862068966
step: 3980, train_loss: 1.075406551361084, acc: 0.550599992275238, val_loss: 1.1313998699188232, val_acc: 0.5284000039100647,  lr: 0.008973441379310346
step: 3981, train_loss: 1.0665549039840698, acc: 0.5619999766349792, val_loss: 1.129943609237671, val_acc: 0.525600016117096,  lr: 0.008973096896551725
step: 3982, train_loss: 1.1013331413269043, acc: 0.5432000160217285, val_loss: 1.1270527839660645, val_acc: 0.5231999754905701,  lr: 0.008972752413793104
step: 3983, train_loss: 1.1186046600341797, acc: 0.5266000032424927, val_loss: 1.1326996088027954, val_acc: 0.5238000154495239,  lr: 0.008972407931034484
step: 3984, train_loss: 1.166764259338379, acc: 0.5049999952316284, val_loss: 1.1410243511199951, val_acc: 0.5234000086784363,  lr: 0.008972063448275861
step: 3985, train_loss: 1.094328761100769, acc: 0.5325999855995178, val_loss: 1.1391938924789429, val_acc: 0.519599974155426,  lr: 0.00897171896551724
step: 3986, train_loss: 1.2068573236465454, acc: 0.5026000142097473, val_loss: 1.1381207704544067, val_acc: 0.5181999802589417,  lr: 0.00897137448275862
step: 3987, train_loss: 1.135727047920227, acc: 0.5242000222206116, val_loss: 1.1372361183166504, val_acc: 0.5220000147819519,  lr: 0.00897103
step: 3988, train_loss: 1.1164478063583374, acc: 0.5360000133514404, val_loss: 1.1411911249160767, val_acc: 0.5203999876976013,  lr: 0.008970685517241379
step: 3989, train_loss: 1.1404165029525757, acc: 0.5267999768257141, val_loss: 1.1417497396469116, val_acc: 0.5194000005722046,  lr: 0.008970341034482758
step: 3990, train_loss: 1.2161414623260498, acc: 0.48539999127388, val_loss: 1.1420114040374756, val_acc: 0.5270000100135803,  lr: 0.008969996551724138
step: 3991, train_loss: 1.1120187044143677, acc: 0.5235999822616577, val_loss: 1.1340181827545166, val_acc: 0.5302000045776367,  lr: 0.008969652068965517
step: 3992, train_loss: 1.099377989768982, acc: 0.5368000268936157, val_loss: 1.1359525918960571, val_acc: 0.5264000296592712,  lr: 0.008969307586206897
step: 3993, train_loss: 1.0973981618881226, acc: 0.5475999712944031, val_loss: 1.1348532438278198, val_acc: 0.5266000032424927,  lr: 0.008968963103448276
step: 3994, train_loss: 1.1708568334579468, acc: 0.515999972820282, val_loss: 1.1372530460357666, val_acc: 0.5217999815940857,  lr: 0.008968618620689655
step: 3995, train_loss: 1.1386804580688477, acc: 0.5194000005722046, val_loss: 1.1460901498794556, val_acc: 0.515999972820282,  lr: 0.008968274137931035
step: 3996, train_loss: 1.0879987478256226, acc: 0.5496000051498413, val_loss: 1.1402899026870728, val_acc: 0.5202000141143799,  lr: 0.008967929655172414
step: 3997, train_loss: 1.098270058631897, acc: 0.5364000201225281, val_loss: 1.1385161876678467, val_acc: 0.5224000215530396,  lr: 0.008967585172413793
step: 3998, train_loss: 1.126700520515442, acc: 0.5239999890327454, val_loss: 1.136259913444519, val_acc: 0.5224000215530396,  lr: 0.008967240689655173
step: 3999, train_loss: 1.086788296699524, acc: 0.5483999848365784, val_loss: 1.136719822883606, val_acc: 0.5206000208854675,  lr: 0.008966896206896552
step: 4000, train_loss: 1.1144734621047974, acc: 0.5284000039100647, val_loss: 1.1389416456222534, val_acc: 0.5212000012397766,  lr: 0.008966551724137932
step: 4001, train_loss: 1.1639045476913452, acc: 0.5059999823570251, val_loss: 1.1443896293640137, val_acc: 0.5180000066757202,  lr: 0.008966207241379311
step: 4002, train_loss: 1.078549861907959, acc: 0.5468000173568726, val_loss: 1.1486479043960571, val_acc: 0.5210000276565552,  lr: 0.00896586275862069
step: 4003, train_loss: 1.1518458127975464, acc: 0.5221999883651733, val_loss: 1.1521642208099365, val_acc: 0.5194000005722046,  lr: 0.00896551827586207
step: 4004, train_loss: 1.0816103219985962, acc: 0.5404000282287598, val_loss: 1.1496076583862305, val_acc: 0.517799973487854,  lr: 0.008965173793103449
step: 4005, train_loss: 1.0717790126800537, acc: 0.5551999807357788, val_loss: 1.1553725004196167, val_acc: 0.5117999911308289,  lr: 0.008964829310344827
step: 4006, train_loss: 1.0609256029129028, acc: 0.5546000003814697, val_loss: 1.152834177017212, val_acc: 0.5072000026702881,  lr: 0.008964484827586206
step: 4007, train_loss: 1.1440869569778442, acc: 0.5188000202178955, val_loss: 1.1658248901367188, val_acc: 0.5040000081062317,  lr: 0.008964140344827586
step: 4008, train_loss: 1.069350242614746, acc: 0.5472000241279602, val_loss: 1.1518486738204956, val_acc: 0.5108000040054321,  lr: 0.008963795862068965
step: 4009, train_loss: 1.1087405681610107, acc: 0.5335999727249146, val_loss: 1.1571862697601318, val_acc: 0.5094000101089478,  lr: 0.008963451379310344
step: 4010, train_loss: 1.087327480316162, acc: 0.5442000031471252, val_loss: 1.1536519527435303, val_acc: 0.5120000243186951,  lr: 0.008963106896551724
step: 4011, train_loss: 1.1028447151184082, acc: 0.5425999760627747, val_loss: 1.157856822013855, val_acc: 0.5127999782562256,  lr: 0.008962762413793103
step: 4012, train_loss: 1.0796641111373901, acc: 0.5443999767303467, val_loss: 1.164729118347168, val_acc: 0.5088000297546387,  lr: 0.008962417931034482
step: 4013, train_loss: 1.0596561431884766, acc: 0.557200014591217, val_loss: 1.1705032587051392, val_acc: 0.5076000094413757,  lr: 0.008962073448275862
step: 4014, train_loss: 1.1100566387176514, acc: 0.5288000106811523, val_loss: 1.1662259101867676, val_acc: 0.5081999897956848,  lr: 0.008961728965517241
step: 4015, train_loss: 1.1743289232254028, acc: 0.5037999749183655, val_loss: 1.1553828716278076, val_acc: 0.5127999782562256,  lr: 0.00896138448275862
step: 4016, train_loss: 1.1365885734558105, acc: 0.5210000276565552, val_loss: 1.1634421348571777, val_acc: 0.5054000020027161,  lr: 0.00896104
step: 4017, train_loss: 1.1764401197433472, acc: 0.5034000277519226, val_loss: 1.160692811012268, val_acc: 0.5059999823570251,  lr: 0.00896069551724138
step: 4018, train_loss: 1.0856155157089233, acc: 0.5442000031471252, val_loss: 1.158875823020935, val_acc: 0.5090000033378601,  lr: 0.008960351034482759
step: 4019, train_loss: 1.118451476097107, acc: 0.5314000248908997, val_loss: 1.16096830368042, val_acc: 0.5085999965667725,  lr: 0.008960006551724138
step: 4020, train_loss: 1.1313612461090088, acc: 0.5302000045776367, val_loss: 1.1640058755874634, val_acc: 0.5134000182151794,  lr: 0.008959662068965518
step: 4021, train_loss: 1.056779146194458, acc: 0.5587999820709229, val_loss: 1.174864411354065, val_acc: 0.5031999945640564,  lr: 0.008959317586206897
step: 4022, train_loss: 1.1516172885894775, acc: 0.5120000243186951, val_loss: 1.1697787046432495, val_acc: 0.5023999810218811,  lr: 0.008958973103448276
step: 4023, train_loss: 1.0893419981002808, acc: 0.54339998960495, val_loss: 1.1706913709640503, val_acc: 0.5013999938964844,  lr: 0.008958628620689656
step: 4024, train_loss: 1.0830168724060059, acc: 0.5361999869346619, val_loss: 1.1722962856292725, val_acc: 0.5080000162124634,  lr: 0.008958284137931035
step: 4025, train_loss: 1.2162526845932007, acc: 0.48420000076293945, val_loss: 1.1863981485366821, val_acc: 0.49939998984336853,  lr: 0.008957939655172414
step: 4026, train_loss: 1.1416375637054443, acc: 0.5203999876976013, val_loss: 1.185257077217102, val_acc: 0.4973999857902527,  lr: 0.008957595172413794
step: 4027, train_loss: 1.0964009761810303, acc: 0.5383999943733215, val_loss: 1.1821043491363525, val_acc: 0.4986000061035156,  lr: 0.008957250689655173
step: 4028, train_loss: 1.1736129522323608, acc: 0.5058000087738037, val_loss: 1.1688973903656006, val_acc: 0.508400022983551,  lr: 0.008956906206896553
step: 4029, train_loss: 1.073822021484375, acc: 0.5558000206947327, val_loss: 1.176903486251831, val_acc: 0.49480000138282776,  lr: 0.008956561724137932
step: 4030, train_loss: 1.1235780715942383, acc: 0.5383999943733215, val_loss: 1.1733914613723755, val_acc: 0.4952000081539154,  lr: 0.008956217241379311
step: 4031, train_loss: 1.0855369567871094, acc: 0.5472000241279602, val_loss: 1.1804845333099365, val_acc: 0.49140000343322754,  lr: 0.00895587275862069
step: 4032, train_loss: 1.1703550815582275, acc: 0.5044000148773193, val_loss: 1.1759644746780396, val_acc: 0.49619999527931213,  lr: 0.00895552827586207
step: 4033, train_loss: 1.1109883785247803, acc: 0.5321999788284302, val_loss: 1.1784776449203491, val_acc: 0.49779999256134033,  lr: 0.008955183793103448
step: 4034, train_loss: 1.200991153717041, acc: 0.48820000886917114, val_loss: 1.1893268823623657, val_acc: 0.49239999055862427,  lr: 0.008954839310344827
step: 4035, train_loss: 1.061862587928772, acc: 0.5460000038146973, val_loss: 1.1875990629196167, val_acc: 0.49540001153945923,  lr: 0.008954494827586207
step: 4036, train_loss: 1.14389169216156, acc: 0.5260000228881836, val_loss: 1.187314510345459, val_acc: 0.49720001220703125,  lr: 0.008954150344827586
step: 4037, train_loss: 1.117173671722412, acc: 0.531000018119812, val_loss: 1.185670256614685, val_acc: 0.5004000067710876,  lr: 0.008953805862068965
step: 4038, train_loss: 1.0541307926177979, acc: 0.5508000254631042, val_loss: 1.178688645362854, val_acc: 0.5,  lr: 0.008953461379310345
step: 4039, train_loss: 1.0623650550842285, acc: 0.5532000064849854, val_loss: 1.1915240287780762, val_acc: 0.4968000054359436,  lr: 0.008953116896551724
step: 4040, train_loss: 1.1085395812988281, acc: 0.5378000140190125, val_loss: 1.1813445091247559, val_acc: 0.503000020980835,  lr: 0.008952772413793103
step: 4041, train_loss: 1.1390882730484009, acc: 0.5260000228881836, val_loss: 1.177428960800171, val_acc: 0.503000020980835,  lr: 0.008952427931034483
step: 4042, train_loss: 1.0349113941192627, acc: 0.573199987411499, val_loss: 1.1768243312835693, val_acc: 0.5059999823570251,  lr: 0.008952083448275862
step: 4043, train_loss: 1.0822322368621826, acc: 0.5437999963760376, val_loss: 1.1687673330307007, val_acc: 0.5045999884605408,  lr: 0.008951738965517242
step: 4044, train_loss: 1.1236220598220825, acc: 0.5224000215530396, val_loss: 1.1713067293167114, val_acc: 0.5016000270843506,  lr: 0.008951394482758621
step: 4045, train_loss: 1.1816892623901367, acc: 0.5023999810218811, val_loss: 1.168668508529663, val_acc: 0.501800000667572,  lr: 0.00895105
step: 4046, train_loss: 1.1048600673675537, acc: 0.5353999733924866, val_loss: 1.1707143783569336, val_acc: 0.4975999891757965,  lr: 0.00895070551724138
step: 4047, train_loss: 1.0845084190368652, acc: 0.5443999767303467, val_loss: 1.1729129552841187, val_acc: 0.49799999594688416,  lr: 0.008950361034482759
step: 4048, train_loss: 1.061414361000061, acc: 0.5582000017166138, val_loss: 1.1667286157608032, val_acc: 0.5009999871253967,  lr: 0.008950016551724138
step: 4049, train_loss: 1.0712312459945679, acc: 0.5532000064849854, val_loss: 1.166812777519226, val_acc: 0.5063999891281128,  lr: 0.008949672068965518
step: 4050, train_loss: 1.1104258298873901, acc: 0.5303999781608582, val_loss: 1.1594563722610474, val_acc: 0.5121999979019165,  lr: 0.008949327586206897
step: 4051, train_loss: 1.0644419193267822, acc: 0.5558000206947327, val_loss: 1.1699211597442627, val_acc: 0.5040000081062317,  lr: 0.008948983103448277
step: 4052, train_loss: 1.200137734413147, acc: 0.4896000027656555, val_loss: 1.1632466316223145, val_acc: 0.5063999891281128,  lr: 0.008948638620689656
step: 4053, train_loss: 1.0856616497039795, acc: 0.5432000160217285, val_loss: 1.1564640998840332, val_acc: 0.5108000040054321,  lr: 0.008948294137931035
step: 4054, train_loss: 1.1279882192611694, acc: 0.5289999842643738, val_loss: 1.169511079788208, val_acc: 0.5027999877929688,  lr: 0.008947949655172413
step: 4055, train_loss: 1.0771609544754028, acc: 0.5493999719619751, val_loss: 1.1691656112670898, val_acc: 0.49900001287460327,  lr: 0.008947605172413792
step: 4056, train_loss: 1.0800515413284302, acc: 0.5565999746322632, val_loss: 1.1724638938903809, val_acc: 0.49720001220703125,  lr: 0.008947260689655172
step: 4057, train_loss: 1.1443535089492798, acc: 0.5134000182151794, val_loss: 1.1654410362243652, val_acc: 0.5063999891281128,  lr: 0.008946916206896551
step: 4058, train_loss: 1.2425436973571777, acc: 0.47920000553131104, val_loss: 1.1677318811416626, val_acc: 0.5041999816894531,  lr: 0.00894657172413793
step: 4059, train_loss: 1.132848858833313, acc: 0.520799994468689, val_loss: 1.162221908569336, val_acc: 0.503000020980835,  lr: 0.00894622724137931
step: 4060, train_loss: 1.080923318862915, acc: 0.5491999983787537, val_loss: 1.1591508388519287, val_acc: 0.5034000277519226,  lr: 0.00894588275862069
step: 4061, train_loss: 1.0788605213165283, acc: 0.5486000180244446, val_loss: 1.1625244617462158, val_acc: 0.5004000067710876,  lr: 0.008945538275862069
step: 4062, train_loss: 1.1812604665756226, acc: 0.4957999885082245, val_loss: 1.1483187675476074, val_acc: 0.5127999782562256,  lr: 0.008945193793103448
step: 4063, train_loss: 1.0695679187774658, acc: 0.5424000024795532, val_loss: 1.1461766958236694, val_acc: 0.5149999856948853,  lr: 0.008944849310344827
step: 4064, train_loss: 1.177871823310852, acc: 0.5009999871253967, val_loss: 1.1410865783691406, val_acc: 0.5152000188827515,  lr: 0.008944504827586207
step: 4065, train_loss: 1.2064018249511719, acc: 0.49639999866485596, val_loss: 1.1362658739089966, val_acc: 0.5166000127792358,  lr: 0.008944160344827586
step: 4066, train_loss: 1.0820547342300415, acc: 0.5447999835014343, val_loss: 1.1327009201049805, val_acc: 0.5194000005722046,  lr: 0.008943815862068966
step: 4067, train_loss: 1.1099861860275269, acc: 0.5320000052452087, val_loss: 1.1242257356643677, val_acc: 0.5253999829292297,  lr: 0.008943471379310345
step: 4068, train_loss: 1.193835973739624, acc: 0.5026000142097473, val_loss: 1.1232253313064575, val_acc: 0.5285999774932861,  lr: 0.008943126896551724
step: 4069, train_loss: 1.2382593154907227, acc: 0.475600004196167, val_loss: 1.1306700706481934, val_acc: 0.531000018119812,  lr: 0.008942782413793104
step: 4070, train_loss: 1.1844829320907593, acc: 0.5027999877929688, val_loss: 1.124208927154541, val_acc: 0.5347999930381775,  lr: 0.008942437931034483
step: 4071, train_loss: 1.2180522680282593, acc: 0.483599990606308, val_loss: 1.1233712434768677, val_acc: 0.5270000100135803,  lr: 0.008942093448275863
step: 4072, train_loss: 1.0908750295639038, acc: 0.5378000140190125, val_loss: 1.1172105073928833, val_acc: 0.531000018119812,  lr: 0.008941748965517242
step: 4073, train_loss: 1.0899600982666016, acc: 0.546999990940094, val_loss: 1.1147897243499756, val_acc: 0.5332000255584717,  lr: 0.008941404482758621
step: 4074, train_loss: 1.1854146718978882, acc: 0.49540001153945923, val_loss: 1.1172925233840942, val_acc: 0.5329999923706055,  lr: 0.00894106
step: 4075, train_loss: 1.2013994455337524, acc: 0.49140000343322754, val_loss: 1.1229362487792969, val_acc: 0.5296000242233276,  lr: 0.00894071551724138
step: 4076, train_loss: 1.2345044612884521, acc: 0.47859999537467957, val_loss: 1.1277403831481934, val_acc: 0.5339999794960022,  lr: 0.00894037103448276
step: 4077, train_loss: 1.0779896974563599, acc: 0.5496000051498413, val_loss: 1.1165295839309692, val_acc: 0.5392000079154968,  lr: 0.008940026551724139
step: 4078, train_loss: 1.1386288404464722, acc: 0.5194000005722046, val_loss: 1.1245529651641846, val_acc: 0.5325999855995178,  lr: 0.008939682068965518
step: 4079, train_loss: 1.1187598705291748, acc: 0.525600016117096, val_loss: 1.1276707649230957, val_acc: 0.5293999910354614,  lr: 0.008939337586206898
step: 4080, train_loss: 1.1720846891403198, acc: 0.5052000284194946, val_loss: 1.121334433555603, val_acc: 0.5325999855995178,  lr: 0.008938993103448277
step: 4081, train_loss: 1.1375638246536255, acc: 0.5221999883651733, val_loss: 1.1089140176773071, val_acc: 0.5418000221252441,  lr: 0.008938648620689655
step: 4082, train_loss: 1.1293715238571167, acc: 0.5267999768257141, val_loss: 1.1044273376464844, val_acc: 0.5393999814987183,  lr: 0.008938304137931034
step: 4083, train_loss: 1.1237753629684448, acc: 0.5335999727249146, val_loss: 1.1077240705490112, val_acc: 0.5397999882698059,  lr: 0.008937959655172413
step: 4084, train_loss: 1.0760382413864136, acc: 0.5537999868392944, val_loss: 1.102277398109436, val_acc: 0.5443999767303467,  lr: 0.008937615172413793
step: 4085, train_loss: 1.1606826782226562, acc: 0.5117999911308289, val_loss: 1.107924222946167, val_acc: 0.5321999788284302,  lr: 0.008937270689655172
step: 4086, train_loss: 1.1084192991256714, acc: 0.5320000052452087, val_loss: 1.109192132949829, val_acc: 0.532800018787384,  lr: 0.008936926206896552
step: 4087, train_loss: 1.1400634050369263, acc: 0.5194000005722046, val_loss: 1.1100518703460693, val_acc: 0.5389999747276306,  lr: 0.008936581724137931
step: 4088, train_loss: 1.0497323274612427, acc: 0.5577999949455261, val_loss: 1.1146085262298584, val_acc: 0.5379999876022339,  lr: 0.00893623724137931
step: 4089, train_loss: 1.050389051437378, acc: 0.5583999752998352, val_loss: 1.111283302307129, val_acc: 0.5396000146865845,  lr: 0.00893589275862069
step: 4090, train_loss: 1.0736408233642578, acc: 0.5569999814033508, val_loss: 1.1083112955093384, val_acc: 0.5371999740600586,  lr: 0.008935548275862069
step: 4091, train_loss: 1.1266900300979614, acc: 0.5306000113487244, val_loss: 1.1149961948394775, val_acc: 0.5307999849319458,  lr: 0.008935203793103448
step: 4092, train_loss: 1.062303900718689, acc: 0.5564000010490417, val_loss: 1.1174429655075073, val_acc: 0.5260000228881836,  lr: 0.008934859310344828
step: 4093, train_loss: 1.0603501796722412, acc: 0.5442000031471252, val_loss: 1.1152403354644775, val_acc: 0.5317999720573425,  lr: 0.008934514827586207
step: 4094, train_loss: 1.1238073110580444, acc: 0.5311999917030334, val_loss: 1.1223714351654053, val_acc: 0.5296000242233276,  lr: 0.008934170344827587
step: 4095, train_loss: 1.123815894126892, acc: 0.5260000228881836, val_loss: 1.1195957660675049, val_acc: 0.5332000255584717,  lr: 0.008933825862068966
step: 4096, train_loss: 1.1124862432479858, acc: 0.5353999733924866, val_loss: 1.115938663482666, val_acc: 0.5325999855995178,  lr: 0.008933481379310345
step: 4097, train_loss: 1.1804730892181396, acc: 0.5063999891281128, val_loss: 1.1275739669799805, val_acc: 0.5212000012397766,  lr: 0.008933136896551725
step: 4098, train_loss: 1.10869300365448, acc: 0.5321999788284302, val_loss: 1.1240988969802856, val_acc: 0.5275999903678894,  lr: 0.008932792413793104
step: 4099, train_loss: 1.1304751634597778, acc: 0.5257999897003174, val_loss: 1.1210918426513672, val_acc: 0.5302000045776367,  lr: 0.008932447931034483
step: 4100, train_loss: 1.106808066368103, acc: 0.5371999740600586, val_loss: 1.1337071657180786, val_acc: 0.5297999978065491,  lr: 0.008932103448275863
step: 4101, train_loss: 1.0338326692581177, acc: 0.576200008392334, val_loss: 1.1360359191894531, val_acc: 0.5299999713897705,  lr: 0.008931758965517242
step: 4102, train_loss: 1.1240501403808594, acc: 0.527400016784668, val_loss: 1.1299155950546265, val_acc: 0.5264000296592712,  lr: 0.00893141448275862
step: 4103, train_loss: 1.0229297876358032, acc: 0.5753999948501587, val_loss: 1.130531668663025, val_acc: 0.5202000141143799,  lr: 0.00893107
step: 4104, train_loss: 1.0801180601119995, acc: 0.5472000241279602, val_loss: 1.1361476182937622, val_acc: 0.5139999985694885,  lr: 0.008930725517241379
step: 4105, train_loss: 1.0809534788131714, acc: 0.5400000214576721, val_loss: 1.132780909538269, val_acc: 0.5171999931335449,  lr: 0.008930381034482758
step: 4106, train_loss: 1.0801093578338623, acc: 0.546999990940094, val_loss: 1.1309832334518433, val_acc: 0.5152000188827515,  lr: 0.008930036551724137
step: 4107, train_loss: 1.0569206476211548, acc: 0.550599992275238, val_loss: 1.1298632621765137, val_acc: 0.5192000269889832,  lr: 0.008929692068965517
step: 4108, train_loss: 1.084130883216858, acc: 0.5479999780654907, val_loss: 1.132170557975769, val_acc: 0.5185999870300293,  lr: 0.008929347586206896
step: 4109, train_loss: 1.0532958507537842, acc: 0.5573999881744385, val_loss: 1.1383848190307617, val_acc: 0.517799973487854,  lr: 0.008929003103448276
step: 4110, train_loss: 1.0486027002334595, acc: 0.5580000281333923, val_loss: 1.1427582502365112, val_acc: 0.5174000263214111,  lr: 0.008928658620689655
step: 4111, train_loss: 1.2345342636108398, acc: 0.4805999994277954, val_loss: 1.1486080884933472, val_acc: 0.5162000060081482,  lr: 0.008928314137931034
step: 4112, train_loss: 1.058692216873169, acc: 0.5509999990463257, val_loss: 1.1513936519622803, val_acc: 0.5145999789237976,  lr: 0.008927969655172414
step: 4113, train_loss: 1.1576694250106812, acc: 0.5116000175476074, val_loss: 1.1482956409454346, val_acc: 0.5139999985694885,  lr: 0.008927625172413793
step: 4114, train_loss: 1.09535551071167, acc: 0.5388000011444092, val_loss: 1.1565754413604736, val_acc: 0.5073999762535095,  lr: 0.008927280689655172
step: 4115, train_loss: 1.0665221214294434, acc: 0.5540000200271606, val_loss: 1.154563546180725, val_acc: 0.5135999917984009,  lr: 0.008926936206896552
step: 4116, train_loss: 1.1436415910720825, acc: 0.5296000242233276, val_loss: 1.1572439670562744, val_acc: 0.5099999904632568,  lr: 0.008926591724137931
step: 4117, train_loss: 1.0913958549499512, acc: 0.5442000031471252, val_loss: 1.1639436483383179, val_acc: 0.5031999945640564,  lr: 0.00892624724137931
step: 4118, train_loss: 1.0609151124954224, acc: 0.553600013256073, val_loss: 1.1654245853424072, val_acc: 0.503000020980835,  lr: 0.00892590275862069
step: 4119, train_loss: 1.0733531713485718, acc: 0.5532000064849854, val_loss: 1.1558345556259155, val_acc: 0.5145999789237976,  lr: 0.00892555827586207
step: 4120, train_loss: 1.1011360883712769, acc: 0.5307999849319458, val_loss: 1.1636853218078613, val_acc: 0.5109999775886536,  lr: 0.008925213793103449
step: 4121, train_loss: 1.1975135803222656, acc: 0.49459999799728394, val_loss: 1.162072777748108, val_acc: 0.5109999775886536,  lr: 0.008924869310344828
step: 4122, train_loss: 1.0734920501708984, acc: 0.551800012588501, val_loss: 1.1656897068023682, val_acc: 0.5044000148773193,  lr: 0.008924524827586208
step: 4123, train_loss: 1.180406928062439, acc: 0.5019999742507935, val_loss: 1.1574454307556152, val_acc: 0.5073999762535095,  lr: 0.008924180344827587
step: 4124, train_loss: 1.1382157802581787, acc: 0.5271999835968018, val_loss: 1.1541334390640259, val_acc: 0.5212000012397766,  lr: 0.008923835862068966
step: 4125, train_loss: 1.1292310953140259, acc: 0.5281999707221985, val_loss: 1.1446608304977417, val_acc: 0.5202000141143799,  lr: 0.008923491379310346
step: 4126, train_loss: 1.217708945274353, acc: 0.48240000009536743, val_loss: 1.1396136283874512, val_acc: 0.5235999822616577,  lr: 0.008923146896551725
step: 4127, train_loss: 1.1671595573425293, acc: 0.5149999856948853, val_loss: 1.144344687461853, val_acc: 0.521399974822998,  lr: 0.008922802413793104
step: 4128, train_loss: 1.036847710609436, acc: 0.5626000165939331, val_loss: 1.1401524543762207, val_acc: 0.5249999761581421,  lr: 0.008922457931034484
step: 4129, train_loss: 1.0847522020339966, acc: 0.5478000044822693, val_loss: 1.1445666551589966, val_acc: 0.5199999809265137,  lr: 0.008922113448275861
step: 4130, train_loss: 1.0799250602722168, acc: 0.5454000234603882, val_loss: 1.148945927619934, val_acc: 0.5112000107765198,  lr: 0.008921768965517241
step: 4131, train_loss: 1.130568504333496, acc: 0.5224000215530396, val_loss: 1.1410012245178223, val_acc: 0.5203999876976013,  lr: 0.00892142448275862
step: 4132, train_loss: 1.1006191968917847, acc: 0.5461999773979187, val_loss: 1.1417365074157715, val_acc: 0.5112000107765198,  lr: 0.00892108
step: 4133, train_loss: 1.068422555923462, acc: 0.5562000274658203, val_loss: 1.1445106267929077, val_acc: 0.5113999843597412,  lr: 0.008920735517241379
step: 4134, train_loss: 1.1020647287368774, acc: 0.5288000106811523, val_loss: 1.1418031454086304, val_acc: 0.510200023651123,  lr: 0.008920391034482758
step: 4135, train_loss: 1.0676045417785645, acc: 0.5612000226974487, val_loss: 1.1436078548431396, val_acc: 0.5098000168800354,  lr: 0.008920046551724138
step: 4136, train_loss: 1.0555415153503418, acc: 0.5569999814033508, val_loss: 1.1417492628097534, val_acc: 0.5117999911308289,  lr: 0.008919702068965517
step: 4137, train_loss: 1.1239784955978394, acc: 0.5198000073432922, val_loss: 1.14909827709198, val_acc: 0.5099999904632568,  lr: 0.008919357586206897
step: 4138, train_loss: 1.2040269374847412, acc: 0.4860000014305115, val_loss: 1.1421397924423218, val_acc: 0.51419997215271,  lr: 0.008919013103448276
step: 4139, train_loss: 1.057944655418396, acc: 0.5576000213623047, val_loss: 1.1341955661773682, val_acc: 0.5156000256538391,  lr: 0.008918668620689655
step: 4140, train_loss: 1.0965903997421265, acc: 0.5415999889373779, val_loss: 1.1354808807373047, val_acc: 0.5288000106811523,  lr: 0.008918324137931035
step: 4141, train_loss: 1.1713393926620483, acc: 0.5085999965667725, val_loss: 1.1403305530548096, val_acc: 0.5220000147819519,  lr: 0.008917979655172414
step: 4142, train_loss: 1.0883715152740479, acc: 0.5428000092506409, val_loss: 1.1416435241699219, val_acc: 0.5171999931335449,  lr: 0.008917635172413793
step: 4143, train_loss: 1.0597164630889893, acc: 0.5509999990463257, val_loss: 1.1352617740631104, val_acc: 0.5189999938011169,  lr: 0.008917290689655173
step: 4144, train_loss: 1.0391924381256104, acc: 0.5685999989509583, val_loss: 1.1427819728851318, val_acc: 0.5180000066757202,  lr: 0.008916946206896552
step: 4145, train_loss: 1.1618117094039917, acc: 0.5108000040054321, val_loss: 1.1395800113677979, val_acc: 0.5199999809265137,  lr: 0.008916601724137932
step: 4146, train_loss: 1.0278971195220947, acc: 0.5630000233650208, val_loss: 1.1398992538452148, val_acc: 0.5162000060081482,  lr: 0.008916257241379311
step: 4147, train_loss: 1.109452486038208, acc: 0.5296000242233276, val_loss: 1.1483081579208374, val_acc: 0.5156000256538391,  lr: 0.00891591275862069
step: 4148, train_loss: 1.1363763809204102, acc: 0.5248000025749207, val_loss: 1.153388261795044, val_acc: 0.5145999789237976,  lr: 0.00891556827586207
step: 4149, train_loss: 1.222076177597046, acc: 0.48820000886917114, val_loss: 1.155657172203064, val_acc: 0.5192000269889832,  lr: 0.008915223793103449
step: 4150, train_loss: 1.1036648750305176, acc: 0.5317999720573425, val_loss: 1.1555389165878296, val_acc: 0.5134000182151794,  lr: 0.008914879310344827
step: 4151, train_loss: 1.1629258394241333, acc: 0.5088000297546387, val_loss: 1.1711379289627075, val_acc: 0.4986000061035156,  lr: 0.008914534827586206
step: 4152, train_loss: 1.2074130773544312, acc: 0.4991999864578247, val_loss: 1.1683233976364136, val_acc: 0.504800021648407,  lr: 0.008914190344827586
step: 4153, train_loss: 1.1066492795944214, acc: 0.5320000052452087, val_loss: 1.1613364219665527, val_acc: 0.5163999795913696,  lr: 0.008913845862068965
step: 4154, train_loss: 1.1652860641479492, acc: 0.5037999749183655, val_loss: 1.165959119796753, val_acc: 0.5199999809265137,  lr: 0.008913501379310344
step: 4155, train_loss: 1.1248446702957153, acc: 0.5289999842643738, val_loss: 1.1631559133529663, val_acc: 0.5162000060081482,  lr: 0.008913156896551724
step: 4156, train_loss: 1.1040635108947754, acc: 0.5379999876022339, val_loss: 1.1541633605957031, val_acc: 0.5212000012397766,  lr: 0.008912812413793103
step: 4157, train_loss: 1.0827542543411255, acc: 0.5440000295639038, val_loss: 1.1553468704223633, val_acc: 0.5080000162124634,  lr: 0.008912467931034482
step: 4158, train_loss: 1.0905710458755493, acc: 0.5415999889373779, val_loss: 1.147412896156311, val_acc: 0.5235999822616577,  lr: 0.008912123448275862
step: 4159, train_loss: 1.060131549835205, acc: 0.5544000267982483, val_loss: 1.1517318487167358, val_acc: 0.5181999802589417,  lr: 0.008911778965517241
step: 4160, train_loss: 1.1471037864685059, acc: 0.5194000005722046, val_loss: 1.1457401514053345, val_acc: 0.5167999863624573,  lr: 0.00891143448275862
step: 4161, train_loss: 1.0831289291381836, acc: 0.5482000112533569, val_loss: 1.1537301540374756, val_acc: 0.5134000182151794,  lr: 0.00891109
step: 4162, train_loss: 1.1865962743759155, acc: 0.5049999952316284, val_loss: 1.1550943851470947, val_acc: 0.5188000202178955,  lr: 0.00891074551724138
step: 4163, train_loss: 1.080852746963501, acc: 0.5410000085830688, val_loss: 1.1422476768493652, val_acc: 0.5198000073432922,  lr: 0.008910401034482759
step: 4164, train_loss: 1.2186534404754639, acc: 0.4830000102519989, val_loss: 1.137319564819336, val_acc: 0.522599995136261,  lr: 0.008910056551724138
step: 4165, train_loss: 1.1474878787994385, acc: 0.515999972820282, val_loss: 1.1330550909042358, val_acc: 0.5235999822616577,  lr: 0.008909712068965518
step: 4166, train_loss: 1.206201434135437, acc: 0.49459999799728394, val_loss: 1.1343214511871338, val_acc: 0.5253999829292297,  lr: 0.008909367586206897
step: 4167, train_loss: 1.1914045810699463, acc: 0.5, val_loss: 1.1322078704833984, val_acc: 0.5220000147819519,  lr: 0.008909023103448276
step: 4168, train_loss: 1.073871374130249, acc: 0.5551999807357788, val_loss: 1.128587007522583, val_acc: 0.5249999761581421,  lr: 0.008908678620689656
step: 4169, train_loss: 1.1236056089401245, acc: 0.5338000059127808, val_loss: 1.1324543952941895, val_acc: 0.532800018787384,  lr: 0.008908334137931035
step: 4170, train_loss: 1.1355228424072266, acc: 0.531000018119812, val_loss: 1.1237882375717163, val_acc: 0.532800018787384,  lr: 0.008907989655172414
step: 4171, train_loss: 1.1571377515792847, acc: 0.5121999979019165, val_loss: 1.1214594841003418, val_acc: 0.534600019454956,  lr: 0.008907645172413794
step: 4172, train_loss: 1.1275020837783813, acc: 0.5252000093460083, val_loss: 1.1183598041534424, val_acc: 0.5368000268936157,  lr: 0.008907300689655173
step: 4173, train_loss: 1.0450998544692993, acc: 0.5630000233650208, val_loss: 1.1220948696136475, val_acc: 0.5320000052452087,  lr: 0.008906956206896553
step: 4174, train_loss: 1.0807228088378906, acc: 0.5577999949455261, val_loss: 1.1207224130630493, val_acc: 0.5307999849319458,  lr: 0.008906611724137932
step: 4175, train_loss: 1.1077461242675781, acc: 0.5306000113487244, val_loss: 1.1197508573532104, val_acc: 0.534600019454956,  lr: 0.008906267241379311
step: 4176, train_loss: 1.0729146003723145, acc: 0.5514000058174133, val_loss: 1.1227905750274658, val_acc: 0.5302000045776367,  lr: 0.00890592275862069
step: 4177, train_loss: 1.1422368288040161, acc: 0.52920001745224, val_loss: 1.1215379238128662, val_acc: 0.5285999774932861,  lr: 0.008905578275862068
step: 4178, train_loss: 1.0501444339752197, acc: 0.5598000288009644, val_loss: 1.1217093467712402, val_acc: 0.5285999774932861,  lr: 0.008905233793103448
step: 4179, train_loss: 1.1967391967773438, acc: 0.4936000108718872, val_loss: 1.1119111776351929, val_acc: 0.5415999889373779,  lr: 0.008904889310344827
step: 4180, train_loss: 1.0491373538970947, acc: 0.5577999949455261, val_loss: 1.110511302947998, val_acc: 0.5357999801635742,  lr: 0.008904544827586207
step: 4181, train_loss: 1.0819813013076782, acc: 0.5479999780654907, val_loss: 1.1101751327514648, val_acc: 0.5317999720573425,  lr: 0.008904200344827586
step: 4182, train_loss: 1.043698787689209, acc: 0.5590000152587891, val_loss: 1.1072505712509155, val_acc: 0.5401999950408936,  lr: 0.008903855862068965
step: 4183, train_loss: 1.1410211324691772, acc: 0.5166000127792358, val_loss: 1.1164191961288452, val_acc: 0.5374000072479248,  lr: 0.008903511379310345
step: 4184, train_loss: 1.100450038909912, acc: 0.5379999876022339, val_loss: 1.1132551431655884, val_acc: 0.5374000072479248,  lr: 0.008903166896551724
step: 4185, train_loss: 1.1345434188842773, acc: 0.5198000073432922, val_loss: 1.115906000137329, val_acc: 0.5342000126838684,  lr: 0.008902822413793103
step: 4186, train_loss: 1.062927007675171, acc: 0.5569999814033508, val_loss: 1.1162751913070679, val_acc: 0.5339999794960022,  lr: 0.008902477931034483
step: 4187, train_loss: 1.2101047039031982, acc: 0.490200012922287, val_loss: 1.1199042797088623, val_acc: 0.5374000072479248,  lr: 0.008902133448275862
step: 4188, train_loss: 1.0890426635742188, acc: 0.5397999882698059, val_loss: 1.1217501163482666, val_acc: 0.5335999727249146,  lr: 0.008901788965517242
step: 4189, train_loss: 1.1321264505386353, acc: 0.5217999815940857, val_loss: 1.1325911283493042, val_acc: 0.5248000025749207,  lr: 0.008901444482758621
step: 4190, train_loss: 1.1164236068725586, acc: 0.5257999897003174, val_loss: 1.125929594039917, val_acc: 0.5302000045776367,  lr: 0.0089011
step: 4191, train_loss: 1.1049176454544067, acc: 0.5396000146865845, val_loss: 1.1273298263549805, val_acc: 0.5317999720573425,  lr: 0.00890075551724138
step: 4192, train_loss: 1.1648452281951904, acc: 0.5099999904632568, val_loss: 1.121225357055664, val_acc: 0.527400016784668,  lr: 0.008900411034482759
step: 4193, train_loss: 1.0909792184829712, acc: 0.5478000044822693, val_loss: 1.127449870109558, val_acc: 0.52920001745224,  lr: 0.008900066551724138
step: 4194, train_loss: 1.1327670812606812, acc: 0.5284000039100647, val_loss: 1.1309551000595093, val_acc: 0.5278000235557556,  lr: 0.008899722068965518
step: 4195, train_loss: 1.1918771266937256, acc: 0.501800000667572, val_loss: 1.1365790367126465, val_acc: 0.5202000141143799,  lr: 0.008899377586206897
step: 4196, train_loss: 1.109994649887085, acc: 0.5379999876022339, val_loss: 1.1326982975006104, val_acc: 0.527400016784668,  lr: 0.008899033103448277
step: 4197, train_loss: 1.1247669458389282, acc: 0.5357999801635742, val_loss: 1.1269395351409912, val_acc: 0.5296000242233276,  lr: 0.008898688620689656
step: 4198, train_loss: 1.1134306192398071, acc: 0.5325999855995178, val_loss: 1.1320003271102905, val_acc: 0.5278000235557556,  lr: 0.008898344137931035
step: 4199, train_loss: 1.1488385200500488, acc: 0.5184000134468079, val_loss: 1.141054391860962, val_acc: 0.5210000276565552,  lr: 0.008897999655172413
step: 4200, train_loss: 1.0705493688583374, acc: 0.5605999827384949, val_loss: 1.1333544254302979, val_acc: 0.5235999822616577,  lr: 0.008897655172413792
step: 4201, train_loss: 1.1268800497055054, acc: 0.522599995136261, val_loss: 1.133910059928894, val_acc: 0.5271999835968018,  lr: 0.008897310689655172
step: 4202, train_loss: 1.0543161630630493, acc: 0.5595999956130981, val_loss: 1.1369901895523071, val_acc: 0.5275999903678894,  lr: 0.008896966206896551
step: 4203, train_loss: 1.054192304611206, acc: 0.551800012588501, val_loss: 1.1367346048355103, val_acc: 0.5230000019073486,  lr: 0.00889662172413793
step: 4204, train_loss: 1.0837756395339966, acc: 0.5479999780654907, val_loss: 1.1418958902359009, val_acc: 0.5189999938011169,  lr: 0.00889627724137931
step: 4205, train_loss: 1.1721303462982178, acc: 0.5027999877929688, val_loss: 1.1543711423873901, val_acc: 0.5152000188827515,  lr: 0.00889593275862069
step: 4206, train_loss: 1.1021292209625244, acc: 0.5400000214576721, val_loss: 1.1480551958084106, val_acc: 0.522599995136261,  lr: 0.008895588275862069
step: 4207, train_loss: 1.1180585622787476, acc: 0.5324000120162964, val_loss: 1.150766134262085, val_acc: 0.5184000134468079,  lr: 0.008895243793103448
step: 4208, train_loss: 1.1024059057235718, acc: 0.5311999917030334, val_loss: 1.1522809267044067, val_acc: 0.5167999863624573,  lr: 0.008894899310344827
step: 4209, train_loss: 1.048567295074463, acc: 0.5612000226974487, val_loss: 1.15386164188385, val_acc: 0.5149999856948853,  lr: 0.008894554827586207
step: 4210, train_loss: 1.1109236478805542, acc: 0.5360000133514404, val_loss: 1.1506595611572266, val_acc: 0.5184000134468079,  lr: 0.008894210344827586
step: 4211, train_loss: 1.1075022220611572, acc: 0.5415999889373779, val_loss: 1.1482590436935425, val_acc: 0.521399974822998,  lr: 0.008893865862068966
step: 4212, train_loss: 1.1160184144973755, acc: 0.5311999917030334, val_loss: 1.1496152877807617, val_acc: 0.5163999795913696,  lr: 0.008893521379310345
step: 4213, train_loss: 1.0800849199295044, acc: 0.5546000003814697, val_loss: 1.1561152935028076, val_acc: 0.515999972820282,  lr: 0.008893176896551724
step: 4214, train_loss: 1.1242249011993408, acc: 0.527999997138977, val_loss: 1.1512489318847656, val_acc: 0.5180000066757202,  lr: 0.008892832413793104
step: 4215, train_loss: 1.0588667392730713, acc: 0.5595999956130981, val_loss: 1.1461467742919922, val_acc: 0.5171999931335449,  lr: 0.008892487931034483
step: 4216, train_loss: 1.0739444494247437, acc: 0.5501999855041504, val_loss: 1.1443777084350586, val_acc: 0.5156000256538391,  lr: 0.008892143448275863
step: 4217, train_loss: 1.094467043876648, acc: 0.5475999712944031, val_loss: 1.14543879032135, val_acc: 0.5224000215530396,  lr: 0.008891798965517242
step: 4218, train_loss: 1.1861324310302734, acc: 0.4984000027179718, val_loss: 1.14955735206604, val_acc: 0.5192000269889832,  lr: 0.008891454482758621
step: 4219, train_loss: 1.1011321544647217, acc: 0.5464000105857849, val_loss: 1.151258111000061, val_acc: 0.5167999863624573,  lr: 0.00889111
step: 4220, train_loss: 1.1664323806762695, acc: 0.5058000087738037, val_loss: 1.1529117822647095, val_acc: 0.5138000249862671,  lr: 0.00889076551724138
step: 4221, train_loss: 1.0550193786621094, acc: 0.5576000213623047, val_loss: 1.149367332458496, val_acc: 0.517799973487854,  lr: 0.00889042103448276
step: 4222, train_loss: 1.0528788566589355, acc: 0.5595999956130981, val_loss: 1.1426044702529907, val_acc: 0.5194000005722046,  lr: 0.008890076551724139
step: 4223, train_loss: 1.1289176940917969, acc: 0.5257999897003174, val_loss: 1.1436973810195923, val_acc: 0.5235999822616577,  lr: 0.008889732068965518
step: 4224, train_loss: 1.0550053119659424, acc: 0.5586000084877014, val_loss: 1.1432130336761475, val_acc: 0.519599974155426,  lr: 0.008889387586206898
step: 4225, train_loss: 1.029510736465454, acc: 0.5590000152587891, val_loss: 1.146794319152832, val_acc: 0.5085999965667725,  lr: 0.008889043103448277
step: 4226, train_loss: 1.123085379600525, acc: 0.5347999930381775, val_loss: 1.155143141746521, val_acc: 0.503600001335144,  lr: 0.008888698620689655
step: 4227, train_loss: 1.0643391609191895, acc: 0.5468000173568726, val_loss: 1.160181999206543, val_acc: 0.5067999958992004,  lr: 0.008888354137931034
step: 4228, train_loss: 1.1616981029510498, acc: 0.5108000040054321, val_loss: 1.1560635566711426, val_acc: 0.5130000114440918,  lr: 0.008888009655172413
step: 4229, train_loss: 1.1335686445236206, acc: 0.5228000283241272, val_loss: 1.1512346267700195, val_acc: 0.51419997215271,  lr: 0.008887665172413793
step: 4230, train_loss: 1.0980311632156372, acc: 0.5382000207901001, val_loss: 1.1553986072540283, val_acc: 0.5123999714851379,  lr: 0.008887320689655172
step: 4231, train_loss: 1.19247305393219, acc: 0.49459999799728394, val_loss: 1.1444051265716553, val_acc: 0.5234000086784363,  lr: 0.008886976206896552
step: 4232, train_loss: 1.2432810068130493, acc: 0.4742000102996826, val_loss: 1.146896243095398, val_acc: 0.524399995803833,  lr: 0.008886631724137931
step: 4233, train_loss: 1.1095255613327026, acc: 0.532800018787384, val_loss: 1.144502878189087, val_acc: 0.5285999774932861,  lr: 0.00888628724137931
